var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>Drove is a container orchestrator built at PhonePe. It is focused on simplicity, container performance, and easy operations.</p> <p></p>"},{"location":"index.html#features","title":"Features","text":"<p>The following sections go over the features.</p>"},{"location":"index.html#functional","title":"Functional","text":"<ul> <li>Application (service) and application container lifecycle management including mandated readiness checks, health checks, and pre-shutdown hooks to enable operators to take containers out of rotation easily and shut them down gracefully if needed.</li> <li>Ensures the required (specified) number of containers will always be present in the cluster. It will detect failures across the cluster and bring containers up/down to maintain the required instance count.</li> <li>Provides endpoint information to be consumed by routers like drove-gateway+nginx/traefik, etc., to expose containers over vhost.</li> <li>Supports short-lived container-based tasks. This helps folks build newer systems that can spin up containers as needed on the cluster. (See epoch).</li> <li>Provides functionality for real-time log streaming and log download for all instances.</li> <li>Log generation is handled by Drove in a file layout suitable for existing log shipping mechanisms as well as for streaming to rsyslog servers (if needed).</li> <li>Provides a functional read-only web-based console for checking cluster, application, task, and instance states, log streaming, etc.</li> <li>Provides APIs for both read and write operations.</li> <li>Supports discovery for sibling containers to support dynamic cluster reconfiguration in frameworks like Hazelcast.</li> <li>Support extra metadata in the form of tags on instances. This can be used in external systems for routing or other use-cases, as this information is available at the endpoint as well.</li> <li>CLI system for easy deployments and app/task lifecycle management.</li> <li>NGinx based router called drove-gateway for efficient communication with the cluster itself and containers deployed on it.</li> </ul>"},{"location":"index.html#operations","title":"Operations","text":"<ul> <li>Only two components (controller and executor) to make a cluster (plus Zookeeper for coordination, drove-gateway for routing if needed).</li> <li>All components dockerised to allow for easy deployment as well as upgrades.</li> <li>Simple single file YAML based configuration for the controller and executor. </li> <li>Cluster can be set to maintenance mode, where it pauses making changes to the cluster and turns off safeguards around ensuring the required number of containers get reported from the executor nodes. This will allow the SRE team to do seamless software updates across the whole cluster in a few minutes, irrespective of the size.</li> <li>Blacklisting of the executor nodes will automatically move all the running application containers to other nodes and prevent any further allocations to this node. This allows the node to be taken down for maintenance that needs longer periods of time to complete the OS/patch application, hardware maintenance, etc.</li> <li>Detect and kill any Zombie container nodes. On mesos, SRE team needs to be involved to manually kill such containers.</li> </ul>"},{"location":"index.html#performance","title":"Performance","text":"<ul> <li>Scheduler needs to be aware of NUMA hardware topology of the node and prevent containers from being split across nodes.</li> <li>Scheduler will pin containers to specific cores on a NUMA node so as to stop containers from stepping on each other\u2019s toes during peak hours and allow them to fully utilize the multi-level caches associated with the allocated CPU cores. Some balance is anyways gained by enabling hyper-threading on the executor nodes. This should be sufficient to provide a significant boost to the application performance.</li> <li>Allows for specialised nodes in the cluster. For example, there might be nodes with GPU available. We would want to run ML models that can utilise such hardware rather than allocate generic service containers on such nodes. To this end, the scheduler supports tagging and allows for containers to be explicitly mapped to tagged nodes.</li> <li>Allows for different placement policies to provide some flexibility to users in where they want to place their container nodes. This sometimes helps developers deploy specific apps to specific nodes where they might have been granted special privileges to perform deeper than usual investigations of running service containers (for example, take heap-dumps to specific mounted volumes, etc.).</li> <li>Allows for configuration injection at container startup. Such configuration can be streamed in as part of the deployment specification, mounted in from executor hosts, or fetched via API calls by the controllers or executors.</li> <li>Provides provisions to allow for extension of the scheduler to implement different scheduling algorithms in the code later on.</li> <li>Sometimes, NUMA localization and CPU pinning are overkill for clusters that don't need to extract the last bit of performance. For example, testing/staging clusters. To this end, Drove supports the following features:<ul> <li>Allows turning off NUMA and core pinning at executor level.</li> <li>Allows to specify multipliers for available CPU/memory to accommodate overprovisioning on the cluster.</li> </ul> </li> <li>Because the above are set at an executor level, the cluster can have different types of nodes with different required performance characteristics appropriately tagged. Relevant apps can be deployed based on performance requirements.</li> </ul>"},{"location":"index.html#resilience","title":"Resilience","text":"<ul> <li>Small number of moving pieces. Keeps the minimal amount of dependencies in the system. This reduces the exposure to failures by effectively reducing the number of external dependencies and possible failure points.</li> <li>Controller stores state on external system (Zookeeper) for now.</li> <li>Executor stores all container specific states in the container metadata itself. No other state is maintained/needed by executor.</li> <li>Containers keep running even when most of the system is down. This means that even when the cluster coordinators, executors, state storage, etc. are down, the already deployed containers keep on running as is. If a service discovery mechanism is implemented properly, this effectively protects the system against service disruptions, even in the face of failure of critical cluster components. At PhonePe, we use Ranger for service discovery. </li> <li>Container state reconciliation is part of the executor system, so that executor service can be restarted easily without affecting application or task deployments. In other words, the executor needs to recognise the containers started by itself on restarts and report their state as usual to the controller.</li> <li>Keeps things as simple as possible. Drove uses a few simple constructs (scale up/scale down) and implements all application/task features using that.</li> <li>Multi-mode cluster messaging ensures that faster updates will be sent to controller via sync channels, while the controller(s) keep refreshing the cluster state periodically, irrespective of the last synced data. Drove assumes that communication failures would happen. Even if new changes can\u2019t be propagated from executor to controller, it tries to keep existing topology as updated as possible.</li> <li>Built in safeguards to detect and kill any rogue (Zombie) container instances that have remained back for some reason (maybe some bug in the orchestrator, etc.).</li> <li>Controller is highly available with one leader active at a time. Any communication issues with Zookeeper will lead to quick death of the controller so that another controller can take up its place as quickly as possible.</li> <li>Leader can be tracked using the <code>ping</code> api and is used by components such as drove-gateway to provide a Virtual Host that can be used to interact with the cluster via the UI or the CLI, and other tools.</li> </ul>"},{"location":"index.html#security","title":"Security","text":"<ul> <li>Clearly designate roles for read and write operations. Write operations include cluster maintenance and app and task lifecycle maintenance.</li> <li>Authentication system is easily extensible.</li> <li>Supports basic auth as the minimal auth requirement. User credentials are stored in bcrypt format in controller config files.</li> <li>Support a no-auth mode for starter clusters.</li> <li>Provides audit logs for events in the system. Such logs can get aggregated and/or shipped out independently by existing log aggregation systems like logrotate-+rsync or (r)syslog, etc., by configuring the appropriate loggers in the controller configuration file.</li> <li>Separate authentication system for intra-cluster authentication and for edge. This will mean that even if external auth is compromised (or vice versa), the system will keep working as is.</li> <li>Shared secret is used for intra-cluster authentication.</li> <li>Dynamically generated tokens are injected into container instances for seamless sibling discovery. This provides a way for developers to implement clustering mechanisms for frameworks like Hazelcast (provided already).</li> </ul>"},{"location":"index.html#observability","title":"Observability","text":"<ul> <li>Real-time event stream from the controller can be used for any other event driven system like drove-gateway, etc., to refresh upstream topology.</li> <li>Metrics are available on admin ports for both the controllers and executors. Something like Telegraf can be used to collect and send them to the centralised metrics management system of your choice. (At PhonePe, we use Telegraf, which pushes the metrics to our custom metrics collection service, backed by a modified version of OpenTSDB. We use Grafana to visualize the same metrics.).</li> <li>Published metrics from controllers include system health metrics around themselves.</li> <li>Published metrics from executors contain system health metrics as well as other metrics around the containers running on them. This includes, but is not limited to, CPU, Memory and network usage.</li> </ul>"},{"location":"index.html#unsupported-features","title":"Unsupported Features","text":"<ul> <li>Auto-scaling of containers: In PhonePe, we have an extensive metrics ingestion system and an auto-scaler that works on a proprietary algorithm to scale containers up and down based on the same. This works independent of the orchestration system in play (we were on Drove and Mesos both at the same time during the transition period) and calls APIs on the deployment system that handles scaling operations independently. Any implementation at the orchestration level will not work as the contributors to the metrics might be running on different clusters, and scaling them independently will bring in more complexities rather than solving for simplicity.</li> <li>Network level traffic control: At PhonePe, network security is handled at VRF level, and container-level access control is not needed. All services are already integrated with the OAuth2 compliant internal authentication and authorization system and perform security checks for the same at the application layer. As a matter of fact, we want containers to be as close to the raw network level as possible to ensure we can extract the highest level of network performance possible, other things being constant.</li> <li>End-to-end configuration management: At this point of time, app/task configuration is maintained independently at PhonePe, subject to our approval workflows based on the compliance domain for the application, which can be static or dynamic and may be tied to deployments.</li> <li>Multi-DC clusters: We have not tested a single Drove cluster spanning across multiple data centers.</li> </ul>"},{"location":"index.html#terminology","title":"Terminology","text":"<p>Before we delve into the details, let's get acquainted with the required terminology:</p> <ul> <li>Application - A service running on the cluster. Such a service can have an exposed port and will have an automatically configured virtual host on Drove Gateway.</li> <li>Task - A transient container-based task.</li> <li>Controller Nodes - The brains of the cluster. Only one cluster is the leader and hence the decision maker in the system.</li> <li>Executor Nodes - The workhorse nodes of the cluster where the actual containers are run.</li> <li>Drove CLI - A command line client to interact with the cluster.</li> <li>Drove Gateway - Used to provide ingress to the leader and containers running on the cluster.</li> <li>Epoch - A cron-type scheduler to spin up tasks on a Drove cluster based on pre-defined schedules.</li> </ul>"},{"location":"index.html#github-repositories","title":"Github Repositories","text":"<ul> <li>Uber Repo - https://github.com/PhonePe/drove-orchestrator</li> <li>Drove Orchestrator Code - https://github.com/PhonePe/drove</li> <li>Drove CLI - https://github.com/PhonePe/drove-cli</li> <li>Drove Gateway - https://github.com/PhonePe/drove-gateway</li> <li>Epoch - https://github.com/PhonePe/epoch</li> <li>Epoch CLI - https://github.com/PhonePe/epoch-cli</li> <li>CoreDNS plugin for Drove - https://github.com/PhonePe/coredns-drove</li> </ul>"},{"location":"index.html#license","title":"License","text":"<p>Apache 2</p>"},{"location":"getting-started.html","title":"Getting Started","text":"<p>To get a trivial cluster up and running on a machine, the compose file can be used.</p>"},{"location":"getting-started.html#update-etc-hosts-to-interact-wih-nginx","title":"Update etc hosts to interact wih nginx","text":"<p>Add the following lines to <code>/etc/hosts</code> <pre><code>127.0.0.1   drove.local\n127.0.0.1   testapp.local\n</code></pre></p>"},{"location":"getting-started.html#download-the-compose-file","title":"Download the compose file","text":"<pre><code>wget https://raw.githubusercontent.com/PhonePe/drove-orchestrator/master/compose/compose.yaml\n</code></pre>"},{"location":"getting-started.html#bringing-up-a-demo-cluster","title":"Bringing up a demo cluster","text":"<p><pre><code>cd compose\ndocker-compose up\n</code></pre> This will start zookeeper,drove controller, executor and nginx/drove-gateway. The following ports are used:</p> <ul> <li>Zookeeper - 2181</li> <li>Executor - 3000</li> <li>Controller - 4000</li> <li>Gateway - 7000</li> </ul> <p>Drove credentials would be <code>admin/admin</code> and <code>guest/guest</code> for read-write and read-only permissions respectively.</p> <p>You should be able to access the UI at http://drove.local:7000</p>"},{"location":"getting-started.html#install-drove-cli","title":"Install drove-cli","text":"<p>Install the CLI for drove <pre><code>pip install drove-cli\n</code></pre></p>"},{"location":"getting-started.html#create-client-configuration","title":"Create Client Configuration","text":"<p>Put the following in <code>${HOME}/.drove</code></p> <pre><code>[local]\nendpoint = http://drove.local:4000\nusername = admin\npassword = admin\n</code></pre>"},{"location":"getting-started.html#deploy-an-app","title":"Deploy an app","text":"<p>Get the sample app spec: <pre><code>wget https://raw.githubusercontent.com/PhonePe/drove-cli/master/sample/test_app.json\n</code></pre></p> <p>Now deploy the app. <pre><code>drove -c local apps create test_app.json\n</code></pre></p>"},{"location":"getting-started.html#scale-the-app","title":"Scale the app","text":"<p><pre><code>drove -c local apps scale TEST_APP-1 1 -w\n</code></pre> This would expose the app as <code>testapp.local</code>. Endpoint would be: http://testapp.local:7000.</p> <p>You can test the app by running the following commands:</p> <pre><code>curl http://testapp.local:7000/\ncurl http://testapp.local:7000/files/drove.txt\n</code></pre>"},{"location":"getting-started.html#suspend-and-destroy-the-app","title":"Suspend and destroy the app","text":"<pre><code>drove -c local apps scale TEST_APP-1 0 -w\ndrove -c local apps destroy TEST_APP-1\n</code></pre>"},{"location":"getting-started.html#accessing-the-code","title":"Accessing the code","text":"<p>Code is hosted on github.</p> <p>Cloning everything:</p> <pre><code>git clone git@github.com:PhonePe/drove-orchestrator.git\ngit submodule init\ngit submodule update\n</code></pre>"},{"location":"apis/index.html","title":"Introduction","text":"<p>This section lists all the APIs that a user can communicate with.</p>"},{"location":"apis/index.html#making-an-api-call","title":"Making an API call","text":"<p>Use a standard HTTP client in the language of your choice to make a call to the leader controller (the cluster virtual host exposed by drove-gateway-nginx).</p> <p>Tip</p> <p>In case you are using Java, we recommend using the drove-client library along with the http-transport.</p> <p>If multiple controllers endpoints are provided, the client will track the leader automatically. This will reduce your dependency on drove-gateway.</p>"},{"location":"apis/index.html#authentication","title":"Authentication","text":"<p>Drove uses basic auth for authentication. (You can extend to use any other auth format like OAuth). The basic auth credentials need to be sent out in the standard format in the <code>Authorization</code> header.</p>"},{"location":"apis/index.html#response-format","title":"Response format","text":"<p>The response format is standard for all API calls:</p> <pre><code>{\n    \"status\": \"SUCCESS\",//(1)!\n    \"data\": {//(2)!\n        \"taskId\": \"T0012\"\n    },\n    \"message\": \"success\"//(3)!\n}\n</code></pre> <ol> <li><code>SUCCESS</code> or <code>FAILURE</code> as the case may be.</li> <li>Content of this field is contextual to the response.</li> <li>Will contain <code>success</code> if the call was successful or relevant error message.</li> </ol> <p>Warning</p> <p>APIs will return relevant HTTP status codes in case of error (for example <code>400</code> for validation errors, <code>401</code> for authentication failure). However, you must always ensure that the <code>status</code> field is set to <code>SUCCESS</code> for assuming the api call is succesful, even when HTTP status code is <code>2xx</code>.</p> <p>APIs in Drove belong to the following major classes:</p> <ul> <li>Application Management</li> <li>Task Management</li> <li>Cluster Management</li> <li>Log Access</li> </ul> <p>Tip</p> <p>Response models for these apis can be found in drove-models</p> <p>Note</p> <p>There are no publicly accessible APIs exposed by individual executors.</p>"},{"location":"apis/application.html","title":"Application Management","text":""},{"location":"apis/application.html#issue-application-operation-command","title":"Issue application operation command","text":"<p><code>POST /apis/v1/applications/operations</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/operations' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data '{\n    \"type\": \"SCALE\",\n    \"appId\": \"TEST_APP-1\",\n    \"requiredInstances\": 1,\n    \"opSpec\": {\n        \"timeout\": \"1m\",\n        \"parallelism\": 20,\n        \"failureStrategy\": \"STOP\"\n    }\n}'\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"appId\": \"TEST_APP-1\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Tip</p> <p>Relevant payloads for application commands can be found in application operations section.</p>"},{"location":"apis/application.html#cancel-currently-running-operation","title":"Cancel currently running operation","text":"<p><code>POST /apis/v1/applications/operations/{appId}/cancel</code></p> <p>Request <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/operations/TEST_APP/cancel' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/application.html#get-list-of-applications","title":"Get list of applications","text":"<p><code>GET /apis/v1/applications</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/applications' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"TEST_APP-1\": {\n            \"id\": \"TEST_APP-1\",\n            \"name\": \"TEST_APP\",\n            \"requiredInstances\": 0,\n            \"healthyInstances\": 0,\n            \"totalCPUs\": 0,\n            \"totalMemory\": 0,\n            \"state\": \"MONITORING\",\n            \"created\": 1719826995764,\n            \"updated\": 1719892126096\n        }\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/application.html#get-info-for-an-app","title":"Get info for an app","text":"<p><code>GET /apis/v1/applications/{id}</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/applications/TEST_APP-1' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"id\": \"TEST_APP-1\",\n        \"name\": \"TEST_APP\",\n        \"requiredInstances\": 1,\n        \"healthyInstances\": 1,\n        \"totalCPUs\": 1,\n        \"totalMemory\": 128,\n        \"state\": \"RUNNING\",\n        \"created\": 1719826995764,\n        \"updated\": 1719892279019\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/application.html#get-raw-json-specs","title":"Get raw JSON specs","text":"<p><code>GET /apis/v1/applications/{id}/spec</code></p> <p>Request</p> <pre><code>curl --location 'http://drove.local:7000/apis/v1/applications/TEST_APP-1/spec' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"name\": \"TEST_APP\",\n        \"version\": \"1\",\n        \"executable\": {\n            \"type\": \"DOCKER\",\n            \"url\": \"ghcr.io/appform-io/perf-test-server-httplib\",\n            \"dockerPullTimeout\": \"100 seconds\"\n        },\n        \"exposedPorts\": [\n            {\n                \"name\": \"main\",\n                \"port\": 8000,\n                \"type\": \"HTTP\"\n            }\n        ],\n        \"volumes\": [],\n        \"configs\": [\n            {\n                \"type\": \"INLINE\",\n                \"localFilename\": \"/testfiles/drove.txt\",\n                \"data\": \"\"\n            }\n        ],\n        \"type\": \"SERVICE\",\n        \"resources\": [\n            {\n                \"type\": \"CPU\",\n                \"count\": 1\n            },\n            {\n                \"type\": \"MEMORY\",\n                \"sizeInMB\": 128\n            }\n        ],\n        \"placementPolicy\": {\n            \"type\": \"ANY\"\n        },\n        \"healthcheck\": {\n            \"mode\": {\n                \"type\": \"HTTP\",\n                \"protocol\": \"HTTP\",\n                \"portName\": \"main\",\n                \"path\": \"/\",\n                \"verb\": \"GET\",\n                \"successCodes\": [\n                    200\n                ],\n                \"payload\": \"\",\n                \"connectionTimeout\": \"1 second\",\n                \"insecure\": false\n            },\n            \"timeout\": \"1 second\",\n            \"interval\": \"5 seconds\",\n            \"attempts\": 3,\n            \"initialDelay\": \"0 seconds\"\n        },\n        \"readiness\": {\n            \"mode\": {\n                \"type\": \"HTTP\",\n                \"protocol\": \"HTTP\",\n                \"portName\": \"main\",\n                \"path\": \"/\",\n                \"verb\": \"GET\",\n                \"successCodes\": [\n                    200\n                ],\n                \"payload\": \"\",\n                \"connectionTimeout\": \"1 second\",\n                \"insecure\": false\n            },\n            \"timeout\": \"1 second\",\n            \"interval\": \"3 seconds\",\n            \"attempts\": 3,\n            \"initialDelay\": \"0 seconds\"\n        },\n        \"tags\": {\n            \"superSpecialApp\": \"yes_i_am\",\n            \"say_my_name\": \"heisenberg\"\n        },\n        \"env\": {\n            \"CORES\": \"8\"\n        },\n        \"exposureSpec\": {\n            \"vhost\": \"testapp.local\",\n            \"portName\": \"main\",\n            \"mode\": \"ALL\"\n        },\n        \"preShutdown\": {\n            \"hooks\": [\n                {\n                    \"type\": \"HTTP\",\n                    \"protocol\": \"HTTP\",\n                    \"portName\": \"main\",\n                    \"path\": \"/\",\n                    \"verb\": \"GET\",\n                    \"successCodes\": [\n                        200\n                    ],\n                    \"payload\": \"\",\n                    \"connectionTimeout\": \"1 second\",\n                    \"insecure\": false\n                }\n            ],\n            \"waitBeforeKill\": \"3 seconds\"\n        }\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Note</p> <p><code>configs</code> section data will not be returned by any api calls</p>"},{"location":"apis/application.html#get-list-of-currently-active-instances","title":"Get list of currently active instances","text":"<p><code>GET /apis/v1/applications/{id}/instances</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/applications/TEST_APP-1/instances' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": [\n        {\n            \"appId\": \"TEST_APP-1\",\n            \"appName\": \"TEST_APP\",\n            \"instanceId\": \"AI-58eb1111-8c2c-4ea2-a159-8fc68010a146\",\n            \"executorId\": \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\",\n            \"localInfo\": {\n                \"hostname\": \"ppessdev\",\n                \"ports\": {\n                    \"main\": {\n                        \"containerPort\": 8000,\n                        \"hostPort\": 33857,\n                        \"portType\": \"HTTP\"\n                    }\n                }\n            },\n            \"resources\": [\n                {\n                    \"type\": \"CPU\",\n                    \"cores\": {\n                        \"0\": [\n                            2\n                        ]\n                    }\n                },\n                {\n                    \"type\": \"MEMORY\",\n                    \"memoryInMB\": {\n                        \"0\": 128\n                    }\n                }\n            ],\n            \"state\": \"HEALTHY\",\n            \"metadata\": {},\n            \"errorMessage\": \"\",\n            \"created\": 1719892354194,\n            \"updated\": 1719893180105\n        }\n    ],\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/application.html#get-list-of-old-instances","title":"Get list of old instances","text":"<p><code>GET /apis/v1/applications/{id}/instances/old</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/applications/TEST_APP-1/instances/old' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": [\n        {\n            \"appId\": \"TEST_APP-1\",\n            \"appName\": \"TEST_APP\",\n            \"instanceId\": \"AI-869e34ed-ebf3-4908-bf48-719475ca5640\",\n            \"executorId\": \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\",\n            \"resources\": [\n                {\n                    \"type\": \"CPU\",\n                    \"cores\": {\n                        \"0\": [\n                            2\n                        ]\n                    }\n                },\n                {\n                    \"type\": \"MEMORY\",\n                    \"memoryInMB\": {\n                        \"0\": 128\n                    }\n                }\n            ],\n            \"state\": \"STOPPED\",\n            \"metadata\": {},\n            \"errorMessage\": \"Error while pulling image ghcr.io/appform-io/perf-test-server-httplib: Status 500: {\\\"message\\\":\\\"Get \\\\\\\"https://ghcr.io/v2/\\\\\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\\\"}\\n\",\n            \"created\": 1719892279039,\n            \"updated\": 1719892354099\n        }\n    ],\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/application.html#get-info-for-an-instance","title":"Get info for an instance","text":"<p><code>GET /apis/v1/applications/{appId}/instances/{instanceId}</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/applications/TEST_APP-1/instances/AI-58eb1111-8c2c-4ea2-a159-8fc68010a146' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"appId\": \"TEST_APP-1\",\n        \"appName\": \"TEST_APP\",\n        \"instanceId\": \"AI-58eb1111-8c2c-4ea2-a159-8fc68010a146\",\n        \"executorId\": \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\",\n        \"localInfo\": {\n            \"hostname\": \"ppessdev\",\n            \"ports\": {\n                \"main\": {\n                    \"containerPort\": 8000,\n                    \"hostPort\": 33857,\n                    \"portType\": \"HTTP\"\n                }\n            }\n        },\n        \"resources\": [\n            {\n                \"type\": \"CPU\",\n                \"cores\": {\n                    \"0\": [\n                        2\n                    ]\n                }\n            },\n            {\n                \"type\": \"MEMORY\",\n                \"memoryInMB\": {\n                    \"0\": 128\n                }\n            }\n        ],\n        \"state\": \"HEALTHY\",\n        \"metadata\": {},\n        \"errorMessage\": \"\",\n        \"created\": 1719892354194,\n        \"updated\": 1719893440105\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/application.html#application-endpoints","title":"Application Endpoints","text":"<p><code>GET /apis/v1/endpoints</code></p> <p>Info</p> <p>This API provides up-to-date information about the host and port information about application instances running on the cluster. This information can be used for Service Discovery systems to keep their information in sync with changes in the topology of applications running on the cluster.</p> <p>Tip</p> <p>Any <code>tag</code> specified in the application specification is also exposed on endpoint. This can be used to implement complicated routing logic if needed in the NGinx template on Drove Gateway.</p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/endpoints' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": [\n        {\n            \"appId\": \"TEST_APP-1\",\n            \"vhost\": \"testapp.local\",\n            \"tags\": {\n                \"superSpecialApp\": \"yes_i_am\",\n                \"say_my_name\": \"heisenberg\"\n            },\n            \"hosts\": [\n                {\n                    \"host\": \"ppessdev\",\n                    \"port\": 44315,\n                    \"portType\": \"HTTP\"\n                }\n            ]\n        },\n        {\n            \"appId\": \"TEST_APP-2\",\n            \"vhost\": \"testapp.local\",\n            \"tags\": {\n                \"superSpecialApp\": \"yes_i_am\",\n                \"say_my_name\": \"heisenberg\"\n            },\n            \"hosts\": [\n                {\n                    \"host\": \"ppessdev\",\n                    \"port\": 46623,\n                    \"portType\": \"HTTP\"\n                }\n            ]\n        }\n    ],\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/cluster.html","title":"Cluster Management","text":""},{"location":"apis/cluster.html#ping-api","title":"Ping API","text":"<p><code>GET /apis/v1/ping</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/ping' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": \"pong\",\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Tip</p> <p>Use this api call to determine the leader in a cluster. This api will return a HTTP 200 only for the leader controller. All other controllers in the cluster will return 4xx for this api call.</p>"},{"location":"apis/cluster.html#cluster-management_1","title":"Cluster Management","text":""},{"location":"apis/cluster.html#get-current-cluster-state","title":"Get current cluster state","text":"<p><code>GET /apis/v1/cluster</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/cluster' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"leader\": \"ppessdev:4000\",\n        \"state\": \"NORMAL\",\n        \"numExecutors\": 1,\n        \"numApplications\": 1,\n        \"numActiveApplications\": 1,\n        \"freeCores\": 9,\n        \"usedCores\": 1,\n        \"totalCores\": 10,\n        \"freeMemory\": 18898,\n        \"usedMemory\": 128,\n        \"totalMemory\": 19026\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/cluster.html#set-maintenance-mode-on-cluster","title":"Set maintenance mode on cluster","text":"<p><code>POST /apis/v1/cluster/maintenance/set</code></p> <p>Request <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/cluster/maintenance/set' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"state\": \"MAINTENANCE\",\n        \"updated\": 1719897526772\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/cluster.html#remove-maintenance-mode-from-cluster","title":"Remove maintenance mode from cluster","text":"<p><code>POST /apis/v1/cluster/maintenance/unset</code></p> <p>Request <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/cluster/maintenance/unset' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"state\": \"NORMAL\",\n        \"updated\": 1719897573226\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Warning</p> <p>Cluster will remain in maintenance mode for some time (about 2 minutes) internally even after maintenance mode is removed.</p>"},{"location":"apis/cluster.html#executor-management","title":"Executor Management","text":""},{"location":"apis/cluster.html#get-list-of-executors","title":"Get list of executors","text":"<p><code>GET /apis/v1/cluster/executors</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/cluster/executors' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": [\n        {\n            \"executorId\": \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\",\n            \"hostname\": \"ppessdev\",\n            \"port\": 3000,\n            \"transportType\": \"HTTP\",\n            \"freeCores\": 9,\n            \"usedCores\": 1,\n            \"freeMemory\": 18898,\n            \"usedMemory\": 128,\n            \"tags\": [\n                \"ppessdev\"\n            ],\n            \"state\": \"ACTIVE\"\n        }\n    ],\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/cluster.html#get-detailed-info-for-one-executor","title":"Get detailed info for one executor","text":"<p><code>GET /apis/v1/cluster/executors/{id}</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/cluster/executors/a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"type\": \"EXECUTOR\",\n        \"hostname\": \"ppessdev\",\n        \"port\": 3000,\n        \"transportType\": \"HTTP\",\n        \"updated\": 1719897100104,\n        \"state\": {\n            \"executorId\": \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\",\n            \"cpus\": {\n                \"type\": \"CPU\",\n                \"freeCores\": {\n                    \"0\": [\n                        3,\n                        4,\n                        5,\n                        6,\n                        7,\n                        8,\n                        9,\n                        10,\n                        11\n                    ]\n                },\n                \"usedCores\": {\n                    \"0\": [\n                        2\n                    ]\n                }\n            },\n            \"memory\": {\n                \"type\": \"MEMORY\",\n                \"freeMemory\": {\n                    \"0\": 18898\n                },\n                \"usedMemory\": {\n                    \"0\": 128\n                }\n            }\n        },\n        \"instances\": [\n            {\n                \"appId\": \"TEST_APP-1\",\n                \"appName\": \"TEST_APP\",\n                \"instanceId\": \"AI-58eb1111-8c2c-4ea2-a159-8fc68010a146\",\n                \"executorId\": \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\",\n                \"localInfo\": {\n                    \"hostname\": \"ppessdev\",\n                    \"ports\": {\n                        \"main\": {\n                            \"containerPort\": 8000,\n                            \"hostPort\": 33857,\n                            \"portType\": \"HTTP\"\n                        }\n                    }\n                },\n                \"resources\": [\n                    {\n                        \"type\": \"CPU\",\n                        \"cores\": {\n                            \"0\": [\n                                2\n                            ]\n                        }\n                    },\n                    {\n                        \"type\": \"MEMORY\",\n                        \"memoryInMB\": {\n                            \"0\": 128\n                        }\n                    }\n                ],\n                \"state\": \"HEALTHY\",\n                \"metadata\": {},\n                \"errorMessage\": \"\",\n                \"created\": 1719892354194,\n                \"updated\": 1719897100104\n            }\n        ],\n        \"tasks\": [],\n        \"tags\": [\n            \"ppessdev\"\n        ],\n        \"blacklisted\": false\n    },\n    \"message\": \"success\"\n}\n</code></pre>"},{"location":"apis/cluster.html#take-executor-out-of-rotation","title":"Take executor out of rotation","text":"<p><code>POST /apis/v1/cluster/executors/blacklist</code></p> <p>Request <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/cluster/executors/blacklist?id=a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre></p> <p>Note</p> <p>Unlike other POST apis, the executors to be blacklisted are passed as query parameter <code>id</code>. To blacklist multiple executors, pass <code>.../blacklist?id=&lt;id1&gt;&amp;id=&lt;id2&gt;...</code></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"successful\": [\n            \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\"\n        ],\n        \"failed\": []\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/cluster.html#bring-executor-back-into-rotation","title":"Bring executor back into rotation","text":"<p><code>POST /apis/v1/cluster/executors/unblacklist</code></p> <p>Request <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/cluster/executors/unblacklist?id=a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre></p> <p>Note</p> <p>Unlike other POST apis, the executors to be un-blacklisted are passed as query parameter <code>id</code>. To un-blacklist multiple executors, pass <code>.../unblacklist?id=&lt;id1&gt;&amp;id=&lt;id2&gt;...</code></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"successful\": [\n            \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\"\n        ],\n        \"failed\": []\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/cluster.html#drove-cluster-events","title":"Drove Cluster Events","text":"<p>The following APIs can be used to monitor events on Drove. If the data needs to be consumed, the <code>/latest</code> API should be used. For simply knowing if an event of a certain type has occurred or not, the <code>/summary</code> is sufficient.</p>"},{"location":"apis/cluster.html#event-list","title":"Event List","text":"<p><code>GET /apis/v1/cluster/events/latest</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/cluster/events/latest?size=1024&amp;lastSyncTime=0' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"events\": [\n            {\n                \"metadata\": {\n                    \"CURRENT_INSTANCES\": 0,\n                    \"APP_ID\": \"TEST_APP-1\",\n                    \"PLACEMENT_POLICY\": \"ANY\",\n                    \"APP_VERSION\": \"1\",\n                    \"CPU_COUNT\": 1,\n                    \"CURRENT_STATE\": \"RUNNING\",\n                    \"PORTS\": \"main:8000:http\",\n                    \"MEMORY\": 128,\n                    \"EXECUTABLE\": \"ghcr.io/appform-io/perf-test-server-httplib\",\n                    \"VHOST\": \"testapp.local\",\n                    \"APP_NAME\": \"TEST_APP\"\n                },\n                \"type\": \"APP_STATE_CHANGE\",\n                \"id\": \"a2b7d673-2bc2-4084-8415-d8d37cafa63d\",\n                \"time\": 1719977632050\n            },\n            {\n                \"metadata\": {\n                    \"APP_NAME\": \"TEST_APP\",\n                    \"APP_ID\": \"TEST_APP-1\",\n                    \"PORTS\": \"main:44315:http\",\n                    \"EXECUTOR_ID\": \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\",\n                    \"EXECUTOR_HOST\": \"ppessdev\",\n                    \"CREATED\": 1719977629042,\n                    \"INSTANCE_ID\": \"AI-5efbb94f-835c-4c62-a073-a68437e60339\",\n                    \"CURRENT_STATE\": \"HEALTHY\"\n                },\n                \"type\": \"INSTANCE_STATE_CHANGE\",\n                \"id\": \"55d5876f-94ac-4c5d-a580-9c3b296add46\",\n                \"time\": 1719977631534\n            }\n        ],\n        \"lastSyncTime\": 1719977632050//(1)!\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <ol> <li>Pass this as the parameter <code>lastSyncTime</code> in the next call to <code>events</code> api to receive latest events.</li> </ol> Query Parameter Validation Description lastSyncTime +ve long range Time when the last sync call happened on the server. Defaults to 0 (initial sync). size 1-1024 Number of latest events to return. Defaults to 1024. We recommend leaving this as is."},{"location":"apis/cluster.html#event-summary","title":"Event Summary","text":"<p><code>GET /apis/v1/cluster/events/summary</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/cluster/events/summary?lastSyncTime=0' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre> Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"eventsCount\": {\n            \"INSTANCE_STATE_CHANGE\": 8,\n            \"APP_STATE_CHANGE\": 17,\n            \"EXECUTOR_BLACKLISTED\": 1,\n            \"EXECUTOR_UN_BLACKLISTED\": 1\n        },\n        \"lastSyncTime\": 1719977632050//(1)!\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <ol> <li>Pass this as the parameter <code>lastSyncTime</code> in the next call to <code>events</code> api to receive latest events.</li> </ol>"},{"location":"apis/cluster.html#continuous-monitoring-for-events","title":"Continuous monitoring for events","text":"<p>This is applicable for both the APIs listed above</p> <ul> <li>In the first call to events api, pass <code>lastSyncTime</code> as zero.</li> <li>In the response there will be a field <code>lastSyncTime</code></li> <li>Pass the last received <code>lastSyncTime</code> as the <code>lastSyncTime</code> param in the next call</li> <li>This api is cheap enough, you should plan to make calls to it every few seconds</li> </ul> <p>Info</p> <p>Model for the events can be found here.</p> <p>Tip</p> <p>Java programs should definitely look at using the event listener library  to listen to cluster events</p>"},{"location":"apis/logs.html","title":"Log Related APIs","text":""},{"location":"apis/logs.html#get-list-if-log-files","title":"Get list if log files","text":"<p>Application <code>GET /apis/v1/logfiles/applications/{appId}/{instanceId}/list</code></p> <p>Task <code>GET /apis/v1/logfiles/tasks/{sourceAppName}/{taskId}/list</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/logfiles/applications/TEST_APP-1/AI-5efbb94f-835c-4c62-a073-a68437e60339/list' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"files\": [\n        \"output.log-2024-07-04\",\n        \"output.log-2024-07-03\",\n        \"output.log\"\n    ]\n}\n</code></pre></p>"},{"location":"apis/logs.html#download-log-files","title":"Download Log Files","text":"<p>Application <code>GET /apis/v1/logfiles/applications/{appId}/{instanceId}/download/{fileName}</code></p> <p>Task <code>GET /apis/v1/logfiles/tasks/{sourceAppName}/{taskId}/download/{fileName}</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/logfiles/applications/TEST_APP-1/AI-5efbb94f-835c-4c62-a073-a68437e60339/download/output.log' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <p>Note</p> <p>The <code>Content-Disposition</code> header is set properly to the actual filename. For the above example it would be set to <code>attachment; filename=output.log</code>.</p>"},{"location":"apis/logs.html#read-chunks-from-log","title":"Read chunks from log","text":"<p>Application <code>GET /apis/v1/logfiles/applications/{appId}/{instanceId}/read/{fileName}</code></p> <p>Task <code>GET /apis/v1/logfiles/tasks/{sourceAppName}/{taskId}/read/{fileName}</code></p> Query Parameter Validation Description offset Default -1, should be positive number The offset of the file to read from. length Should be a positive number Number of bytes to read. <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/logfiles/applications/TEST_APP-1/AI-5efbb94f-835c-4c62-a073-a68437e60339/read/output.log' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"data\": \"\", //(1)!\n    \"offset\": 43318 //(2)!\n}\n</code></pre></p> <ol> <li>Will contain raw data or empty string (in case of first call)</li> <li>Offset to be passed in the next call</li> </ol>"},{"location":"apis/logs.html#how-to-tail-logs","title":"How to tail logs","text":"<ol> <li>Have a fixed buffer size in ming 1024/4096 etc</li> <li>Make a call to <code>/read</code> api with offset=<code>-1</code>, length = <code>buffer size</code></li> <li>The call will return no data, but will have a valid offset</li> <li>Pass this offset in the next call, data will be returned if available (or empty). The response will also return the offset to pass in the .ext call.</li> <li>The <code>data</code> returned might be empty or less than <code>length</code> depending on availability.</li> <li>Keep repeating (4) to keep tailing log</li> </ol> <p>Warning</p> <ul> <li>Offset = 0 means start of the file</li> <li>First call must be -1 for <code>tail</code> type functionality</li> </ul>"},{"location":"apis/task.html","title":"Task Management","text":""},{"location":"apis/task.html#issue-task-operation","title":"Issue task operation","text":"<p><code>POST /apis/v1/tasks/operations</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/tasks/operations' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data '{\n    \"type\": \"KILL\",\n    \"sourceAppName\" : \"TEST_APP\",\n    \"taskId\" : \"T0012\",\n    \"opSpec\": {\n        \"timeout\": \"5m\",\n        \"parallelism\": 1,\n        \"failureStrategy\": \"STOP\"\n    }\n}'\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"taskId\": \"T0012\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Tip</p> <p>Relevant payloads for task commands can be found in task operations section.</p>"},{"location":"apis/task.html#search-for-task","title":"Search for task","text":"<p><code>POST /apis/v1/tasks/search</code></p>"},{"location":"apis/task.html#list-all-tasks","title":"List all tasks","text":"<p><code>GET /apis/v1/tasks</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/tasks' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": [\n        {\n            \"sourceAppName\": \"TEST_APP\",\n            \"taskId\": \"T0013\",\n            \"instanceId\": \"TI-c2140806-2bb5-4ed3-9bb9-0c0c5fd0d8d6\",\n            \"executorId\": \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\",\n            \"hostname\": \"ppessdev\",\n            \"executable\": {\n                \"type\": \"DOCKER\",\n                \"url\": \"ghcr.io/appform-io/test-task\",\n                \"dockerPullTimeout\": \"100 seconds\"\n            },\n            \"resources\": [\n                {\n                    \"type\": \"CPU\",\n                    \"cores\": {\n                        \"0\": [\n                            2\n                        ]\n                    }\n                },\n                {\n                    \"type\": \"MEMORY\",\n                    \"memoryInMB\": {\n                        \"0\": 512\n                    }\n                }\n            ],\n            \"volumes\": [],\n            \"env\": {\n                \"ITERATIONS\": \"10\"\n            },\n            \"state\": \"RUNNING\",\n            \"metadata\": {},\n            \"errorMessage\": \"\",\n            \"created\": 1719827035480,\n            \"updated\": 1719827038414\n        }\n    ],\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"apis/task.html#get-task-instance-details","title":"Get Task Instance Details","text":"<p><code>GET /apis/v1/tasks/{sourceAppName}/instances/{taskId}</code></p> <p>Request <pre><code>curl --location 'http://drove.local:7000/apis/v1/tasks/TEST_APP/instances/T0012' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4='\n</code></pre></p> <p>Response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"sourceAppName\": \"TEST_APP\",\n        \"taskId\": \"T0012\",\n        \"instanceId\": \"TI-6cf36f5c-6480-4ed5-9e2d-f79d9648529a\",\n        \"executorId\": \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\",\n        \"hostname\": \"ppessdev\",\n        \"executable\": {\n            \"type\": \"DOCKER\",\n            \"url\": \"ghcr.io/appform-io/test-task\",\n            \"dockerPullTimeout\": \"100 seconds\"\n        },\n        \"resources\": [\n            {\n                \"type\": \"CPU\",\n                \"cores\": {\n                    \"0\": [\n                        3\n                    ]\n                }\n            },\n            {\n                \"type\": \"MEMORY\",\n                \"memoryInMB\": {\n                    \"0\": 512\n                }\n            }\n        ],\n        \"volumes\": [],\n        \"env\": {\n            \"ITERATIONS\": \"10\"\n        },\n        \"state\": \"STOPPED\",\n        \"metadata\": {},\n        \"taskResult\": {\n            \"status\": \"SUCCESSFUL\",\n            \"exitCode\": 0\n        },\n        \"errorMessage\": \"\",\n        \"created\": 1719823470267,\n        \"updated\": 1719823483836\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"applications/index.html","title":"Introduction","text":"<p>An application is a virtual representation of a running service in the system.</p> <p>Running containers for an application are called application instances.</p> <p>An application specification contains the following details about the application:</p> <ul> <li>Name - Name of the application</li> <li>Version - Version of this specification</li> <li>Executable - The container to deploy on the cluster</li> <li>Ports - Ports to be exposed from the container</li> <li>Resources - CPU and Memory required for the container</li> <li>Placement Policy - How containers are to be placed in the cluster</li> <li>Healthchecks - Healthcheck details</li> <li>Readiness Checks - Readiness checks to pass before container is considered to be healthy</li> <li>Pre Shutdown Hooks - Pre shutdown hooks to run on container before it is killed</li> <li>Environment Variables - Environment variables and values</li> <li>Exposure Information - Virtual host information</li> <li>Volumes - Volumes to be mounted into the container</li> <li>Configs - Configs/files to be mounted into the container</li> <li>Logging details - Logging spec (for example rsyslog server)</li> <li>Tags - A map of strings for additional metadata</li> </ul> <p>Info</p> <p>Once a spec is registered to the cluster, it can not be changed</p>"},{"location":"applications/index.html#application-id","title":"Application ID","text":"<p>Once an application is created on the cluster, an Application id is generated. The format of this id currently is: <code>{name}-{version}</code>. All further operations to be done on the application will need to refer to it by this ID.</p>"},{"location":"applications/index.html#application-states-and-operations","title":"Application States and Operations","text":"<p>An application on a Drove cluster follows a fixed lifecycle modelled as a state machine. State transitions are triggered by operations. Operations can be issued externally using API calls or may be generated internally by the application monitoring system.</p>"},{"location":"applications/index.html#states","title":"States","text":"<p>Applications on a Drove cluster can be one of the following states:</p> <ul> <li>INIT - This is an intermediate state during which the application is being initialized and  the spec is being validated. This is the origination state of the application.</li> <li>MONITORING - A stable state in which application is created or suspended and does not have any running instances</li> <li>RUNNING - A stable state in which application has the expected non-zero number of healthy application instances running on the cluster</li> <li>OUTAGE_DETECTED - An intermediate state when Drove has detected that the current number of application instances is not matching the expected number of instances.</li> <li>SCALING_REQUESTED - An intermediate state that signifies that application instances are being spun up or shut down to get the number of running instances to match the expected instances.</li> <li>STOP_INSTANCES_REQUESTED - An intermediate state that signifies that specific instances of the application are being killed as requested by the user/system.</li> <li>REPLACE_INSTANCES_REQUESTED - An _intermediate state _that signifies that instances of the application are being replaced with newer instances as requested by the user. This signifies that the app is effectively being restarted.</li> <li>DESTROY_REQUESTED - An intermediate state that signifies that the user has requested to destroy the application and remove it from the cluster.</li> <li>DESTROYED - An intermediate state that signifies that the app has been destroyed and metadata cleanup is underway. This is the terminal state of an application.</li> </ul>"},{"location":"applications/index.html#operations","title":"Operations","text":"<p>The following application operations are recognized by Drove:</p> <ul> <li>CREATE - Create an application. Take the Application Specification. Fails if an app with the same application id (name + version) already exists on the cluster</li> <li>DESTROY - Destroy an application. Takes app id as parameter. Deletes all metadata about the application from the cluster. Allowed only if the application is in Monitoring state (i.e. has zero running instances).</li> <li>START_INSTANCES - Create new application instances. Takes the app id as well as the number of new instances to deploy. Allowed only if the application is in Monitoring or Running state.</li> <li>STOP_INSTANCES - Stop running application instances. Takes the app id, list of instance ids to be stopped as well as flag to denote if replacement instances are to be started by Drove or not. Allowed only if the application is in Monitoring or Running state.</li> <li>SCALE - Scale the application up and down to the specified number of instances. Drove will internally calculate whether to spin new containers up or spin old containers down as needed. Allowed if the app is in Monitoring or Running state. It is better to use either START or STOP instances command above to be more explicit in behavior. The SCALE operation is mostly for internal use by Drove, but can be issued externally as well.</li> <li>REPLACE_INSTANCES - Replace application instances with newer ones. Can be used to do rolling restarts on the cluster. Specific instances can be targeted as well by passing an optional list of instance ids to be replaced. Allowed only when the application is in Running state.</li> <li>SUSPEND - A shortcut to set expected instances for an application to zero. This will get translated into a SCALE operation and any running instances will be gracefully shut down. Allowed only when the application is in running state.</li> <li>RECOVER - Internal command used to restore application state on controller failover.</li> </ul> <p>Tip</p> <p>All operations can take an optional Cluster Operation Spec which can be used to control the timeout and parallelism of tasks generated by the operation.</p>"},{"location":"applications/index.html#application-state-machine","title":"Application State Machine","text":"<p>The following state machine signifies the states and transitions as affected by cluster state and operations issued.</p> <p></p>"},{"location":"applications/instances.html","title":"Application Instances","text":"<p>Application instances are running containers for an application. The state machine for instances are managed in a decentralised manner on the cluster nodes locally and not by the controllers. This includes running health checks, readiness checks and shutdown hooks on the container, container loss detection and container state recovery on executor service restart.</p> <p>Regular updates about the instance state are provided by executors to the controllers and are used to keep the application state up-to-date or trigger application operations to bring the applications to stable states.</p>"},{"location":"applications/instances.html#application-instance-states","title":"Application Instance States","text":"<p>An application instance can be in one of the following states at one point in time:</p> <ul> <li>PENDING - Container state machine start has been triggered.</li> <li>PROVISIONING - Docker image is being downloaded</li> <li>PROVISIONING_FAILED - Docker image download failed</li> <li>STARTING - Docker run is being executed</li> <li>START_FAILED - Docker run failed</li> <li>UNREADY - Docker started, readiness check not yet started.</li> <li>READINESS_CHECK_FAILED - Readiness check was run and has failed terminally</li> <li>READY - Readiness checks have passed</li> <li>HEALTHY - Health check has passed. Container is running properly and passing regular health checks</li> <li>UNHEALTHY - Regular health check has failed. Container will stop.</li> <li>STOPPING - Shutdown hooks are being called and docker kill be be issued</li> <li>DEPROVISIONING - Docker image is being cleaned up</li> <li>STOPPED - Docker stop has completed</li> <li>LOST - Container has exited unexpectedly while executor service was down</li> <li>UNKNOWN - All running containers are in this state when executor service is getting restarted and before startup recovery has kicked in</li> </ul>"},{"location":"applications/instances.html#application-instance-state-machine","title":"Application Instance State Machine","text":"<p>Instance state machine transitions might be triggered on receipt of commands issued by the controller or due to internal changes in the container (might have died or started failing health checks) as well as external factors like executor service restarts.</p> <p></p> <p>Note</p> <p>No operations are allowed to be performed on application instances directly through the executor</p>"},{"location":"applications/operations.html","title":"Application Operations","text":"<p>This page discusses operations relevant to Application management. Please go over the Application State Machine and Application Instance State Machine to understand the different states an application (and it's instances) can be in and how operations applied move an application from one state to another.</p> <p>Note</p> <p>Please go through Cluster Op Spec to understand the operation parameters being sent.</p> <p>Note</p> <p>Only one operation can be active on a particular <code>{appName,version}</code> combination.</p> <p>Warning</p> <p>Only the leader controller will accept and process operations. To avoid confusion, use the controller endpoint exposed by Drove Gateway to issue commands.</p>"},{"location":"applications/operations.html#how-to-initiate-an-operation","title":"How to initiate an operation","text":"<p>Tip</p> <p>Use the Drove CLI to perform all manual operations.</p> <p>All operations for application lifecycle management need to be issued via a POST HTTP call to the leader controller endpoint on the path <code>/apis/v1/applications/operations</code>. API will return HTTP OK/200 and relevant json response as payload.</p> <p>Sample api call:</p> <pre><code>curl --location 'http://drove.local:7000/apis/v1/applications/operations' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data '{\n    \"type\": \"START_INSTANCES\",\n    \"appId\": \"TEST_APP-3\",\n    \"instances\": 1,\n    \"opSpec\": {\n        \"timeout\": \"5m\",\n        \"parallelism\": 32,\n        \"failureStrategy\": \"STOP\"\n    }\n}'\n</code></pre> <p>Note</p> <p>In the above examples, <code>http://drove.local:7000</code> is the endpoint of the leader. <code>TEST_APP-3</code> is the Application ID. Authorization is basic auth.</p>"},{"location":"applications/operations.html#cluster-operation-specification","title":"Cluster Operation Specification","text":"<p>When an operation is submitted to the cluster, a cluster op spec needs to be specified. This is needed to control different aspects of the operation, including parallelism of an operation or increase the timeout for the operation and so on. </p> <p>The following aspects of an operation can be configured:</p> Name Option Description Timeout <code>timeout</code> The duration after which Drove considers the operation to have timed out. Parallelism <code>parallelism</code> Parallelism of the task. (Range: 1-32) Failure Strategy <code>failureStrategy</code> Set this to <code>STOP</code>. <p>Note</p> <p>For internal recovery operations, Drove generates it's own operations. For that, Drove applies the following cluster operation spec:</p> <ul> <li>timeout - 300 seconds</li> <li>parallelism - 1</li> <li>failureStrategy - <code>STOP</code></li> </ul> <p>The default operation spec can be configured in the controller configuration file. It is recommended to set this to a something like 8 for faster recovery.</p>"},{"location":"applications/operations.html#how-to-cancel-an-operation","title":"How to cancel an operation","text":"<p>Operations can be requested to be cancelled asynchronously. A POST call needs to be made to leader controller endpoint on the api <code>/apis/v1/operations/{applicationId}/cancel</code> (1) to achieve this.</p> <ol> <li><code>applicationId</code> is the Application ID for the application</li> </ol> <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/operations/TEST_APP-3/cancel' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre> <p>Warning</p> <p>Operation cancellation is not instantaneous. Cancellation will be affected only after current execution of the active operation is complete.</p>"},{"location":"applications/operations.html#create-an-application","title":"Create an application","text":"<p>Before deploying containers on the cluster, an application needs to be created.</p> <p>Preconditions:</p> <ul> <li>App should not exist in the cluster</li> </ul> <p>State Transition: </p> <ul> <li>none \u2192 <code>MONITORING</code></li> </ul> <p>To create an application, an Application Spec needs to be created first. </p> <p>Once ready, CLI command needs to be issued or the following payload needs to be sent:</p> Drove CLIJSON <pre><code>drove -c local apps create sample/test_app.json\n</code></pre> <p>Sample Request Payload <pre><code>{\n    \"type\": \"CREATE\",\n    \"spec\": {...}, //(1)!\n    \"opSpec\": { //(2)!\n        \"timeout\": \"5m\",\n        \"parallelism\": 1,\n        \"failureStrategy\": \"STOP\"\n    }\n}\n</code></pre></p> <ol> <li>Spec as mentioned in Application Specification</li> <li>Operation spec as mentioned in Cluster Op Spec</li> </ol> <p>Sample response <pre><code>{\n    \"data\" : {\n        \"appId\" : \"TEST_APP-1\"\n    },\n    \"message\" : \"success\",\n    \"status\" : \"SUCCESS\"\n}\n</code></pre></p>"},{"location":"applications/operations.html#starting-new-instances-of-an-application","title":"Starting new instances of an application","text":"<p>New instances can be started by issuing the  <code>START_INSTANCES</code> command.</p> <p>Preconditions - Application must be in one of the following states: <code>MONITORING</code>, <code>RUNNING</code></p> <p>State Transition:</p> <ul> <li>{<code>RUNNING</code>, <code>MONITORING</code>} \u2192 <code>RUNNING</code></li> </ul> <p>The following command/payload will start <code>2</code> new instances of the application.</p> Drove CLIJSON <pre><code>drove -c local apps deploy TEST_APP-1 2\n</code></pre> <p>Sample Request Payload <pre><code>{\n    \"type\": \"START_INSTANCES\",\n    \"appId\": \"TEST_APP-1\",//(1)!\n    \"instances\": 2,//(2)!\n    \"opSpec\": {//(3)!\n        \"timeout\": \"5m\",\n        \"parallelism\": 32,\n        \"failureStrategy\": \"STOP\"\n    }\n}\n</code></pre></p> <ol> <li>Application ID</li> <li>Number of instances to be started</li> <li>Operation spec as mentioned in Cluster Op Spec</li> </ol> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"appId\": \"TEST_APP-1\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"applications/operations.html#suspending-an-application","title":"Suspending an application","text":"<p>All instances of an application can be shut down by issuing the  <code>SUSPEND</code> command.</p> <p>Preconditions - Application must be in one of the following states: <code>MONITORING</code>, <code>RUNNING</code></p> <p>State Transition:</p> <ul> <li>{<code>RUNNING</code>, <code>MONITORING</code>} \u2192 <code>MONITORING</code></li> </ul> <p>The following command/payload will suspend all instances of the application.</p> Drove CLIJSON <pre><code>drove -c local apps suspend TEST_APP-1\n</code></pre> <p>Sample Request Payload <pre><code>{\n    \"type\": \"SUSPEND\",\n    \"appId\": \"TEST_APP-1\",//(1)!\n    \"opSpec\": {//(2)!\n        \"timeout\": \"5m\",\n        \"parallelism\": 32,\n        \"failureStrategy\": \"STOP\"\n    }\n}\n</code></pre></p> <ol> <li>Application ID</li> <li>Operation spec as mentioned in Cluster Op Spec</li> </ol> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"appId\": \"TEST_APP-1\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"applications/operations.html#scaling-the-application-up-or-down","title":"Scaling the application up or down","text":"<p>Scaling the application to required number of containers can be achieved using the <code>SCALE</code> command. Application can be either scaled up or down using this command.</p> <p>Preconditions - Application must be in one of the following states: <code>MONITORING</code>, <code>RUNNING</code></p> <p>State Transition:</p> <ul> <li>{<code>RUNNING</code>, <code>MONITORING</code>} \u2192 <code>MONITORING</code> if <code>requiredInstances</code> is set to 0</li> <li>{<code>RUNNING</code>, <code>MONITORING</code>} \u2192 <code>RUNNING</code> if <code>requiredInstances</code> is non 0</li> </ul> Drove CLIJSON <pre><code>drove -c local apps scale TEST_APP-1 2\n</code></pre> <p>Sample Request Payload <pre><code>{\n    \"type\": \"SCALE\",\n    \"appId\": \"TEST_APP-1\", //(3)!\n    \"requiredInstances\": 2, //(1)!\n    \"opSpec\": { //(2)!\n        \"timeout\": \"1m\",\n        \"parallelism\": 20,\n        \"failureStrategy\": \"STOP\"\n    }\n}\n</code></pre></p> <ol> <li>Absolute number of instances to be maintained on the cluster for the application</li> <li>Operation spec as mentioned in Cluster Op Spec</li> <li>Application ID</li> </ol> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"appId\": \"TEST_APP-1\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Note</p> <p>During scale down, older instances are stopped first</p> <p>Tip</p> <p>If implementing automation on top of Drove APIs, just use the <code>SCALE</code> command to scale up or down instead of using <code>START_INSTANCES</code> or <code>SUSPEND</code> separately.</p>"},{"location":"applications/operations.html#restarting-an-application","title":"Restarting an application","text":"<p>Application can be restarted by issuing the <code>REPLACE_INSTANCES</code> operation. In this case, first <code>clusterOpSpec.parallelism</code> number of containers are spun up first and then an equivalent number of them are spun down. This ensures that cluster maintains enough capacity is maintained in the cluster to handle incoming traffic as the restart is underway. </p> <p>Warning</p> <p>If the cluster does not have sufficient capacity to spin up new containers, this operation will get stuck. So adjust your parallelism accordingly.</p> <p>Preconditions - Application must be in <code>RUNNING</code> state.</p> <p>State Transition:</p> <ul> <li><code>RUNNING</code> \u2192 <code>REPLACE_INSTANCES_REQUESTED</code> \u2192 <code>RUNNING</code></li> </ul> Drove CLIJSON <pre><code>drove -c local apps restart TEST_APP-1\n</code></pre> <p>Sample Request Payload <pre><code>{\n    \"type\": \"REPLACE_INSTANCES\",\n    \"appId\": \"TEST_APP-1\", //(1)!\n    \"instanceIds\": [], //(2)!\n    \"opSpec\": { //(3)!\n        \"timeout\": \"1m\",\n        \"parallelism\": 20,\n        \"failureStrategy\": \"STOP\"\n    }\n}\n</code></pre></p> <ol> <li>Application ID</li> <li>Instances that need to be restarted. This is optional. If nothing is passed, all instances will be replaced.</li> <li>Operation spec as mentioned in Cluster Op Spec</li> </ol> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"appId\": \"TEST_APP-1\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Tip</p> <p>To replace specific instances, pass their application instance ids (starts with <code>AI-...</code>) in the <code>instanceIds</code> parameter in the JSON payload.</p>"},{"location":"applications/operations.html#stop-or-replace-specific-instances-of-an-application","title":"Stop or replace specific instances of an application","text":"<p>Application instances can be killed by issuing the <code>STOP_INSTANCES</code> operation. Default behaviour of Drove is to replace killed instances by new instances. Such new instances are always spun up before the specified(old) instances are stopped. If <code>skipRespawn</code> parameter is set to true, the application instance is killed but no new instances are spun up to replace it.</p> <p>Warning</p> <p>If the cluster does not have sufficient capacity to spin up new containers, and <code>skipRespawn</code> is not set or set to <code>false</code>, this operation will get stuck.</p> <p>Preconditions - Application must be in <code>RUNNING</code> state.</p> <p>State Transition:</p> <ul> <li><code>RUNNING</code> \u2192 <code>STOP_INSTANCES_REQUESTED</code> \u2192 <code>RUNNING</code> if final number of instances is non zero</li> <li><code>RUNNING</code> \u2192 <code>STOP_INSTANCES_REQUESTED</code> \u2192 <code>MONITORING</code> if final number of instances is zero</li> </ul> Drove CLIJSON <pre><code>drove -c local apps appinstances kill TEST_APP-1 AI-601d160e-c692-4ddd-8b7f-4c09b30ed02e\n</code></pre> <p>Sample Request Payload <pre><code>{\n    \"type\": \"STOP_INSTANCES\",\n    \"appId\" : \"TEST_APP-1\",//(1)!\n    \"instanceIds\" : [ \"AI-601d160e-c692-4ddd-8b7f-4c09b30ed02e\" ],//(2)!\n    \"skipRespawn\" : true,//(3)!\n    \"opSpec\": {//(4)!\n        \"timeout\": \"5m\",\n        \"parallelism\": 1,\n        \"failureStrategy\": \"STOP\"\n    }\n}\n</code></pre></p> <ol> <li>Application ID</li> <li>Instance ids to be stopped</li> <li>Do not spin up new containers to replace the stopped ones. This is set ot <code>false</code> by default.</li> <li>Operation spec as mentioned in Cluster Op Spec</li> </ol> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"appId\": \"TEST_APP-1\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"applications/operations.html#destroy-an-application","title":"Destroy an application","text":"<p>To remove an application deployment (<code>appName</code>-<code>version</code> combo) the <code>DESTROY</code> command can be issued.</p> <p>Preconditions:</p> <ul> <li>App should not exist in the cluster</li> </ul> <p>State Transition: </p> <ul> <li><code>MONITORING</code> \u2192 <code>DESTROY_REQUESTED</code> \u2192 <code>DESTROYED</code> \u2192 none</li> </ul> <p>To create an application, an Application Spec needs to be created first. </p> <p>Once ready, CLI command needs to be issued or the following payload needs to be sent:</p> Drove CLIJSON <pre><code>drove -c local apps destroy TEST_APP_1\n</code></pre> <p>Sample Request Payload <pre><code>{\n    \"type\": \"DESTROY\",\n    \"appId\" : \"TEST_APP-1\",//(1)!\n    \"opSpec\": {//(2)!\n        \"timeout\": \"5m\",\n        \"parallelism\": 2,\n        \"failureStrategy\": \"STOP\"\n    }\n}\n</code></pre></p> <ol> <li>Spec as mentioned in Application Specification</li> <li>Operation spec as mentioned in Cluster Op Spec</li> </ol> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"appId\": \"TEST_APP-1\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Warning</p> <p>All metadata for an app and it's instances are completely obliterated from Drove's storage once an app is destroyed</p>"},{"location":"applications/outage.html","title":"Outage Detection and Recovery","text":"<p>Drove tracks all instances for an app deployment in the cluster. It will ensure the required number of containers is always running on the cluster.</p>"},{"location":"applications/outage.html#instance-health-detection-and-tracking","title":"Instance health detection and tracking","text":"<p>Executor runs periodic health checks on the container according to check spec configuration. - Runs readiness checks to ensure container is started properly before declaring it healthy - Runs health checks on the container at regular intervals to ensure it is in operating condition</p> <p>Behavior for both is configured by setting the appropriate options in the application specification.</p> <p>Result of such health checks (both success and failure) are reported to the controller. Appropriate action is taken to shut down containers that fail readiness or health checks. </p>"},{"location":"applications/outage.html#container-crash","title":"Container crash","text":"<p>If container for an application crashes, Drove will automatically spin up a container in it's place.</p>"},{"location":"applications/outage.html#executor-node-hardware-failure","title":"Executor node hardware failure","text":"<p>If an executor node fails, instances running on that node will be lost. This is detected by the outage detector and new containers are spun up on other parts of the cluster.</p>"},{"location":"applications/outage.html#executor-service-temporary-unavailability","title":"Executor service temporary unavailability","text":"<p>On restart, executor service reads the metadata embedded in the container and registers them. It performs a reconciliation with the leader controller to kill any local containers if the unavailability was too long and controller has already spun up new alternatives.</p>"},{"location":"applications/outage.html#zombie-container-detection-and-cleanup","title":"Zombie (container) detection and cleanup","text":"<p>Executor service keeps track of all containers it is supposed to run by running periodic reconciliation with the leader controller. Any mismatch gets handled:</p> <ul> <li>if a container is found that is not supposed to be running, it is killed</li> <li>If a container that is supposed to be running is not found, it is marked as lost and reported to the controller. This triggers the controller to spin up an alternative container on the cluster.</li> </ul>"},{"location":"applications/specification.html","title":"Application Specification","text":"<p>An application is defined using JSON. We use a sample configuration below to explain the options.</p>"},{"location":"applications/specification.html#sample-application-definition","title":"Sample Application Definition","text":"<pre><code>{\n    \"name\": \"TEST_APP\", // (1)!\n    \"version\": \"1\", // (2)!\n    \"type\": \"SERVICE\", // (3)!\n    \"executable\": { //(4)!\n        \"type\": \"DOCKER\", // (5)!\n        \"url\": \"ghcr.io/appform-io/perf-test-server-httplib\",// (6)!\n        \"dockerPullTimeout\": \"100 seconds\"// (7)!\n    },\n    \"resources\": [//(20)!\n        {\n            \"type\": \"CPU\",\n            \"count\": 1//(21)!\n        },\n        {\n            \"type\": \"MEMORY\",\n            \"sizeInMB\": 128//(22)!\n        }\n    ],\n    \"volumes\": [//(12)!\n        {\n            \"pathInContainer\": \"/data\",//(13)!\n            \"pathOnHost\": \"/mnt/datavol\",//(14)!\n            \"mode\" : \"READ_WRITE\"//(15)!\n        }\n    ],\n    \"configs\" : [//(16)!\n        {\n            \"type\" : \"INLINE\",//(17)!\n            \"localFilename\": \"/testfiles/drove.txt\",//(18)!\n            \"data\" : \"RHJvdmUgdGVzdA==\"//(19)!\n        }\n    ],\n    \"placementPolicy\": {//(23)!\n        \"type\": \"ANY\"//(24)!\n    },\n    \"exposedPorts\": [//(8)!\n        {\n            \"name\": \"main\",//(9)!\n            \"port\": 8000,//(10)!\n            \"type\": \"HTTP\"//(11)!\n        }\n    ],\n    \"healthcheck\": {//(25)!\n        \"mode\": {//(26)!\n            \"type\": \"HTTP\", //(27)!\n            \"protocol\": \"HTTP\",//(28)!\n            \"portName\": \"main\",//(29)!\n            \"path\": \"/\",//(30)!\n            \"verb\": \"GET\",//(31)!\n            \"successCodes\": [//(32)!\n                200\n            ],\n            \"payload\": \"\", //(33)!\n            \"connectionTimeout\": \"1 second\" //(34)!\n        },\n        \"timeout\": \"1 second\",//(35)!\n        \"interval\": \"5 seconds\",//(36)!\n        \"attempts\": 3,//(37)!\n        \"initialDelay\": \"0 seconds\"//(38)!\n    },\n    \"readiness\": {//(39)!\n        \"mode\": {\n            \"type\": \"HTTP\",\n            \"protocol\": \"HTTP\",\n            \"portName\": \"main\",\n            \"path\": \"/\",\n            \"verb\": \"GET\",\n            \"successCodes\": [\n                200\n            ],\n            \"payload\": \"\",\n            \"connectionTimeout\": \"1 second\"\n        },\n        \"timeout\": \"1 second\",\n        \"interval\": \"3 seconds\",\n        \"attempts\": 3,\n        \"initialDelay\": \"0 seconds\"\n    },\n    \"exposureSpec\": {//(42)!\n        \"vhost\": \"testapp.local\", //(43)!\n        \"portName\": \"main\", //(44)!\n        \"mode\": \"ALL\"//(45)!\n    },\n    \"env\": {//(41)!\n        \"CORES\": \"8\"\n    },\n    \"args\" : [//(54)!\n        \"./entrypoint.sh\",\n        \"arg1\",\n        \"arg2\"\n    ],\n    \"tags\": { //(40)!\n        \"superSpecialApp\": \"yes_i_am\",\n        \"say_my_name\": \"heisenberg\"\n    },\n    \"preShutdown\": {//(46)!\n        \"hooks\": [ //(47)!\n            {\n                \"type\": \"HTTP\",\n                \"protocol\": \"HTTP\",\n                \"portName\": \"main\",\n                \"path\": \"/\",\n                \"verb\": \"GET\",\n                \"successCodes\": [\n                    200\n                ],\n                \"payload\": \"\",\n                \"connectionTimeout\": \"1 second\"\n            }\n        ],\n        \"waitBeforeKill\": \"3 seconds\"//(48)!\n    },\n    \"logging\": {//(49)!\n        \"type\": \"LOCAL\",//(50)!\n        \"maxSize\": \"100m\",//(51)!\n        \"maxFiles\": 3,//(52)!\n        \"compress\": true//(53)!\n    }\n}\n</code></pre> <ol> <li>A human readable name for the application. This will remain constant for different versions of the app.</li> <li>A version number. Drove does not enforce any format for this, but it is recommended to increment this for changes in spec.</li> <li>This should be fixed to <code>SERVICE</code> for an application/service.</li> <li>Coordinates for the executable. Refer to Executable Specification for details.</li> <li>Right now the only type supported is <code>DOCKER</code>.</li> <li>Docker container address</li> <li>Timeout for container pull.</li> <li>The ports to be exposed from the container.</li> <li>A logical name for the port. This will be used to reference this port in other sections.</li> <li>Actual port number as mentioned in Dockerfile.</li> <li>Type of port. Can be: <code>HTTP</code>, <code>HTTPS</code>, <code>TCP</code>, <code>UDP</code>.</li> <li>Volumes to be mounted. Refer to Volume Specification for details.</li> <li>Path that will be visible inside the container for this mount.</li> <li>Actual path on the host machine for the mount.</li> <li>Mount mode can be <code>READ_WRITE</code> and <code>READ_ONLY</code></li> <li>Configuration to be injected as file inside the container. Please refer to Config Specification for details.</li> <li>Type of config. Can be <code>INLINE</code>, <code>EXECUTOR_LOCAL_FILE</code>, <code>CONTROLLER_HTTP_FETCH</code> and <code>EXECUTOR_HTTP_FETCH</code>. Specifies how drove will get the contents to be injected..</li> <li>File name for the config inside the container.</li> <li>Serialized form of the data, this and other parameters will vary according to the <code>type</code> specified above.</li> <li>List of resources required to run this application. Check Resource Requirements Specification for more details.</li> <li>Number of CPU cores to be allocated.</li> <li>Amount of memory to be allocated expressed in Megabytes</li> <li>Specifies how the container will be placed on the cluster. Check Placement Policy for details.</li> <li>Type of placement can be <code>ANY</code>, <code>ONE_PER_HOST</code>, <code>MATCH_TAG</code>, <code>NO_TAG</code>, <code>RULE_BASED</code>, <code>ANY</code> and <code>COMPOSITE</code>. Rest of the parameters in this section will depend on the type.</li> <li>Health check to ensure service is running fine. Refer to Check Specification for details.</li> <li>Mode of health check, can be api call or command.</li> <li>Type of this check spec. Type can be <code>HTTP</code> or <code>CMD</code>. Rest of the options in this example are HTTP specific.</li> <li>API call protocol. Can be <code>HTTP</code>/<code>HTTPS</code></li> <li>Port name as mentioned in the <code>exposedPorts</code> section.</li> <li>HTTP path. Include query params here.</li> <li>HTTP method. Can be <code>GET</code>,<code>PUT</code> or <code>POST</code>.</li> <li>Set of HTTP status codes which can be considered as success.</li> <li>Payload to be sent for <code>POST</code> and <code>PUT</code> calls.</li> <li>Connection timeout for the port.</li> <li>Timeout for the check run.</li> <li>Interval between check runs.</li> <li>Max attempts after which the overall check is considered to be a failure.</li> <li>Time to wait before starting check runs.</li> <li>Readiness check to pass for the container to be considered as ready. Refer to Check Specification for details.</li> <li>Key value metadata that can be used in external systems.</li> <li>Custom environment variables. Additional variables are injected by Drove as well. See Environment Variables section for details.</li> <li>Specifies the virtual host on which this container is exposed.</li> <li>FQDN for the virtual host.</li> <li>Port name as specified in <code>exposedPorts</code> section.</li> <li>Mode for exposure. Set this to <code>ALL</code> for now.</li> <li>Things to do before a container is shutdown. Check Pre Shutdown Behavior for more details.</li> <li>Hooks (HTTP api call or shell command) to run before shutting down the container. Format is same as health/readiness checks. Refer to HTTP Check Actions and Command Check Options for details.</li> <li>Time to wait before killing the container. The container will be in <code>UNREADY</code> state during this time and hence won't have api calls routed to it via Drove Gateway.</li> <li>Specify how docker log files are configured. Refer to Logging Specification</li> <li>Log to local file</li> <li>Maximum File Size</li> <li>Number of latest log files to retain</li> <li>Log files will be compressed</li> <li>List of command line arguments. See Command Line Arguments for details.</li> </ol>"},{"location":"applications/specification.html#executable-specification","title":"Executable Specification","text":"<p>Right now Drove supports only docker containers. However as engines, both docker and podman are supported. Drove executors will fetch the executable directly from the registry based on the configuration provided.</p> Name Option Description Type <code>type</code> Set type to <code>DOCKER</code>. URL <code>url</code> Docker container URL`. Timeout <code>dockerPullTimeout</code> Timeout for docker image pull. <p>Note</p> <p>Drove supports docker registry authentication. This can be configured in the executor configuration file.</p>"},{"location":"applications/specification.html#resource-requirements-specification","title":"Resource Requirements Specification","text":"<p>This section specifies the hardware resources required to run the container. Right now only CPU and MEMORY are supported as resource types that can be reserved for a container.</p>"},{"location":"applications/specification.html#cpu-requirements","title":"CPU Requirements","text":"<p>Specifies number of cores to be assigned to the container.</p> Name Option Description Type <code>type</code> Set type to <code>CPU</code> for this. Count <code>count</code> Number of cores to be assigned."},{"location":"applications/specification.html#memory-requirements","title":"Memory Requirements","text":"<p>Specifies amount of memory to be allocated to a container.</p> Name Option Description Type <code>type</code> Set type to <code>MEMORY</code> for this. Count <code>sizeInMB</code> Amount of memory (in Mega Bytes) to be allocated. <p>Sample <pre><code>[\n    {\n        \"type\": \"CPU\",\n        \"count\": 1\n    },\n    {\n        \"type\": \"MEMORY\",\n        \"sizeInMB\": 128\n    }\n]\n</code></pre></p> <p>Note</p> <p>Both <code>CPU</code> and <code>MEMORY</code> configurations are mandatory.</p>"},{"location":"applications/specification.html#volume-specification","title":"Volume Specification","text":"<p>Files and directories can be mounted from the executor host into the container. The <code>volumes</code> section contains a list of volumes that need to be mounted.</p> Name Option Description Path In Container <code>pathInContainer</code> Path that will be visible inside the container for this mount. Path On Host <code>pathOnHost</code> Actual path on the host machine for the mount. Mount Mode <code>mode</code> Mount mode can be <code>READ_WRITE</code> and <code>READ_ONLY</code> to allow the containerized process to write or read to the volume. <p>Info</p> <p>We do not support mounting remote volumes as of now.</p>"},{"location":"applications/specification.html#config-specification","title":"Config Specification","text":"<p>Drove supports injection of configuration files into containers. The specifications for the same are discussed below.</p>"},{"location":"applications/specification.html#inline-config","title":"Inline config","text":"<p>Inline configuration can be added in the Application Specification itself. This will manifest as a file inside the container.</p> <p>The following details are needed for this:</p> Name Option Description Type <code>type</code> Set the value to <code>INLINE</code> Local Filename <code>localFilename</code> File name for the config inside the container. Data <code>data</code> Base64 encoded string for the data. The value for this will be masked on UI. <p>Config file: <pre><code>port: 8080\nlogLevel: DEBUG\n</code></pre> Corresponding config specification: <pre><code>{\n    \"type\" : \"INLINE\",\n    \"localFilename\" : \"/config/service.yml\",\n    \"data\" : \"cG9ydDogODA4MApsb2dMZXZlbDogREVCVUcK\"\n}\n</code></pre></p> <p>Warning</p> <p>The full base 64 encoded config data will get stored in Drove ZK and will be pushed to executors inline. It is not recommended to stream large config files to containers using this method. This will probably need additional configuration on your ZK cluster.</p>"},{"location":"applications/specification.html#locally-loaded-config","title":"Locally loaded config","text":"<p>Config file from a path on the executor directly. Such files can be distributed to the executor host using existing configuration management systems such as OpenTofu, Salt etc.</p> <p>The following details are needed for this:</p> Name Option Description Type <code>type</code> Set the value to <code>EXECUTOR_LOCAL_FILE</code> Local Filename <code>localFilename</code> File name for the config inside the container. File path <code>filePathOnHost</code> Path to the config file on executor host. <p>Sample config specification: <pre><code>{\n    \"type\" : \"EXECUTOR_LOCAL_FILE\",\n    \"localFilename\" : \"/config/service.yml\",\n    \"data\" : \"/mnt/configs/myservice/config.yml\"\n}\n</code></pre></p>"},{"location":"applications/specification.html#controller-fetched-config","title":"Controller fetched Config","text":"<p>Config file can be fetched from a remote server by the controller. Once fetched, these will be streamed to the executor as part of the instance specification for starting a container.</p> <p>The following details are needed for this:</p> Name Option Description Type <code>type</code> Set the value to <code>CONTROLLER_HTTP_FETCH</code> Local Filename <code>localFilename</code> File name for the config inside the container. HTTP Call Details <code>http</code> HTTP Call related details. Please refer to HTTP Call Specification for details. <p>Sample config specification: <pre><code>{\n    \"type\" : \"CONTROLLER_HTTP_FETCH\",\n    \"localFilename\" : \"/config/service.yml\",\n    \"http\" : {\n        \"protocol\" : \"HTTP\",\n        \"hostname\" : \"configserver.internal.yourdomain.net\",\n        \"port\" : 8080,\n        \"path\" : \"/configs/myapp\",\n        \"username\" : \"appuser\",\n        \"password\" : \"secretpassword\"\n    }\n}\n</code></pre></p> <p>Note</p> <p>The controller will make an API call for every single time it asks an executor to spin up a container. Please make sure to account for this in your configuration management system.</p>"},{"location":"applications/specification.html#executor-fetched-config","title":"Executor fetched Config","text":"<p>Config file can be fetched from a remote server by the executor before spinning up a container. Once fetched, the payload will be injected as a config file into the container.</p> <p>The following details are needed for this:</p> Name Option Description Type <code>type</code> Set the value to <code>EXECUTOR_HTTP_FETCH</code> Local Filename <code>localFilename</code> File name for the config inside the container. HTTP Call Details <code>http</code> HTTP Call related details. Please refer to HTTP Call Specification for details. <p>Sample config specification: <pre><code>{\n    \"type\" : \"EXECUTOR_HTTP_FETCH\",\n    \"localFilename\" : \"/config/service.yml\",\n    \"http\" : {\n        \"protocol\" : \"HTTP\",\n        \"hostname\" : \"configserver.internal.yourdomain.net\",\n        \"port\" : 8080,\n        \"path\" : \"/configs/myapp\",\n        \"username\" : \"appuser\",\n        \"password\" : \"secretpassword\"\n    }\n}\n</code></pre></p> <p>Note</p> <p>All executors will make an API call for every single time they spin up a container for this application. Please make sure to account for this in your configuration management system.</p>"},{"location":"applications/specification.html#http-call-specification","title":"HTTP Call Specification","text":"<p>This section details the options that can set when making http calls to a configuration management system from controllers or executors.</p> <p>The following options are available for HTTP call:</p> Name Option Description Protocol <code>protocol</code> Protocol to use for upstream call. Can be <code>HTTP</code> or <code>HTTPS</code>. Hostname <code>hostname</code> Host to call. Port <code>port</code> Provide custom port. Defaults to 80 for http and 443 for https. API Path <code>path</code> Path component of the URL. Include query parameters here. Defaults to <code>/</code> HTTP Method <code>verb</code> Type of call, use <code>GET</code>, <code>POST</code> or <code>PUT</code>. Defaults to <code>GET</code>. Success Code <code>successCodes</code> List of HTTP status codes which is considered as success. Defaults to <code>[200]</code> Payload <code>payload</code> Data to be used for POST and PUT calls Connection Timeout <code>connectionTimeout</code> Timeout for upstream connection. Operation timeout <code>operationTimeout</code> Timeout for actual operation. Username <code>username</code> Username to be used basic auth. This field is masked out on the UI. Password <code>password</code> Password to be used for basic auth. This field is masked on the UI. Authorization Header <code>authHeader</code> Data to be passed in HTTP <code>Authorization</code> header. This field is masked on the UI. Additional Headers <code>headers</code> Any other headers to be passed to the upstream in the HTTP calls. This is a map of Skip SSL Checks <code>insecure</code> Skip hostname and certification checks during SSL handshake with the upstream."},{"location":"applications/specification.html#placement-policy-specification","title":"Placement Policy Specification","text":"<p>Placement policy governs how Drove deploys containers on the cluster. The following sections discuss the different placement policies available and how they can be configured to achieve optimal placement of containers.</p> <p>Warning</p> <p>All policies will work only at a <code>{appName, version}</code> combination level. They will not ensure constraints at an <code>appName</code> level. This means that for somethinge like a one per node placement, for the same <code>appName</code>, multiple containers can run on the same host if multiple deployments with different <code>version</code>s are active in a cluster. Same applies for all policies like N per host and so on.</p> <p>Important details about executor tagging</p> <ul> <li>All hosts have at-least one tag, it's own hostname.</li> <li>The <code>TAG</code> policy will consider them as valid tags. This can be used to place containers on specific hosts if needed.</li> <li>This is handled specially in all other policy types and they will consider executors having only the hostname tag as untagged.</li> <li>A host with a tag (other than host) will not have any containers running if not placed on them specifically using the <code>MATCH_TAG</code> policy</li> </ul>"},{"location":"applications/specification.html#any-placement","title":"Any Placement","text":"<p>Containers for a <code>{appName, version}</code> combination can run on any un-tagged executor host.</p> Name Option Description Policy Type <code>type</code> Put <code>ANY</code> as policy. <p>Sample: <pre><code>{\n    \"type\" : \"ANY\"\n}\n</code></pre></p> <p>Tip</p> <p>For most use-cases this is the placement policy to use.</p>"},{"location":"applications/specification.html#one-per-host-placement","title":"One Per Host Placement","text":"<p>Ensures that only one container for a particular <code>{appName, version}</code> combination is running on an executor host at a time.</p> Name Option Description Policy Type <code>type</code> Put <code>ONE_PER_HOST</code> as policy. <p>Sample: <pre><code>{\n    \"type\" : \"ONE_PER_HOST\"\n}\n</code></pre></p>"},{"location":"applications/specification.html#max-n-per-host-placement","title":"Max N Per Host Placement","text":"<p>Ensures that at most N containers for a <code>{appName, version}</code> combination is running on an executor host at a time.</p> Name Option Description Policy Type <code>type</code> Put <code>MAX_N_PER_HOST</code> as policy. Max count <code>max</code> The maximum num of containers that can run on an executor. Range: 1-64 <p>Sample: <pre><code>{\n    \"type\" : \"MAX_N_PER_HOST\",\n    \"max\": 3\n}\n</code></pre></p>"},{"location":"applications/specification.html#match-tag-placement","title":"Match Tag Placement","text":"<p>Ensures that containers for a <code>{appName, version}</code> combination are running on an executor host that has the tags as mentioned in the policy.</p> Name Option Description Policy Type <code>type</code> Put <code>MATCH_TAG</code> as policy. Max count <code>tag</code> The tag to match. <p>Sample: <pre><code>{\n    \"type\" : \"MATCH_TAG\",\n    \"tag\": \"gpu_enabled\"\n}\n</code></pre></p>"},{"location":"applications/specification.html#no-tag-placement","title":"No Tag Placement","text":"<p>Ensures that containers for a <code>{appName, version}</code> combination are running on an executor host that has no tags.</p> Name Option Description Policy Type <code>type</code> Put <code>NO_TAG</code> as policy. <p>Sample: <pre><code>{\n    \"type\" : \"NO_TAG\"\n}\n</code></pre></p> <p>Info</p> <p>The NO_TAG policy is mostly for internal use, and does not need to be specified when deploying containers that do not need any special placement logic.</p>"},{"location":"applications/specification.html#composite-policy-based-placement","title":"Composite Policy Based Placement","text":"<p>Composite policy can be used to combine policies together to create complicated placement requirements.</p> Name Option Description Policy Type <code>type</code> Put <code>COMPOSITE</code> as policy. Polices <code>policies</code> List of policies to combine Combiner <code>combiner</code> Can be <code>AND</code> and <code>OR</code> and signify all-match and any-match logic on the <code>policies</code> mentioned. <p>Sample: <pre><code>{\n    \"type\" : \"COMPOSITE\",\n    \"policies\": [\n        {\n            \"type\": \"ONE_PER_HOST\"\n        },\n        {\n            \"type\": \"MATH_TAG\",\n            \"tag\": \"gpu_enabled\"\n        }\n    ],\n    \"combiner\" : \"AND\"\n}\n</code></pre> The above policy will ensure that only one container of the relevant <code>{appName,version}</code> will run on GPU enabled machines.</p> <p>Tip</p> <p>It is easy to go into situations where no executors match complicated placement policies. Internally, we tend to keep things rather simple and use the ANY placement for most cases and maybe tags in a few places with over-provisioning or for hosts having special hardware </p>"},{"location":"applications/specification.html#environment-variables","title":"Environment variables","text":"<p>This config can be used to inject custom environment variables to containers. The values are defined as part of deployment specification, are same across the cluster and immutable to modifications from inside the container (ie any overrides from inside the container will not be visible across the cluster).</p> <p>Sample: <pre><code>{\n    \"MY_VARIABLE_1\": \"fizz\",\n    \"MY_VARIABLE_2\": \"buzz\"\n}\n</code></pre></p> <p>The following environment variables are injected by Drove to all containers:</p> Variable Name Value HOST Hostname where the container is running. This is for marathon compatibility. PORT_<code>PORT_NUMBER</code> A variable for every port specified in <code>exposedPorts</code> section. The value is the actual port on the host, the specified port is mapped to.  For example if ports 8080 and 8081 are specified, two variables called <code>PORT_8080</code> and <code>PORT_8081</code> will be injected. DROVE_EXECUTOR_HOST Hostname where container is running. DROVE_CONTAINER_ID Container that is deployed DROVE_APP_NAME App name as specified in the Application Specification DROVE_INSTANCE_ID Actual instance ID generated by Drove DROVE_APP_ID Application ID as generated by Drove DROVE_APP_INSTANCE_AUTH_TOKEN A JWT string generated by Drove that can be used by this container to call <code>/apis/v1/internal/...</code> apis. <p>Warning</p> <p>Do not pass secrets using environment variables. These variables are all visible on the UI as is. Please use Configs to inject secrets files and so on.</p>"},{"location":"applications/specification.html#command-line-arguments","title":"Command line arguments","text":"<p>A list of command line arguments that are sent to the container engine to execute inside the container. This is provides ways for you to configure your container behaviour based off such arguments. Please refer to docker documentation for details.</p> <p>Danger</p> <p>This might have security implications from a system point of view. As such Drove provides administrators a way to disable passing arguments at the cluster level by setting <code>disableCmdlArgs</code> to <code>true</code> in the controller configuration.</p>"},{"location":"applications/specification.html#check-specification","title":"Check Specification","text":"<p>One of the cornerstones of managing applications on the cluster is to ensure we keep track of instance health and manage their life cycle depending on their health state. We need to define how to monitor health for containers accordingly. The checks will be executed on Applications and a Check result is generated. The result consists of the following:</p> <ul> <li>Status - Healthy, Unhealthy or Stopped if the container is already in stopping state</li> <li>Message - Any error message as generated by a specific checker</li> </ul>"},{"location":"applications/specification.html#common-options","title":"Common Options","text":"Name Option Description Mode <code>mode</code> The definition of a HTTP call or a Command to be executed in the container. See following sections for details. Timeout <code>timeout</code> Duration for which we wait before declaring a check as failed Interval <code>interval</code> Interval at which check will be retried Attempts <code>attempts</code> Number of times a check is retried before it is declared as a failure Initial Delay <code>initialDelay</code> Delay before executing the check for the first time. <p>Note</p> <p><code>initialDelay</code> is ignored when readiness checks and health checks are run in the recovery path as the container is already running at that point in time.</p>"},{"location":"applications/specification.html#http-check-options","title":"HTTP Check Options","text":"Name Option Description Type <code>type</code> Fixed to HTTP for HTTP checker Protocol <code>protocol</code> HTTP or HTTPS call to be made Port Name <code>portName</code> The name of the container port to make the http call on as specified in the Exposed Ports section in Application Spec Path <code>path</code> The api path to call HTTP method <code>verb</code> The HTTP Verb/Method to invoke. GET/PUT and POST are supported here Success Codes <code>successCodes</code> A set of HTTP status codes that we should consider as a success from this API. Payload <code>payload</code> A string payload that we can pass if the Verb is POST or PUT Connection Timeout <code>connectionTimeout</code> Maximum time for which the checker will wait for the connection to be set up with the container. Insecure <code>insecure</code> Skip hostname and certificate checks for HTTPS ports during checks."},{"location":"applications/specification.html#command-check-options","title":"Command Check Options","text":"Field Option Description Type <code>type</code> Fixed to CMD for command checker Command <code>command</code> Command to execute in the container. (Equivalent to <code>docker exec -it &lt;container&gt; command&gt;</code>)"},{"location":"applications/specification.html#exposure-specification","title":"Exposure Specification","text":"<p>Exposure spec is used to specify the virtual host Drove Gateway exposes to outside world for communication with the containers.</p> <p>The following information needs to be specified:</p> Name Option Description Virtual Host <code>vhost</code> The virtual host to be exposed on NGinx. This should be a fully qualified domain name. Port Name <code>portName</code> The portname to be exposed on the vhost. Port names are defined in <code>exposedPorts</code> section. Exposure Mode <code>mode</code> Use <code>ALL</code> here for now. Signifies that all healthy instances of the app are exposed to traffic. <p>Sample: <pre><code>{\n    \"vhost\": \"teastapp.mydomain\",\n    \"port\": \"main\",\n    \"mode\": \"ALL\"\n}\n</code></pre></p> <p>Note</p> <p>Application instances in any state other than <code>HEALTHY</code> are not considered for exposure. Please check Application Instance State Machine for an understanding of states of instances.</p>"},{"location":"applications/specification.html#configuring-pre-shutdown-behaviour","title":"Configuring Pre Shutdown Behaviour","text":"<p>Before a container is shut down, it is desirable to ensure things are spun down properly. This behaviour can be configured in the <code>preShutdown</code> section of the configuration.</p> Name Option Description Hooks <code>hooks</code> List of api calls and commands to be run on the container before it is killed. Each hook is either a HTTP Call Spec or Command Spec Wait Time <code>waitBeforeKill</code> Time to wait before killing the container. <p>Sample <pre><code>{\n    \"hooks\": [\n        {\n            \"type\": \"HTTP\",\n            \"protocol\": \"HTTP\",\n            \"portName\": \"main\",\n            \"path\": \"/\",\n            \"verb\": \"GET\",\n            \"successCodes\": [\n                200\n            ],\n            \"payload\": \"\",\n            \"connectionTimeout\": \"1 second\"\n        }\n    ],\n    \"waitBeforeKill\": \"3 seconds\"//(48)!\n}\n</code></pre></p> <p>Note</p> <p>The <code>waitBeforeKill</code> timed wait kicks in after all the hooks have been executed.</p>"},{"location":"applications/specification.html#logging-specification","title":"Logging Specification","text":"<p>Can be used to configure how container logs are managed on the system. </p> <p>Note</p> <p>This section affects the docker log driver. Drove will continue to stream logs to it's own logger which can be configured at executor level through the executor configuration file.</p>"},{"location":"applications/specification.html#local-logger-configuration","title":"Local Logger configuration","text":"<p>This is used to configure the <code>json-file</code> log driver.</p> Name Option Description Type <code>type</code> Set the value to <code>LOCAL</code> Max Size <code>maxSize</code> Maximum file size. Anything bigger than this will lead to rotation. Max Files <code>maxFiles</code> Maximum number of logs files to keep. Range: 1-100 Compress <code>compress</code> Enable log file compression. <p>Tip</p> <p>If <code>logging</code> section is omitted, the following configuration is applied by default: - File size: 10m - Number of files: 3 - Compression: on</p>"},{"location":"applications/specification.html#rsyslog-configuration","title":"Rsyslog configuration","text":"<p>In case suers want to stream logs to an rsyslog server, the logging configuration needs to be set to RSYSLOG mode.</p> Name Option Description Type <code>type</code> Set the value to <code>RSYSLOG</code> Server <code>server</code> URL for the rsyslog server. Tag Prefix <code>tagPrefix</code> Prefix to add at the start of a tag Tag Suffix <code>tagSuffix</code> Suffix to add at the en of a tag. <p>Note</p> <p>The default tag is the <code>DROVE_INSTANCE_ID</code>. The <code>tagPrefix</code> and <code>tagSuffix</code> will to before and after this</p>"},{"location":"cluster/cluster.html","title":"Anatomy of a Drove Cluster","text":"<p>The following diagram provides a high level overview of a typical Drove cluster.  The overall topology consists of the following components:</p> <ul> <li>An Apache ZooKeeper cluster for state persistence and coordination</li> <li>A set of controller nodes one of which (the leader) manages the cluster</li> <li>A set of executor nodes on which the containers actually execute</li> <li>NGinx + drove-gateway nodes that expose virtual hosts for the leader controller as well as for the vhosts defined for the various applications running on the cluster</li> </ul>"},{"location":"cluster/cluster.html#apache-zookeeper","title":"Apache ZooKeeper","text":"<p>Zookeeper is a central component in a Drove cluster. It is used in the following manner:</p> <ul> <li>As store for discovery of cluster components like Controller and Executor to each other</li> <li>For electing the leader controller in the cluster</li> <li>As storage for Application and Task Specifications</li> <li>Asynchronous communication channel/transient store for real-time information about controller and executor state in the cluster</li> </ul>"},{"location":"cluster/cluster.html#controller","title":"Controller","text":"<p>The controller service is the brains of a Drove cluster. The role of the controller consists of the following:</p> <ul> <li>Ensure it has a reasonably up-to-date information about the cluster topology and free/used resources</li> <li>Track executor status (blacklisted/online/offline etc) and tagging. - Take corrective actions in case some of them become inaccessible for whatever reason</li> <li>Manages container placement to ensure that application and task containers get placed according to provided placement configuration/spec</li> <li>Manage NUMA node and core affinity ensuring that instances get deployed optimally on cores and NUMA nodes without stepping on each other</li> <li>Provide a UI for users to consume data about cluster, applications and tasks</li> <li>Provide APIs for systems to provision apps, tasks and manage the cluster</li> <li>Provide event stream for other tools and services to follow what is happening on the cluster</li> <li>Provide APIs to list container level logs and provide real-time offset based polling of log contents for application and task instances</li> <li>Implement leader election based HA so that only one controller is active(leader) at a time.</li> <li>All decisions regarding scheduling, state machine management and recovery are taken only by the leader</li> <li>Manage the lifecycle of all applications deployed on the cluster.<ul> <li>Maintain the required number of application instances as specified during deployment. This means that the controller has to monitor all applications running on all nodes and replace any instances or kill any spurious ones to ensure a constant number of instances on the cluster. The required number of instances is maintained as expected count, the current number of instances is maintained as running count.</li> <li>Provide a way to adjust the number of instances for this application. This would help in users being able to scale applications up and down as needed.</li> <li>Provide a way to restart all instances of the application. This would mean the controller would have to orchestrate a continuous string of start-kill operations across instances running on the cluster.</li> <li>Graceful shutdown/suspension of application across the cluster. This comes as a natural extension of the above and is mostly a scale down operation with the expected count set as zero.</li> </ul> </li> <li>Manage task lifecycle<ul> <li>Maintain task state-machine by scheduling it on executors and ensuring it reaches terminal state</li> <li>Provide mechanisms to cancel tasks</li> </ul> </li> <li>Reconcile stale and dead instances for applications and tasks and take corrective measures to ensure steady state if necessary</li> <li>Application instance migration from blacklisted executors</li> <li>Send command messages to executors to start and stop instances with retries and failure recovery</li> </ul>"},{"location":"cluster/cluster.html#executors","title":"Executors","text":"<p>Executors are the agents running on the nodes where the containers are deployed. Role of the executors is the following:</p> <ul> <li>Publish hardware topology of the machine to the controller on startup.</li> <li>Manage container lifecycle including:<ul> <li>Pulling containers from docker repository with optional authentication</li> <li>Start containers with proper options for pinning containers to specific NUMA nodes and cores as specified by controller<ul> <li>Data for an instance is stored as specific docker label values on the containers themselves</li> </ul> </li> <li>Run HTTP call or shell command based readiness checks to ensure application container is ready to serve traffic based on readiness checks specification in start message</li> <li>Monitor application container health by running periodic HTTP call or shell command based health checks as specified by controller in start message</li> <li>Track normal (for tasks) or abnormal (for application instances) container exits. <ul> <li>For tasks, the exit code is collected and used to deduce if task succeeded (exit code is 0) or failed (exit code is non-zero)</li> <li>For application containers, the expectation is for the container to stop only when explicitly requested and hence all exits are considered as failures and handled accordingly</li> </ul> </li> <li>Stop application containers on request from controller</li> <li>Run any pre-shutdown hook calls as specified in the application specification before killing container</li> <li>Cleanup container volumes etc</li> <li>Cleanup docker images (if local image caching is turned off which is the default behaviour)</li> </ul> </li> <li>Send regular local node status updates to ZooKeeper every 20 seconds</li> <li>Send instant updates by making direct HTTP calls to the leader controller when anything changes for any running containers and for every step of the container state machine execution for both task and application instances to allow for faster updates</li> <li>Recover containers on process restart based on the metadata stored as labels on the running container. This data is reconciled with a snapshot of the expected instances on the node as received from the leader controller at that point in time.</li> <li>Find and kill any zombie containers that are not supposed to exist on that node. The check is done every 30 seconds.</li> <li>Provide container log-file listing and offset based content delivery APIs to container</li> </ul>"},{"location":"cluster/cluster.html#nginx-and-drove-gateway","title":"NGinx and Drove-Gateway","text":"<p>Almost all of the traffic between service containers is routed via the internal Ranger based service discovery system at PhonePe. However, traffic from the edge as well and between different protected environments are routed using the well-established virtual host (and additionally, in some unusual cases, header) based routing.</p> <ul> <li>All applications on Drove can specify a Vhost and a port name as endpoint for such routing.</li> <li>Upstream information for such VHosts or endpoints is available from an API from the leading Drove controller.</li> <li>This information can be used to configure any load-balancer or tourer or reverse proxy to expose applications running on Drove to the outside world.</li> <li> <p>We modified an existing project called Nixy so that it gets the upstream information from Drove instead of Marathon. Nixy plays the following role in a cluster:</p> </li> <li> <p>Track the leader controller for a Drove cluster by making ping calls to all specified controllers</p> </li> <li>Provide a special data structure that can be used by a template to expose a vhost that points to the leader controller in a Drove cluster. This can be used for any tools that need to interact with a Drove cluster for deployments, monitoring as well as callback endpoints for OAuth etc etc</li> <li>Listen to relevant events from the Drove cluster to trigger upstream refresh as necessary</li> <li>Provide data structures that include the vhost, upstream endpoints (host:port) and metadata (application level tags) that can be used to build templates that generate NGinx configurations to enable progressively complicated routing of calls from downstream to upstreams hosted on Drove clusters. Data structure exposed to templates, groups all upstream host:port tuples by using the vhost. This allows for multiple deployments for the same VHost to exist. This is needed for a variety of situations including online-updates of services.</li> <li>Supports username/password based authentication and header based (used internally) to Drove clusters.</li> <li>Support for both NGinx Plus and OSS products. Drove-Nixy can make appropriate api calls to corresponding NGinx plus server to only refresh existing VHost on topology change, as well as affect a full reload when new Vhosts are detected. This ensures that there are no connection drops for critical path applications where NGinx Plus might be used. This also solves the issue of NGinx workers going into a hung state due to frequent reloads on busy clusters like our dev testing environment.</li> </ul> <p>Tip</p> <p>The NGinx deployment is standard across all Drove clusters. However, for clusters that receive a lot of traffic using Nginx, the cluster exposing the VHost for Drove itself might be separated from the one exposing the application virtual hosts to allow for easy scalability of the latter. The template for these are configured differently as needed respectively.</p>"},{"location":"cluster/cluster.html#other-components","title":"Other components","text":"<p>There are a few more components that are used for operational management and observability.</p>"},{"location":"cluster/cluster.html#telegraf","title":"Telegraf","text":"<p>PhonePe\u2019s internal metric management system uses a HTTP based metric collector. Telegraf is installed on all Drove nodes to collect metric from the metric port (Admin connector on Dropwizard) and push that information to our metric ingestion system. This information is then used to build dashboards as well as by our Anomaly detection and alerting systems.</p>"},{"location":"cluster/cluster.html#log-management","title":"Log Management","text":"<p>Drove provides a special logger called drove that can be configured to handle compression rotation and archival of container logs. Such container logs are stored on specialised partitions by application/application-instance-id or by source app name/ task id for application and task instances respectively. PhonePe\u2019s standardised log rotation tools are used to monitor and ship out such logs to our central log management system. The same can be replaced or enhanced by running something like promtail on Drove logs to ship out logs to tools like Grafana Loki.</p>"},{"location":"cluster/setup/controller.html","title":"Setting up Controllers","text":"<p>Controllers are the brains of Drove cluster. For HA, at least 2 controllers should be set up.</p> <p>Please note the following behaviour about controllers:</p> <ul> <li>Only one controller is leader at a time. A leader controller does not relinquish control till the process is stopped or dies</li> <li>A controller process will die when it loses connectivity to Zookeeper</li> <li>The process/container for controller should keep restarting till it gets connectivity</li> <li>All decisions for the cluster are taken by the leader controller only</li> <li>During maintenance and package upgrades etc, it is better to roll changes out on non-leaders first and then do the leader at the end</li> <li>The controller process holds all metadata about the cluster, the current states and other information in memory.<ul> <li>Some of this information is backed by Zookeeper based storage layer</li> <li>The other information is recreated dynamically based on updates from executors</li> </ul> </li> <li>Controllers being down does not affect running containers on executors</li> </ul>"},{"location":"cluster/setup/controller.html#controller-configuration-file-reference","title":"Controller Configuration File Reference","text":"<p>The Drove Controller is written on the Dropwizard framework. The configuration to the service is set using a YAML file which needs to be injected into the container. A typical controller configuration file will look like the following:</p> <pre><code>server: #(1)!\n  applicationConnectors: #(2)!\n    - type: http\n      port: 4000\n  adminConnectors: #(3)!\n    - type: http\n      port: 4001\n  applicationContextPath: / #(4)!\n  requestLog: #(5)!\n    appenders:\n      - type: console\n        timeZone: ${DROVE_TIMEZONE}\n      - type: file\n        timeZone: ${DROVE_TIMEZONE}\n        currentLogFilename: /logs/drove-controller-access.log\n        archivedLogFilenamePattern: /logs/drove-controller-access.log-%d-%i\n        archivedFileCount: 3\n        maxFileSize: 100MiB\n\n\nlogging: #(6)!\n  level: INFO\n  loggers:\n    com.phonepe.drove: ${DROVE_LOG_LEVEL}\n\n  appenders:\n    - type: console #(7)!\n      threshold: ALL\n      timeZone: ${DROVE_TIMEZONE}\n      logFormat: \"%(%-5level) [%date] [%logger{0} - %X{appId}] %message%n\"\n    - type: file #(8)!\n      threshold: ALL\n      timeZone: ${DROVE_TIMEZONE}\n      currentLogFilename: /logs/drove-controller.log\n      archivedLogFilenamePattern: /logs/drove-controller.log-%d-%i\n      archivedFileCount: 3\n      maxFileSize: 100MiB\n      logFormat: \"%(%-5level) [%date] [%logger{0} - %X{appId}] %message%n\"\n      archive: true\n\n\nzookeeper: #(9)!\n  connectionString: ${ZK_CONNECTION_STRING}\n\nclusterAuth: #(10)!\n  secrets:\n  - nodeType: CONTROLLER\n    secret: ${DROVE_CONTROLLER_SECRET}\n  - nodeType: EXECUTOR\n    secret: ${DROVE_EXECUTOR_SECRET}\n\nuserAuth: #(11)!\n  enabled: true\n  users:\n    - username: admin\n      password: ${DROVE_ADMIN_PASSWORD}\n      role: EXTERNAL_READ_WRITE\n    - username: guest\n      password: ${DROVE_GUEST_PASSWORD}\n      role: EXTERNAL_READ_ONLY\n\ninstanceAuth: #(12)!\n  secret: ${DROVE_INSTANCE_AUTH_SECRET}\n\noptions: #(13)!\n  maxStaleInstancesCount: 3\n  staleCheckInterval: 1m\n  staleAppAge: 1d\n  staleInstanceAge: 18h\n  staleTaskAge: 1d\n  clusterOpParallelism: 4\n</code></pre> <ol> <li>Server listener configuration. See Dropwizard Server Configuration for the different options.</li> <li>Main port configuration. This is where the UI and APIs will be exposed. Check connector configuration docs for details.</li> <li>Admin port. You can take thread dumps, metrics, run healthchecks on the Drove controller on this port.</li> <li>Base path for UI. Keep this as is.</li> <li>Access logs configuration. See <code>requestLog</code> docs.</li> <li>Main logging configuration. See logging docs.</li> <li>Log to console. Useful in docker-compose.</li> <li>Log to rotating files. Useful for running servers.</li> <li>Configure how to connect to Zookeeper See Zookeeper Config for details.</li> <li>Configuration for authentication between nodes in the cluster. Please check intra node auth config for details.</li> <li>Configure user authentication to access the cluster. Please check User auth config for details.</li> <li>Signing secret for JWT to be embedded in application and task instances. Check Instance auth config for details.</li> <li>Special options to configure controller behaviour. See Controller Options for details.</li> </ol> <p>Tip</p> <p>In case you do not want to expose admin apis to outside the host, please set <code>bindHost</code> in the admin connectors section.</p> <pre><code>adminConnectors:\n  - type: http\n    port: 10001\n    bindHost: 127.0.0.1\n</code></pre>"},{"location":"cluster/setup/controller.html#zookeeper-connection-configuration","title":"Zookeeper Connection Configuration","text":"<p>The following details can be configured.</p> Name Option Description Connection String <code>connectionString</code> The connection string of the form: <code>zkserver:2181,zkserver2:2181...</code> Data namespace <code>namespace</code> The top level node inside which all Drove data will be scoped. Defaults to <code>drove</code> if not set. <p>Sample</p> <pre><code>zookeeper:\n  connectionString: \"192.168.3.10:2181,192.168.3.11:2181,192.168.3.12:2181\"\n  namespace: drovetest\n</code></pre>"},{"location":"cluster/setup/controller.html#intra-node-authentication-configuration","title":"Intra Node Authentication Configuration","text":"<p>Communication between controller and executor is protected by a shared-secret based authentication. The following configuration is meant to configure this. This section consists of a list of 2 members:</p> <ul> <li>Config for controller to talk to executors</li> <li>Config for executors to talk to controller</li> </ul> <p>Each section consists of the following:</p> Name Option Description Node Type <code>nodeType</code> Type of node in the cluster. Can be <code>CONTROLLER</code> or <code>EXECUTOR</code> Secret <code>secret</code> The actual secret to be passed. <p>Sample <pre><code>clusterAuth:\n  secrets:\n  - nodeType: CONTROLLER\n    secret: ControllerSecretValue\n  - nodeType: EXECUTOR\n    secret: ExecutorSecret\n</code></pre></p> <p>Danger</p> <p>The values are passed in the header as is. Please manage the config file ownership to ensure that the files are not world readable.</p> <p>Tip</p> <p>You can use <code>pwgen -s 32</code> to generate secure random strings for usage as secrets.</p>"},{"location":"cluster/setup/controller.html#user-authentication-configuration","title":"User Authentication Configuration","text":"<p>This section is used to configure user details for human and other systems that need to call Drove APIs or access the Drove UI. This is implemented using basic auth.</p> <p>The configuration consists of:</p> Name Option Description Enabled <code>enabled</code> Enable basic auth for the cluster Encoding <code>encoding</code> The actual encoding of the password. Can be <code>PLAIN</code> or <code>CRYPT</code> Caching <code>cachingPolicy</code> Caching policy for the authentication and authorization of the user. Please check CaffeineSpec docs for more details. Set to <code>maximumSize=500, expireAfterAccess=30m</code> by default List of users <code>users</code> A list of users recognized by the system <p>Each entry in the user list consists of:</p> Name Option Description User Name <code>username</code> The actual login username Password <code>password</code> The password for the user. Needs to be set to bcrypt string of the actual password if <code>encoding</code> is set to <code>CRYPT</code> in the parent section. User Role <code>role</code> The role of the user in the cluster. Can be <code>EXTERNAL_READ_WRITE</code> for users who have both read and write permissions or <code>EXTERNAL_READ_ONLY</code> for users with read-only permissions. <p>Sample <pre><code>userAuth:\n  enabled: true\n  encoding: CRYPT\n  users:\n    - username: admin\n      password: \"$2y$10$pfGnPkYrJEGzasvVNPjRu.IJldV9TDa0Vh.u1UdimILWDuhvapc2O\"\n      role: EXTERNAL_READ_WRITE\n    - username: guest\n      password: \"$2y$10$uCJ7WxIvd13C.1oOTs28p.xpJShGiTWuDLY/sGH9JE8nrkSGBFkc6\"\n      role: EXTERNAL_READ_ONLY\n    - username: noread\n      password: \"$2y$10$8mr/zXL5rMW/s/jlBcgXHu0UvyzfdDDvyc.etfuoR.991sn9UOX/K\"\n</code></pre></p> <p>No authentication</p> <p>To configure a cluster without authentication, remove this section entirely.</p> <p>Operator role</p> <p>If <code>role</code> is not set, the user will be able to access the UI, but will not have access to application logs. This comes in handy to provide access to other teams to explore your deployment topology, but not get access to your logs that might contain sensitive information.</p> <p>Password Hashing</p> <p>We strongly recommend using bcrypt passwords for authentication. You can use the following command to generate hashed password strings:</p> <pre><code>htpasswd -nbBC 10 &lt;username&gt; &lt;password&gt;|cut -d ':' -f2\n</code></pre>"},{"location":"cluster/setup/controller.html#instance-authentication-configuration","title":"Instance Authentication Configuration","text":"<p>All application and task instances, get access to an unique JWT that is injected into it by Drove as the environment variable <code>DROVE_APP_INSTANCE_AUTH_TOKEN</code>. This token is signed using a secret. This secret can be configured by setting the <code>secret</code> parameter in the <code>instanceAuth</code> section.</p> <p>Sample <pre><code>instanceAuth:\n  secret: RandomSecret\n</code></pre></p>"},{"location":"cluster/setup/controller.html#controller-options","title":"Controller Options","text":"<p>The following options can be set to influence the behavior of the Drove cluster and the controller.</p> Name Option Description Stale Check Interval <code>staleCheckInterval</code> Interval at which Drove checks for stale application and task metadata for cleanup. Defaults to 1 hour. Expressed in duration. Stale App Age <code>staleAppAge</code> Apps in <code>MONITORING</code> state are cleaned up after some time by Drove. This variable can be used to control the max time for which such apps are maintained in the cluster. Defaults to 7 days. Expressed in duration. Stale App Instances Count <code>maxStaleInstancesCount</code> Maximum number of application instances metadata for stopped or lost instances to be maintained in the cluster. Defaults to 100. Stale Instance Age <code>staleInstanceAge</code> Maximum age for a stale application instance to be retained. Defaults to 7 days. Expressed in duration. Stale Task Age <code>staleTaskAge</code> Maximum time for which metadata for a finished task is retained on the cluster. Defaults to 2 days. Expressed in duration. Event Storage Duration <code>maxEventsStorageDuration</code> Maximum time for which cluster events are retained on the cluster. Defaults to 1 hour. Expressed in duration. Default Operation Timeout <code>clusterOpTimeout</code> Timeout for operations that are initiated by drove itself. For example, instance spin up in case of executor failure, instance migrations etc. Defaults to 5 minutes. Expressed in duration. Operation threads <code>clusterOpParallelism</code> Signified the parallelism for operations internal to the cluster. Defaults to: 1. Range: 1-32. Audited Methods <code>auditedHttpMethods</code> Drove prints an audit log with user details when an api is called by an user. Defaults to <code>[\"POST\", \"PUT\"]</code>. Allowed mount directories <code>allowedMountDirs</code> If provided, Drove will ensure that application and task spec can mount only the directories mentioned in this set on executor host. Disable read-only auth <code>disableReadAuth</code> When <code>userAuth</code> is enabled, setting this option, will enforce authorization only on write operations. Disable command line arguments <code>disableCmdlArgs</code> When set to <code>true</code>, passing command line arguments will be disabled. Default: <code>false</code> (users can pass arguments. <p>Sample <pre><code>options:\n  staleCheckInterval: 5m\n  staleAppAge: 2d\n  maxStaleInstancesCount: 20\n  staleInstanceAge: 1d\n  staleTaskAge: 2d\n  maxEventsStorageDuration: 30m\n  clusterOpParallelism: 32\n  allowedMountDirs:\n   - /mnt/scratch\n</code></pre></p>"},{"location":"cluster/setup/controller.html#stale-data-cleanup","title":"Stale data cleanup","text":"<p>In order to keep internal memory footprint low, reduce the amount of data stored on Zookeeper, and provide a faster experience on the UI,Drove keeps cleaning up data for stale applications, application instances, task instances and cluster events.</p> <p>The retention for such metadata can be controlled using the following config options:</p> <ul> <li><code>staleAppAge</code></li> <li><code>maxStaleInstancesCount</code></li> <li><code>staleInstanceAge</code></li> <li><code>staleTaskAge</code></li> <li><code>maxEventsStorageDuration</code></li> </ul> <p>Warning</p> <p>Configuration changes done to these parameters will have direct impact on memory usage by the controller and memory and disk utilization on the Zookeeper cluster.</p>"},{"location":"cluster/setup/controller.html#internal-operations","title":"Internal Operations","text":"<p>Drove may need to create and issue operations on applications and tasks to manage cluster stability, for maintenance and other reasons. The following parameters can be used to control the speed and parallelism of such operations:</p> <ul> <li><code>clusterOpTimeout</code></li> <li><code>clusterOpParallelism</code></li> </ul> <p>Tip</p> <p>The default value of <code>1</code> for the <code>clusterOpParallelism</code> parameter is generally too low for most clusters. Unless there is a specific problem, it would be advisable to set this to at least 4. If number of instances is quite high for applications (order of tens or hundreds), feel free to set this to 32.</p> <p>Increasing <code>clusterOpParallelism</code> will make recovery faster in case of executor failures, but it will increase cpu utilization on the controller by a little bit.</p>"},{"location":"cluster/setup/controller.html#security-related-options","title":"Security related options","text":"<p>The <code>auditedHttpMethods</code> parameter contains a list of all HTTP methods that need to be audited. This means that if the <code>auditedHttpMethods</code> contains <code>POST</code> and <code>PUT</code>, any drove HTTP POST or PUT apis being called will lead to a audit in the controller logs with the details of the user that made the call.</p> <p>Warning</p> <p>It would be advisable to not add <code>GET</code> to the list. This is because the UI keeps making calls to <code>GET</code> apis on drove to fetch data to render. These calls are automated and happen every few seconds from the browser. This will blow up controller logs size.</p> <p>The <code>allowedMountDirs</code> option whitelists only some directories to be mounted on containers. If this is not provided, containers will be able to mount any directory on the executors. </p> <p>Danger</p> <p>It is highly recommended to set <code>allowedMountDirs</code> to a designated directory that containers might want to use as scratch space if needed. Keeping this empty will almost definitely cause security issues in the long run.</p>"},{"location":"cluster/setup/controller.html#relevant-directories","title":"Relevant directories","text":"<p>Location for data and logs are as follows:</p> <ul> <li><code>/etc/drove/controller/</code> - Configuration files</li> <li><code>/var/log/drove/controller/</code> - Logs</li> </ul> <p>We shall be volume mounting the config and log directories with the same name.</p> <p>Prerequisite Setup</p> <p>If not done already, please complete the prerequisite setup on all machines earmarked for the cluster.</p>"},{"location":"cluster/setup/controller.html#setup-the-config-file","title":"Setup the config file","text":"<p>Create a relevant configuration file in <code>/etc/drove/controller/controller.yml</code>.</p> <p>Sample <pre><code>server:\n  applicationConnectors:\n    - type: http\n      port: 10000\n  adminConnectors:\n    - type: http\n      port: 10001\n  requestLog:\n    appenders:\n      - type: file\n        timeZone: IST\n        currentLogFilename: /var/log/drove/controller/drove-controller-access.log\n        archivedLogFilenamePattern: /var/log/drove/controller/drove-controller-access.log-%d-%i\n        archivedFileCount: 3\n        maxFileSize: 100MiB\n\nlogging:\n  level: INFO\n  loggers:\n    com.phonepe.drove: INFO\n\n\n  appenders:\n    - type: file\n      threshold: ALL\n      timeZone: IST\n      currentLogFilename: /var/log/drove/controller/drove-controller.log\n      archivedLogFilenamePattern: /var/log/drove/controller/drove-controller.log-%d-%i\n      archivedFileCount: 3\n      maxFileSize: 100MiB\n      logFormat: \"%(%-5level) [%date] [%logger{0} - %X{appId}] %message%n\"\n\nzookeeper:\n  connectionString: \"192.168.56.10:2181\"\n\nclusterAuth:\n  secrets:\n  - nodeType: CONTROLLER\n    secret: \"0v8XvJrDc7r86ZY1QCByPTDPninI4Xii\"\n  - nodeType: EXECUTOR\n    secret: \"pOd9sIEXhv0wrGOVc7ebwNvR7twZqyTN\"\n\nuserAuth:\n  enabled: true\n  encoding: CRYPT\n  users:\n    - username: admin\n      password: \"$2y$10$pfGnPkYrJEGzasvVNPjRu.IJldV9TDa0Vh.u1UdimILWDuhvapc2O\"\n      role: EXTERNAL_READ_WRITE\n    - username: guest\n      password: \"$2y$10$uCJ7WxIvd13C.1oOTs28p.xpJShGiTWuDLY/sGH9JE8nrkSGBFkc6\"\n      role: EXTERNAL_READ_ONLY\n\n\ninstanceAuth:\n  secret: \"bd2SIgz9OMPG2L8wA6zxj21oLVLbuLFC\"\n\noptions:\n  maxStaleInstancesCount: 3\n  staleCheckInterval: 1m\n  staleAppAge: 2d\n  staleInstanceAge: 1d\n  staleTaskAge: 1d\n  clusterOpParallelism: 4\n  allowedMountDirs:\n   - /dev/null\n</code></pre></p>"},{"location":"cluster/setup/controller.html#setup-required-environment-variables","title":"Setup required environment variables","text":"<p>Environment variables need to run the drove controller are setup in <code>/etc/drove/controller/controller.env</code>.</p> <pre><code>CONFIG_FILE_PATH=/etc/drove/controller/controller.yml\nJAVA_PROCESS_MIN_HEAP=2g\nJAVA_PROCESS_MAX_HEAP=2g\nZK_CONNECTION_STRING=\"192.168.3.10:2181\"\nJAVA_OPTS=\"-Xlog:gc:/var/log/drove/controller/gc.log -Xlog:gc:::filecount=3,filesize=10M -Xlog:gc::time,level,tags -XX:+UseNUMA -XX:+ExitOnOutOfMemoryError -Djava.security.egd=file:/dev/urandom -Dfile.encoding=utf-8 -Djute.maxbuffer=0x9fffff\"\n</code></pre>"},{"location":"cluster/setup/controller.html#create-systemd-file","title":"Create systemd file","text":"<p>Create a <code>systemd</code> file. Put the following in <code>/etc/systemd/system/drove.controller.service</code>:</p> <pre><code>[Unit]\nDescription=Drove Controller Service\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nUser=drove\nGroup=docker\nTimeoutStartSec=0\nRestart=always\nExecStartPre=-/usr/bin/docker pull ghcr.io/phonepe/drove-controller:latest\nExecStart=/usr/bin/docker run  \\\n    --env-file /etc/drove/controller/controller.env \\\n    --volume /etc/drove/controller:/etc/drove/controller:ro \\\n    --volume /var/log/drove/controller:/var/log/drove/controller \\\n    --publish 10000:10000  \\\n    --publish 10001:10001 \\\n    --hostname %H \\\n    --rm \\\n    --name drove.controller \\\n    ghcr.io/phonepe/drove-controller:latest\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Verify the file with the following command: <pre><code>systemd-analyze verify drove.controller.service\n</code></pre></p> <p>Set permissions <pre><code>chmod 664 /etc/systemd/system/drove.controller.service\n</code></pre></p>"},{"location":"cluster/setup/controller.html#start-the-service-on-all-servers","title":"Start the service on all servers","text":"<p>Use the following to start the service:</p> <pre><code>systemctl daemon-reload\nsystemctl enable drove.controller\nsystemctl start drove.controller\n</code></pre> <p>You can tail the logs at <code>/var/log/drove/controller/drove-controller.log</code>.</p> <p>The console would be available at <code>http://&lt;ip&gt;:10000</code> and admin functionality will be available on <code>http://&lt;ip&gt;:10001</code> according to the above config.</p> <p>Health checks can be performed by running a curl as follows:</p> <pre><code>curl http://localhost:10001/healthcheck\n</code></pre> <p>Note</p> <ul> <li>The healthcheck check api is available on <code>admin</code> port.</li> <li>HTTP status is 200/OK if things are fine.</li> </ul> <p>Once controllers are up, one of them will become the leader. You can check the leader by running the following command:</p> <pre><code>curl http://&lt;ip&gt;:10000/apis/v1/ping\n</code></pre> <p>Only on the leader you should get the following response along with a HTTP status 200/OK: <pre><code>{\n    \"status\":\"SUCCESS\",\n    \"data\":\"pong\",\n    \"message\":\"success\"\n}\n</code></pre></p>"},{"location":"cluster/setup/coredns-drove.html","title":"Setting up CoreDNS Drove","text":"<p>The Drove Plugin for CoreDNS allows for DNS based service discovery of application endpoints deployed on a drove cluster.</p> <p>We provide a Docker to deploy Drove plugin enabled CoreDNS easily. More details about this can be found in the coredns-drove project.</p>"},{"location":"cluster/setup/coredns-drove.html#coredns-drove-plugin-options-reference","title":"CoreDNS Drove plugin options reference","text":"<p>This section discusses the configuration options for the Drove plugin. For detailed coredns options please refer to the coredns manual.</p> <p>The plugin configuration is part of the <code>Corefile</code>, the configuration file taken as input by CoreDNS. To use the provided container the configuration can be provided in one of two ways: - For a standard simpler setup, environment variables can be provided to the container to set it up - For more complicated setups mounting a Corefile is the way to go</p> <p>We adopt the latter in this section as it provides maximum flexibility.</p> <pre><code>.:1053 {\n    log\n    whoami\n    cache 30\n    ready\n    prometheus\n    drove {\n        endpoint \"http://controller1.mydomain:10000,http://controller1.mydomain:10000\" # (1)!\n        access_token \"&lt;DROVE_ACCESS_TOKEN&gt;\" # (2)!\n        user_pass guest guest # (3)!\n        gateway \"gateway1.mydomain,gateway2.mydomain\" #(4)!\n        skip_ssl_check #(5)!\n    }\n    forward . 1.1.1.1 #(6)!\n}\n</code></pre> <ol> <li>List of controller endpoints</li> <li>Access token for drove controller if using access token based access control</li> <li>Drove username and password if using basic auth</li> <li>List of Drove Gateway IP addresses or hostnames</li> <li>Skip SSL checking on controller endpoints</li> <li>Coredns option to forward non matching/global requests to upstream</li> </ol> <p>Note</p> <p>The above config will get the job done, but will forward non-matching requests to upstream (<code>1.1.1.1</code> in this case). Please go through coredns manual to create better configuration if you want.</p>"},{"location":"cluster/setup/coredns-drove.html#relevant-directories","title":"Relevant directories","text":"<p>Location for data and logs are as follows:</p> <ul> <li><code>/etc/drove/dns/</code> - Configuration files</li> </ul> <p>We shall be volume mounting the Corefile directly into the container.</p> <p>Prerequisite Setup</p> <p>If not done already, please complete the prerequisite setup on all machines earmarked for the cluster.</p> <p>Go through the following steps to run <code>drove-dns</code> as a service.</p>"},{"location":"cluster/setup/coredns-drove.html#create-the-corefile-for-coredns","title":"Create the Corefile for CoreDNS","text":"<p>Sample config file <code>/etc/drove/dns/Corefile</code>:</p> <pre><code>.:1053 {\n    log\n    whoami\n    cache 30\n    ready\n    prometheus\n    drove {\n        # Put correct http(s) endpoints for controllers below\n        endpoint \"http://controller1.mydomain:10000,http://controller1.mydomain:10000\"\n        user_pass guest guest\n\n        # Put correct hostname list below\n        gateway \"gateway1.mydomain,gateway2.mydomain\"\n        skip_ssl_check\n    }\n    forward . 1.1.1.1\n}\n</code></pre> <p>Port</p> <p>The docker container exposes port <code>1053</code> for DNS lookups.</p> <p>Replace hostnames and IPs</p> <p>Please remember to update <code>endpoint</code> and <code>gateway</code> values.</p>"},{"location":"cluster/setup/coredns-drove.html#create-systemd-file","title":"Create systemd file","text":"<p>Create a <code>systemd</code> file. Put the following in <code>/etc/systemd/system/drove.gateway.service</code>:</p> <pre><code>[Unit]\nDescription=Drove Enabled CoreDNS Service\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nUser=drove\nGroup=docker\nTimeoutStartSec=0\nRestart=always\nExecStartPre=-/usr/bin/docker pull ghcr.io/phonepe/coredns-drove:latest\nExecStart=/usr/bin/docker run  \\\n    --volume /etc/drove/dns/Corefile:/opt/Corefile:ro \\\n    --network host \\\n    --hostname %H \\\n    --rm \\\n    --name drove.dns \\\n    ghcr.io/phonepe/coredns-drove:latest\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Verify the file with the following command: <pre><code>systemd-analyze verify drove.dns.service\n</code></pre></p> <p>Set permissions <pre><code>chmod 664 /etc/systemd/system/drove.dns.service\n</code></pre></p>"},{"location":"cluster/setup/coredns-drove.html#start-the-service-on-all-servers","title":"Start the service on all servers","text":"<p>Use the following to start the service:</p> <pre><code>systemctl daemon-reload\nsystemctl enable drove.dns\nsystemctl start drove.dns\n</code></pre>"},{"location":"cluster/setup/coredns-drove.html#checking-logs","title":"Checking Logs","text":"<p>You can check logs using: <pre><code>journalctl -u drove.dns -f\n</code></pre></p>"},{"location":"cluster/setup/coredns-drove.html#testing-the-setup","title":"Testing the setup","text":"<p>You need to test to confirm the setup is working as expected. </p> <p>Note</p> <p>Since this server is not configured system wide, we shall pass <code>@127.0.0.1</code> and <code>-p1053</code> options to <code>dig</code> to test name lookups.</p>"},{"location":"cluster/setup/coredns-drove.html#test-global-lookups","title":"Test global lookups","text":"<p>We lookup the ip for mozilla.org  using the following command:</p> <pre><code> dig  @127.0.0.1 -p1053 mozilla.org\n</code></pre> <p>Sample output: <pre><code>; &lt;&lt;&gt;&gt; DiG 9.18.28-0ubuntu0.22.04.1-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.1 -p1053 mozilla.org\n; (1 server found)\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 7871\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;mozilla.org.           IN  A\n\n;; ANSWER SECTION:\nmozilla.org.        373 IN  A   35.190.14.201\n\n;; Query time: 8 msec\n;; SERVER: 127.0.0.1#1053(127.0.0.1) (UDP)\n;; WHEN: Sat Jan 18 00:57:18 UTC 2025\n;; MSG SIZE  rcvd: 67\n</code></pre></p> <p>As you can see the IP <code>35.190.14.201</code> is returned in the <code>ANSWER SECTION</code> of the output.</p>"},{"location":"cluster/setup/coredns-drove.html#test-simple-application-vhost-lookup","title":"Test simple application vhost lookup","text":"<p>Let's assume an application is deployed with <code>testapp.localtest</code> as the Vhost. We are going to do a simple A record lookup to get the IP.</p> <pre><code> dig  @127.0.0.1 -p1053 testapp.localtest\n</code></pre> <p>Sample output: <pre><code>; &lt;&lt;&gt;&gt; DiG 9.18.28-0ubuntu0.22.04.1-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.1 -p1053 testapp.localtest\n; (1 server found)\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 51801\n;; flags: qr rd ra ad; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;testapp.localtest.     IN  A\n\n;; ANSWER SECTION:\ntestapp.localtest.  30  IN  A   192.168.56.10\n\n;; AUTHORITY SECTION:\n.           86400   IN  SOA a.root-servers.net. nstld.verisign-grs.com. 2025011702 1800 900 604800 86400\n\n;; Query time: 12 msec\n;; SERVER: 127.0.0.1#1053(127.0.0.1) (UDP)\n;; WHEN: Sat Jan 18 01:01:39 UTC 2025\n;; MSG SIZE  rcvd: 154\n</code></pre></p> <p>The <code>ANSWER SECTION</code> contains one of the gateway ips as provided in the Corefile (<code>192.168.56.10</code> in the above example).</p>"},{"location":"cluster/setup/coredns-drove.html#test-srv-lookup","title":"Test SRV lookup","text":"<p>SRV record lookups help clients get the host and port combination for the upstreams defined for a Drove application. The clients can use custom logic to talk directly to such application instances completely bypassing the Drove Gateway, thereby eliminating a single point of failure.</p> <pre><code>dig  @127.0.0.1 -p1053 testapp.localtest SRV\n</code></pre> <p>Sample output <pre><code>; &lt;&lt;&gt;&gt; DiG 9.18.28-0ubuntu0.22.04.1-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.1 -p1053 testapp.localtest SRV\n; (1 server found)\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 57387\n;; flags: qr rd ra ad; QUERY: 1, ANSWER: 5, AUTHORITY: 1, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;testapp.localtest.     IN  SRV\n\n;; ANSWER SECTION:\ntestapp.localtest.  30  IN  SRV 1 1 33541 d-exctr.\ntestapp.localtest.  30  IN  SRV 1 1 40891 d-exctr.\ntestapp.localtest.  30  IN  SRV 1 1 39427 d-exctr.\ntestapp.localtest.  30  IN  SRV 1 1 39219 d-exctr.\ntestapp.localtest.  30  IN  SRV 1 1 37681 d-exctr.\n\n;; AUTHORITY SECTION:\n.           86400   IN  SOA a.root-servers.net. nstld.verisign-grs.com. 2025011702 1800 900 604800 86400\n\n;; Query time: 12 msec\n;; SERVER: 127.0.0.1#1053(127.0.0.1) (UDP)\n;; WHEN: Sat Jan 18 01:06:37 UTC 2025\n;; MSG SIZE  rcvd: 341\n</code></pre></p> <p>The <code>ANSWER SECTION</code> contains 5 {port:host} combinations corresponding to the 5 instances deployed for this application on the executor named <code>d-exctr</code>.</p> <p>All tests are successful!!</p>"},{"location":"cluster/setup/coredns-drove.html#using-drovedns-for-system-wide-dns-resolution","title":"Using DroveDNS for system wide DNS resolution","text":"<p>Given the above configuration we can use Drove DNS as the resolver across all systems. The following sections use Ubuntu based servers for reference commands, other distributions should have similar course of actions.</p> <p>In order to achieve system wide DNs resolution, we need to configure <code>systemd-resolved</code>. This is achieved by modifying the <code>/etc/systemd/resolved.conf</code> file.</p> <p>Open the <code>vim /etc/systemd/resolved.conf</code> file in your favourite text editor and go the section called <code>[Resolve]</code>. Add your Drove DNS server(s)' IPs and ports in the <code>DNS</code> field.</p> <pre><code># ... other stuff\n[Resolve]\n# ...\nDNS=192.168.56.10:1053\n#FallbackDNS=\n#...\n</code></pre> <p>Note</p> <p>Multiple IP:port combinations can be provided</p> <p>That's it, the configuration is ready.</p> <p>Restart the resolver using the following commands: <pre><code>service systemd-resolved restart\n</code></pre></p>"},{"location":"cluster/setup/coredns-drove.html#validating-the-setup","title":"Validating the setup","text":"<p>Ensure the settings have taken effect by issueing the command: <pre><code>resolvectl status\n</code></pre></p> <p>In the output of the <code>Global</code> section should look something similar to the following:</p> <pre><code>Global\n         Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\n  resolv.conf mode: foreign\nCurrent DNS Server: 192.168.56.10:1053\n       DNS Servers: 192.168.56.10:1053\n</code></pre> <p>If your configured IPs show up, you can run the same set of tests as before without the ip and port to ensure name lookup is working as expected.</p> <pre><code>dig mozilla.org #(1)!\ndig testapp.localtest #(2)!\ndig testapp.localtest SRV #(3)!\n</code></pre> <ol> <li>Global lookup</li> <li>A record lookup</li> <li>SRV lookup</li> </ol>"},{"location":"cluster/setup/executor-setup.html","title":"Setting up Executor Nodes","text":"<p>We shall setup the executor nodes by setting up the hardware, operating system first and then the executor service itself.</p>"},{"location":"cluster/setup/executor-setup.html#considerations-and-tuning-for-hardware-and-operating-system","title":"Considerations and tuning for hardware and operating system","text":"<p>In the following sections we discus some aspects of scheduling, hardware and settings on the OS to ensure good performance.</p>"},{"location":"cluster/setup/executor-setup.html#cpu-and-memory-considerations","title":"CPU and Memory considerations","text":"<p>The executor nodes are the servers that host and run the actual docker containers. Drove will take into consideration the NUMA topology of these machines to optimize the placement for containers to extract the maximum performance. Along with this, Drove will <code>cpuset</code> the containers to the allocated cores in a non overlapping manner, so that the cores allocated to a container are dedicated to it. Memory allocated to a container is pinned as well and selected from the same NUMA node.</p> <p>Needless to say the minimum amount of CPU that can be given to an application or task is 1. Fractional cpu allocation can be achieved in a predictable manner by configuring over provisioning on executor nodes.</p>"},{"location":"cluster/setup/executor-setup.html#over-provisioning-of-cpu-and-memory","title":"Over Provisioning of CPU and Memory","text":"<p>Drove does not do any kind of burst scaling or overcommitment to ensure application performance remains predictable even under load. Instead, in Drove, there is a feature to make executors appear to have more cores (and memory) than it actually has. This can be used to get more utilization out of executor nodes in clusters that do not need guaranteed performance (for example staging or dev testing clusters). This is achieved by enabling over provisioning.</p> <p>Over provisioning needs to be configured in the executor configuration. It primarily consists of two configs:</p> <ul> <li>CPU Multiplier - an integral multiplier which will be used to multiply the number of available cores</li> <li>Memory Multiplier - an integral multiplier which will be used to multiply available memory</li> </ul> <p>VCores (virtual cores) are internal representation of a CPU core on the executor. If over provisioning is disabled, a vcore will correspond to a physical core. If over provisioning is enabled, 1 CPU core will generate <code>cpu multiplier</code> number of v cores. Drove does do <code>cpuset</code> even on containers running on nodes that have over provisioning enabled, however the physical cores that the containers get bound to are chosen at random, albeit from the same NUMA node. <code>cpuset-mem</code> is always done on the same NUMA node as well.</p> <p>Mixed clusters</p> <p>In some production clusters you might have applications that are non critical in terms of performance and are unable to utilize a full core. These can be tagged to be spun up on some nodes where over provisioning is enabled. Adopting such a cluster topology will ensure that containers that need high performance run on nodes without over provisioning and the smaller apps (like for example operations consoles etc) are run on separate nodes with over provisioning enabled. Just ensure the latter are tagged properly and during app deployment specify this tag in application spec or task spec.</p>"},{"location":"cluster/setup/executor-setup.html#disable-numa-pinning","title":"Disable NUMA Pinning","text":"<p>There is an option to disable memory and core pinning. In this situation, all cores from all NUM nodes show up as being part of one node. <code>cpuset-mems</code> is not called if numa pinning is disabled and therefore you will be leaving some memory performance on the table. We recommend not to dabble with this unless you have tasks and containers that need more than the number of cores available on a single NUMA node. This setting is enabled at executor level by setting <code>disableNUMAPinning: true</code>.</p>"},{"location":"cluster/setup/executor-setup.html#hyper-threading","title":"Hyper-threading","text":"<p>Whether Hyper Threading needs to be enabled or not is a bit dependent on applications deployed and how effectively they can utilize individual CPU cores. For mixed workloads, we recommend Hyper Threading to be enabled on the executor nodes.</p>"},{"location":"cluster/setup/executor-setup.html#isolating-container-and-os-processes","title":"Isolating container and OS processes","text":"<p>Typically we would not want containers to share CPU resources with processes for the operating system, Drove Executor Service as well as Docker engine (if using docker) and so on. While complete isolation would need creating a full scheduler (and passing <code>isolcpus</code> to GRUB parameters), we can get a good middle ground by ensuring such processes utilize only a few CPU cores on the system, and let the Drove executors deploy and pin containers to the rest.</p> <p>This is achieved in two steps:</p> <ul> <li>Make changes to systemd to use only specific cores</li> <li>Exclude these cores in the drove executor configuration</li> </ul> <p>Let's say our server has 2 NUMA nodes, each with 40 hyper-threaded cores. We want to reserve the first 2 cores from each CPU to the OS processes. So we reserve cores <code>[0,1,2,3]</code> for the OS processes.</p> <p>The following line in <code>/etc/systemd/system.conf</code></p> <pre><code>#CPUAffinity=\n</code></pre> <p>needs to be changed to</p> <pre><code>CPUAffinity=0 1 2 3\n</code></pre> <p>Tip</p> <p>Reboot the machine for this to take effect.</p> <p>The changes can be validated post reboot by running the following command:</p> <pre><code>grep Cpus_allowed_list /proc/1/status\n</code></pre> <p>The expected output should be: <pre><code>Cpus_allowed_list:  0-3\n</code></pre></p> <p>Note</p> <p>Refer to this for more details.</p>"},{"location":"cluster/setup/executor-setup.html#gpu-computation","title":"GPU Computation","text":"<p>Nvidia based GPU compute can be enabled at executor level by installing relevant drivers. Please follow the setup guide to enable this. Remember to tag these nodes to isolate them from the primary cluster and use tags to deploy apps and tasks that need GPU.</p>"},{"location":"cluster/setup/executor-setup.html#storage-consideration","title":"Storage consideration","text":"<p>On executor nodes the disk might be under pressure if container (re)deployments are frequent or the containers log very heavily. As such, we recommend the logging directory for Drove be mounted on hardware that will be able to handle this load. Similar considerations need to be given to the log and package directory for docker or podman.</p>"},{"location":"cluster/setup/executor-setup.html#executor-configuration-reference","title":"Executor Configuration Reference","text":"<p>The Drove Executor is written on the Dropwizard framework. The configuration to the service is set using a YAML file which needs to be injected into the container. A typical controller configuration file will look like the following:</p> <pre><code>server: #(1)!\n  applicationConnectors: #(2)!\n    - type: http\n      port: 3000\n  adminConnectors: #(3)!\n    - type: http\n      port: 3001\n  applicationContextPath: /\n  requestLog:\n    appenders:\n      - type: console\n        timeZone: ${DROVE_TIMEZONE}\n      - type: file\n        timeZone: ${DROVE_TIMEZONE}\n        currentLogFilename: /logs/drove-executor-access.log\n        archivedLogFilenamePattern: /logs/drove-executor-access.log-%d-%i\n        archivedFileCount: 3\n        maxFileSize: 100MiB\n\nlogging:\n  level: INFO\n  loggers:\n    com.phonepe.drove: ${DROVE_LOG_LEVEL}\n\n  appenders: #(4)!\n    - type: console #(5)!\n      threshold: ALL\n      timeZone: ${DROVE_TIMEZONE}\n      logFormat: \"%(%-5level) [%date] [%logger{0} - %X{instanceLogId}] %message%n\"\n    - type: file #(6)!\n      threshold: ALL\n      timeZone: ${DROVE_TIMEZONE}\n      currentLogFilename: /logs/drove-executor.log\n      archivedLogFilenamePattern: /logs/drove-executor.log-%d-%i\n      archivedFileCount: 3\n      maxFileSize: 100MiB\n      logFormat: \"%(%-5level) [%date] [%logger{0} - %X{appId}] %message%n\"\n      archive: true\n\n    - type: drove #(7)!\n      logPath: \"/logs/applogs/\"\n      archivedLogFileSuffix: \"%d\"\n      archivedFileCount: 3\n      threshold: TRACE\n      timeZone: ${DROVE_TIMEZONE}\n      logFormat: \"%(%-5level) | %-23date | %-30logger{0} | %message%n\"\n      archive: true\n\nzookeeper: #(8)!\n  connectionString: ${ZK_CONNECTION_STRING}\n\nclusterAuth: #(9)!\n  secrets:\n  - nodeType: CONTROLLER\n    secret: ${DROVE_CONTROLLER_SECRET}\n  - nodeType: EXECUTOR\n    secret: ${DROVE_EXECUTOR_SECRET}\n\nresources: #(10)!\n  osCores: [ 0, 1 ]\n  exposedMemPercentage: 60\n  disableNUMAPinning: ${DROVE_DISABLE_NUMA_PINNING}\n  enableNvidiaGpu: ${DROVE_ENABLE_NVIDIA_GPU}\n\noptions: #(11)!\n  cacheImages: true\n  maxOpenFiles: 10_000\n  logBufferSize: 5m\n  cacheFileSize: 10m\n  cacheFileCount: 3\n</code></pre> <ol> <li>Server listener configuration. See Dropwizard Server Configuration for the different options.</li> <li>Main port configuration. This is where the UI and APIs will be exposed. Check connector configuration docs for details.</li> <li>Admin port. You can take thread dumps, metrics, run healthchecks on the Drove controller on this port.</li> <li>Logging configuration. See logging docs.</li> <li>Log to console. Useful in docker-compose.</li> <li>Log to rotating files. Useful for running servers.</li> <li>Drove application logger configuration. See drove logger config for details.</li> <li>Configure how to connect to Zookeeper See Zookeeper Config for details.</li> <li>Configuration for authentication between nodes in the cluster. Please check intra node auth config for details.</li> <li>Resource configuration for this node.</li> <li>Options to configure executor behaviour. Check executor options section for details.</li> </ol> <p>Tip</p> <p>In case you do not want to expose admin apis to outside the host, please set <code>bindHost</code> in the admin connectors section.</p> <pre><code>adminConnectors:\n  - type: http\n    port: 10001\n    bindHost: 127.0.0.1\n</code></pre>"},{"location":"cluster/setup/executor-setup.html#zookeeper-connection-configuration","title":"Zookeeper Connection Configuration","text":"<p>The following details can be configured.</p> Name Option Description Connection String <code>connectionString</code> The connection string of the form: <code>zkserver:2181,zkserver2:2181...</code> Data namespace <code>namespace</code> The top level node inside which all Drove data will be scoped. Defaults to <code>drove</code> if not set. <p>Sample</p> <pre><code>zookeeper:\n  connectionString: \"192.168.3.10:2181,192.168.3.11:2181,192.168.3.12:2181\"\n  namespace: drovetest\n</code></pre> <p>Note</p> <p>This section is same across the cluster including both controller and executor.</p>"},{"location":"cluster/setup/executor-setup.html#intra-node-authentication-configuration","title":"Intra Node Authentication Configuration","text":"<p>Communication between controller and executor is protected by a shared-secret based authentication. The following configuration is meant to configure this. This section consists of a list of 2 members:</p> <ul> <li>Config for controller to talk to executors</li> <li>Config for executors to talk to controller</li> </ul> <p>Each section consists of the following:</p> Name Option Description Node Type <code>nodeType</code> Type of node in the cluster. Can be <code>CONTROLLER</code> or <code>EXECUTOR</code> Secret <code>secret</code> The actual secret to be passed. <p>Sample <pre><code>clusterAuth:\n  secrets:\n  - nodeType: CONTROLLER\n    secret: ControllerSecretValue\n  - nodeType: EXECUTOR\n    secret: ExecutorSecret\n</code></pre></p> <p>Note</p> <p>This section is same across the cluster including both controller and executor.</p>"},{"location":"cluster/setup/executor-setup.html#drove-application-logger-configuration","title":"Drove Application Logger Configuration","text":"<p>Drove will segregate application and task instance logs in a directory of your choice. The path for such files is set as: - <code>&lt;application id&gt;/&lt;instance id&gt;</code> for Application Instances - <code>&lt;sourceAppName&gt;/&lt;task id&gt;</code> for Task Instances</p> <p>The Drove Log Appender is based of LogBack's Sifting Appender.</p> <p>The following configuration options are supported:</p> Name Option Description Path <code>logPath</code> Directory to host the logs Archive old logs <code>archive</code> Whether to enable log rotation Archived File Suffix <code>archivedLogFileSuffix</code> Suffix for archived log files. Archived File Count <code>archivedFileCount</code> Count of archived log files. Older files are deleted. File Size <code>maxFileSize</code> Size of current log file after which it is archived and a new file is created. Unit: DataSize. Total Size <code>totalSizeCap</code> total size after which deletion takes place. Unit: DataSize. Buffer Size <code>bufferSize</code> Buffer size for the logger. (Set to 8KB by default). Used if <code>immediateFlush</code> is turned off. Immediate Flush <code>immediateFlush</code> Flush logs immediately. Set to <code>true</code> by default (recommended) <p>Sample <pre><code>logging:\n  level: INFO\n  ...\n\n  appenders:\n    # Setup appenders for the executor process itself first\n    ...\n\n    - type: drove\n      logPath: \"/logs/applogs/\"\n      archivedLogFileSuffix: \"%d\"\n      archivedFileCount: 3\n      threshold: TRACE\n      timeZone: ${DROVE_TIMEZONE}\n      logFormat: \"%(%-5level) | %-23date | %-30logger{0} | %message%n\"\n      archive: true\n</code></pre></p>"},{"location":"cluster/setup/executor-setup.html#resource-configuration","title":"Resource Configuration","text":"<p>This section can be used to configure how resources are exposed from an executor to the cluster. We have discussed a few of the considerations that will drive the configuration that is being setup.</p> Name Option Description OS Cores <code>osCores</code> A list of cores reserved for use by operating system processes. See the relevant section for details on the pre-steps needed to achieve this. Exposed Memory <code>exposedMemPercentage</code> What percentage of the system memory can be used by the containers running on the host collectively. Range: 50-100 <code>integer</code> NUMA Pinning <code>disableNUMAPinning</code> Disable NUMA and CPU core pinning for containers. Pinning is on by default. (default: <code>false</code>) Nvidia GPU <code>enableNvidiaGpu</code> Enable GPU support on containers. This setting makes all available Nvidia GPUs on the current executor machine available for any container running on this executor. GPU resources are not discovered on the executor, managed and rationed between containers. Needs to be used in conjunction with tagging (see <code>tags</code> below) to ensure only the applications which require a GPU end up on the executor with GPUs. Tags <code>tags</code> A set of strings that can be used in <code>TAG</code> placement policy to route application and task instances to this executor. Over Provisioning <code>overProvisioning</code> Setup over provisioning configuration. <p>Tagging</p> <p>The current hostname is always added as a tag by default and is handled specially to allow for non-tagged deployments to be routed to this executor. If any tag is specified in the <code>tags</code> config, this node will receive containers only when <code>MATCH_TAG</code> placement is used. Please check relevant sections to specify correct placement policies for applications and tasks.</p> <p>Sample <pre><code>resources:\n  osCores: [0,1,2,3]\n  exposedMemPercentage: 90\n</code></pre></p>"},{"location":"cluster/setup/executor-setup.html#over-provisioning-configuration","title":"Over provisioning configuration","text":"<p>Drove strives to ensure that containers can run unencumbered on CPU cores allocated to them. This means that the minimum allocation unit possible is <code>1</code> for cores. It does not support fractional CPU.</p> <p>However, there are situations where we would want some non-critical applications to run the cluster but not waste CPU. The <code>overProvisioning</code> configuration aims to provide user a way to turn off NUMA pinning on the executor and run more containers than it normally would.</p> <p>To ensure predictability, we do not want pinned and non-pinned containers running on the same host. Hence, an executor host can either be running in pinned mode or in non-pinned mode.</p> <p>To enable more containers than we could usually deploy and to still retain some level of control on how small you want a container to go, we specify multipliers on CPU and memory.</p> <p>Example: - Let's say your executor server has 40 cores available. If you set <code>cpuMultiplier</code> as 4, this node will now show up as having 160 cores to the controller. - Let's say your server had 512GB of memory, setting <code>memoryMultiplier</code> to 2 will make drove see it as 1TB.</p> Name Option Description Enabled <code>enabled</code> Set this to true to enable over provisioning. Default: <code>false</code> CPU Multiplier <code>cpuMultiplier</code> Multiplier to be applied to enable cpu over provisioning. Default: <code>1</code>. Range: 1-20 Memory Multiplier <code>memoryMultiplier</code> Multiplier to be applied to enable memory over provisioning. Default: <code>1</code>. Range: 1-20 <p>Sample <pre><code>resources:\n  exposedMemPercentage: 90\n  overProvisioning:\n    enabled: true\n    memoryMultiplier: 1\n    cpuMultiplier: 3\n</code></pre></p> <p>Tip</p> <p>This feature was developed to allow us to run our development environments cheaper. In such environments there is not much pressure on CPU or memory, but a large number of containers run as developers can spin up containers for features they are working on. There was no point is wasting a full core on containers that get hit twice a minute or less. On production we tend to err on the side of caution and allocate at least one core even to the most trivial applications as of the time of writing this.</p>"},{"location":"cluster/setup/executor-setup.html#executor-options","title":"Executor Options","text":"<p>The following options can be set to influence the behavior for the Drove executors.</p> Name Option Description Hostname <code>hostname</code> Override the hostname that gets exposed to the controller. Make sure this is resolvable. Cache Images <code>cacheImages</code> Cache container images. If this is not passed, a container image is removed when a container dies and no other instance is using the image. Command Timeout <code>containerCommandTimeout</code> Timeout used by the container engine client when issuing container commands to <code>docker</code> or <code>podman</code> Container Socket Path <code>dockerSocketPath</code> The path of socket for docker socket. Comes in handy to configure path for socket when using <code>podman</code> etc. Max Open Files <code>maxOpenFiles</code> Override the maximum number of file descriptors a container can open. Default: 470,000 Log Buffer Size <code>logBufferSize</code> The size of the buffer the executor uses to read logs from container. Unit DataSize. Range: 1-128MB. Default: 10MB Cache File Size <code>cacheFileSize</code> To limit disk usage, configure fixed size log file cache for containers. Unit: DataSize. Range: 10MB-100GB. Default: 20MB. Compression is always enabled. Cache File Count <code>cacheFileSize</code> To limit disk usage, configure fixed count of log file cache for containers. Unit: <code>integer</code>. Max: 1024. Default: 3 <p>Sample <pre><code>options:\n  logBufferSize: 20m\n  cacheFileSize: 30m\n  cacheFileCount: 3\n  cacheImages: true\n</code></pre></p>"},{"location":"cluster/setup/executor-setup.html#relevant-directories","title":"Relevant directories","text":"<p>Location for data and logs are as follows:</p> <ul> <li><code>/etc/drove/executor/</code> - Configuration files</li> <li><code>/var/log/drove/executor/</code> - Executor Logs</li> <li><code>/var/log/drove/executor/instance-logs</code> - Application/Task Instance Logs</li> </ul> <p>We shall be volume mounting the config and log directories with the same name.</p> <p>Prerequisite Setup</p> <p>If not done already, please complete the prerequisite setup on all machines earmarked for the cluster.</p>"},{"location":"cluster/setup/executor-setup.html#setup-the-config-file","title":"Setup the config file","text":"<p>Create a relevant configuration file in <code>/etc/drove/controller/executor.yml</code>.</p> <p>Sample <pre><code>server:\n  applicationConnectors:\n    - type: http\n      port: 11000\n  adminConnectors:\n    - type: http\n      port: 11001\n  requestLog:\n    appenders:\n      - type: file\n        timeZone: IST\n        currentLogFilename: /var/log/drove/executor/drove-executor-access.log\n        archivedLogFilenamePattern: /var/log/drove/executor/drove-executor-access.log-%d-%i\n        archivedFileCount: 3\n        maxFileSize: 100MiB\n\nlogging:\n  level: INFO\n  loggers:\n    com.phonepe.drove: INFO\n\n\n  appenders:\n    - type: file\n      threshold: ALL\n      timeZone: IST\n      currentLogFilename: /var/log/drove/executor/drove-executor.log\n      archivedLogFilenamePattern: /var/log/drove/executor/drove-executor.log-%d-%i\n      archivedFileCount: 3\n      maxFileSize: 100MiB\n      logFormat: \"%(%-5level) [%date] [%logger{0} - %X{appId}] %message%n\"\n    - type: drove\n      logPath: \"/var/log/drove/executor/instance-logs\"\n      archivedLogFileSuffix: \"%d-%i\"\n      archivedFileCount: 0\n      maxFileSize: 1GiB\n      threshold: INFO\n      timeZone: IST\n      logFormat: \"%(%-5level) | %-23date | %-30logger{0} | %message%n\"\n      archive: true\n\nzookeeper:\n  connectionString: \"192.168.56.10:2181\"\n\nclusterAuth:\n  secrets:\n  - nodeType: CONTROLLER\n    secret: \"0v8XvJrDc7r86ZY1QCByPTDPninI4Xii\"\n  - nodeType: EXECUTOR\n    secret: \"pOd9sIEXhv0wrGOVc7ebwNvR7twZqyTN\"\n\nresources:\n  osCores: []\n  exposedMemPercentage: 90\n  disableNUMAPinning: true\n  overProvisioning:\n    enabled: true\n    memoryMultiplier: 10\n    cpuMultiplier: 10\n\noptions:\n  cacheImages: true\n  logBufferSize: 20m\n  cacheFileSize: 30m\n  cacheFileCount: 3\n  cacheImages: true\n</code></pre></p>"},{"location":"cluster/setup/executor-setup.html#setup-required-environment-variables","title":"Setup required environment variables","text":"<p>Environment variables need to run the drove controller are setup in <code>/etc/drove/executor/executor.env</code>.</p> <pre><code>CONFIG_FILE_PATH=/etc/drove/executor/executor.yml\nJAVA_PROCESS_MIN_HEAP=1g\nJAVA_PROCESS_MAX_HEAP=1g\nZK_CONNECTION_STRING=\"192.168.56.10:2181\"\nJAVA_OPTS=\"-Xlog:gc:/var/log/drove/executor/gc.log -Xlog:gc:::filecount=3,filesize=10M -Xlog:gc::time,level,tags -XX:+UseNUMA -XX:+ExitOnOutOfMemoryError -Djava.security.egd=file:/dev/urandom -Dfile.encoding=utf-8 -Djute.maxbuffer=0x9fffff\"\n</code></pre>"},{"location":"cluster/setup/executor-setup.html#create-systemd-file","title":"Create systemd file","text":"<p>Create a <code>systemd</code> file. Put the following in <code>/etc/systemd/system/drove.executor.service</code>:</p> <p><pre><code>[Unit]\nDescription=Drove Executor Service\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nUser=drove\nGroup=docker\nTimeoutStartSec=0\nRestart=always\nExecStartPre=-/usr/bin/docker pull ghcr.io/phonepe/drove-executor:latest\nExecStart=/usr/bin/docker run  \\\n    --env-file /etc/drove/executor/executor.env \\\n    --volume /etc/drove/executor:/etc/drove/executor:ro \\\n    --volume /var/log/drove/executor:/var/log/drove/executor \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --publish 11000:11000  \\\n    --publish 11001:11001 \\\n    --hostname %H \\\n    --rm \\\n    --name drove.executor \\\n    ghcr.io/phonepe/drove-executor:latest\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> Verify the file with the following command: <pre><code>systemd-analyze verify drove.executor.service\n</code></pre></p> <p>Set permissions <pre><code>chmod 664 /etc/systemd/system/drove.executor.service\n</code></pre></p>"},{"location":"cluster/setup/executor-setup.html#start-the-service-on-all-servers","title":"Start the service on all servers","text":"<p>Use the following to start the service:</p> <pre><code>systemctl daemon-reload\nsystemctl enable drove.executor\nsystemctl start drove.executor\n</code></pre> <p>You can tail the logs at <code>/var/log/drove/executor/drove-executor.log</code>.</p> <p>The executor should now show up on the Drove Console.</p>"},{"location":"cluster/setup/gateway.html","title":"Setting up Drove Gateway","text":"<p>The Drove Gateway works as a gateway to expose apps running on a drove cluster to rest of the world.</p> <p>Drove Gateway container uses NGinx and a modified version of Nixy to track drove endpoints. More details about this can be found in the drove-gateway project.</p>"},{"location":"cluster/setup/gateway.html#drove-gateway-nixy-configuration-reference","title":"Drove Gateway Nixy Configuration Reference","text":"<p>The nixy running inside the gateway container is configured using a custom TOML file. This section looks into this file:</p> <pre><code>address = \"127.0.0.1\"# (1)!\nport = \"6000\"\n\n\n# Drove Options\ndrove = [#(2)!\n  \"http://controller1.mydomain:10000\",\n   \"http://controller1.mydomain:10000\"\n   ]\n\nleader_vhost = \"drove-staging.mydomain\"#(3)!\nevent_refresh_interval_sec = 5#(5)!\nuser = \"\"#(6)!\npass = \"\"\naccess_token = \"\"#(7)!\n\n# Parameters to control which apps are exposed as VHost\nrouting_tag = \"externally_exposed\"#(4)!\nrealm = \"api.mydomain,support.mydomain\"#(8)!\nrealm_suffix = \"-external.mydomain\"#(9)!\n\n# Nginx related config\n\nnginx_config = \"/etc/nginx/nginx.conf\"#(10)!\nnginx_template = \"/etc/drove/gateway/nginx.tmpl\"#(11)!\nnginx_cmd = \"nginx\"#(12)!\nnginx_ignore_check = true#(13)!\n\n# NGinx plus specific options\nnginxplusapiaddr=\"127.0.0.1\"#(14)!\nnginx_reload_disabled=true#(15)!\nmaxfailsupstream = 0#(16)!\nfailtimeoutupstream = \"1s\"\nslowstartupstream = \"0s\"\n</code></pre> <ol> <li> <p>Nixy listener configuration. Endpoint for nixy itself.</p> </li> <li> <p>List of Drove controllers. Add all controller nodes here. Nixy will automatically determine and track the current leader. </p> <p>Auto detection is disabled if a single endpoint is specified.</p> </li> <li> <p>Helps create a vhost entry that tracks the leader on the cluster. Use this to expose the Drove endpoint to users. The value for this will be available to the template engine as the <code>LeaderVHost</code> variable.</p> </li> <li> <p>If some special routing behaviour needs to be implemented in  the template based on some tag metadata of the deployed apps, set the routing_tag option to set the tag name to be used. The actual value is derived from app instances and exposed to the template engine as the variable: <code>RoutingTag</code>. Optional.</p> <p>In this example, the RoutingTag variable will be set to the value specified in the <code>routing_tag</code> tag key specified when deploying the Drove Application. For example, if we want to expose the app we can set it to <code>yes</code>, and filter the VHost to be exposed in NGinx template when <code>RoutingTag == \"yes\"</code>.</p> </li> <li> <p>Drove Gateway/Nixy works on event polling on controller. This is the polling interval. Especially if number of NGinx nodes is high. Default is <code>2 seconds</code>. Unless cluster is really busy with a high rate of change of containers, this strikes a good balance between apps becoming discoverable vs putting the leader controller under heavy load.</p> </li> <li> <p><code>user</code> and <code>pass</code> are optional params can be used to set basic auth credentials to the calls made to Drove controllers if basic auth is enabled on the cluster. Leave empty if no basic auth is required.</p> </li> <li> <p>If cluster has some custom header based auth, the following can be used. The contents on this parameter are passed verbatim to the Authorization HTTP header. Leave empty if no token auth is enabled on the cluster.</p> </li> <li> <p>By default drove-gateway will expose all vhost declared in the spec for all drove apps on a cluster (caveat: filtering can be done using RoutingTag as well). If specific vhosts need to be exposed, set the realms parameter to a comma separated list of realms. Optional.</p> </li> <li> <p>Beside perfect vhost matching, Drove Gateway supports suffix based matches as well. A single suffix is supported. Optional.</p> </li> <li> <p>Path to NGinx config.</p> </li> <li> <p>Path to the template file, based on which the template will be generated.</p> </li> <li> <p>NGinx command to use to reload the config. Set this to <code>openresty</code> optionally to use openresty.</p> </li> <li> <p>Ignore calling NGinx command to test the config. Set this to false or delete this line on production. Default: false.</p> </li> <li> <p>If using NGinx plus, set the endpoint to the local server here. If left empty, NGinx plus api based vhost update will be disabled.</p> </li> <li> <p>If specific vhosts are exposed, auto-discovery and updation of config (and NGinx reloads) might not be desired as it will cause connection drops. Set the following parameter to true to disable reloads. Nixy will only update upstreams using the nplus APIs. Default: false.</p> </li> <li> <p>Connection parameters for NGinx plus.</p> </li> </ol> <p>NGinx plus</p> <p>NGinx plus is not shipped with this docker. If you want to use NGinx plus, please build nixy from the source tree here and build your own container.</p>"},{"location":"cluster/setup/gateway.html#relevant-directories","title":"Relevant directories","text":"<p>Location for data and logs are as follows:</p> <ul> <li><code>/etc/drove/gateway/</code> - Configuration files</li> <li><code>/var/log/drove/gateway/</code> - NGinx Logs</li> </ul> <p>We shall be volume mounting the config and log directories with the same name.</p> <p>Prerequisite Setup</p> <p>If not done already, please complete the prerequisite setup on all machines earmarked for the cluster.</p> <p>Go through the following steps to run <code>drove-gateway</code> as a service.</p>"},{"location":"cluster/setup/gateway.html#create-the-toml-config-for-nixy","title":"Create the TOML config for Nixy","text":"<p>Sample config file <code>/etc/drove/gateway/gateway.toml</code>:</p> <pre><code>address = \"127.0.0.1\"\nport = \"6000\"\n\n\n# Drove Options\ndrove = [\n  \"http://controller1.mydomain:10000\",\n   \"http://controller1.mydomain:10000\"\n   ]\n\nleader_vhost = \"drove-staging.mydomain\"\nevent_refresh_interval_sec = 5\nuser = \"guest\"\npass = \"guest\"\n\n\n# Nginx related config\nnginx_config = \"/etc/nginx/nginx.conf\"\nnginx_template = \"/etc/drove/gateway/nginx.tmpl\"\nnginx_cmd = \"nginx\"\nnginx_ignore_check = true\n</code></pre> <p>Replace domain names</p> <p>Please remember to update <code>mydomain</code> to a valid domain you want to use.</p>"},{"location":"cluster/setup/gateway.html#create-template-for-nginx","title":"Create template for NGinx","text":"<p>Create a NGinx template with the following config in <code>/etc/drove/gateway/nginx.tmpl</code></p> <pre><code># Generated by drove-gateway {{datetime}}\n\nuser www-data;\nworker_processes auto;\npid /run/nginx.pid;\n\nevents {\n    use epoll;\n    worker_connections 2048;\n    multi_accept on;\n}\nhttp {\n    server_names_hash_bucket_size  128;\n    add_header X-Proxy {{ .Xproxy }} always;\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log warn;\n    server_tokens off;\n    client_max_body_size 128m;\n    proxy_buffer_size 128k;\n    proxy_buffers 4 256k;\n    proxy_busy_buffers_size 256k;\n    proxy_redirect off;\n    map $http_upgrade $connection_upgrade {\n        default upgrade;\n        ''      close;\n    }\n    # time out settings\n    proxy_send_timeout 120;\n    proxy_read_timeout 120;\n    send_timeout 120;\n    keepalive_timeout 10;\n\n    server {\n        listen       7000 default_server;\n        server_name  _;\n        # Everything is a 503\n        location / {\n            return 503;\n        }\n    }\n    {{if and .LeaderVHost .Leader.Endpoint}}\n    upstream {{.LeaderVHost}} {\n        server {{.Leader.Host}}:{{.Leader.Port}};\n    }\n    server {\n        listen 7000;\n        server_name {{.LeaderVHost}};\n        location / {\n            proxy_set_header HOST {{.Leader.Host}};\n            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;\n            proxy_connect_timeout 30;\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection $connection_upgrade;\n            proxy_pass http://{{.LeaderVHost}};\n        }\n    }\n    {{end}}\n    {{- range $id, $app := .Apps}}\n    upstream {{$app.Vhost}} {\n        {{- range $app.Hosts}}\n        server {{ .Host }}:{{ .Port }};\n        {{- end}}\n    }\n    server {\n        listen 7000;\n        server_name {{$app.Vhost}};\n        location / {\n            proxy_set_header HOST $host;\n            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;\n            proxy_connect_timeout 30;\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection $connection_upgrade;\n            proxy_pass http://{{$app.Vhost}};\n        }\n    }\n    {{- end}}\n}\n</code></pre> <p>The above template will do the following:</p> <ul> <li>Set NGinx port to 7000. This is the port exposed on the Docker container for the gateway. Do not change this.</li> <li>Sets up error and access logs to <code>/var/log/nginx</code>. Log rotation is setup for this path already.</li> <li>Set up a Vhost <code>drove-staging.mydomain</code> that will get auto-updated with the current leader of the Drove cluster</li> <li>Setup automatically updated virtual hosts for all apps on the cluster.</li> </ul>"},{"location":"cluster/setup/gateway.html#create-environment-file","title":"Create environment file","text":"<p>We want to configure the drove gateway container using the required environment variables. To do that, put the following in <code>/etc/drove/gateway/gateway.env</code>:</p> <pre><code>CONFIG_FILE_PATH=/etc/drove/gateway/gateway.toml\nTEMPLATE_FILE_PATH=/etc/drove/gateway/nginx.tmpl\n</code></pre>"},{"location":"cluster/setup/gateway.html#create-systemd-file","title":"Create systemd file","text":"<p>Create a <code>systemd</code> file. Put the following in <code>/etc/systemd/system/drove.gateway.service</code>:</p> <pre><code>[Unit]\nDescription=Drove Gateway Service\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nUser=drove\nGroup=docker\nTimeoutStartSec=0\nRestart=always\nExecStartPre=-/usr/bin/docker pull ghcr.io/phonepe/drove-gateway:latest\nExecStart=/usr/bin/docker run  \\\n    --env-file /etc/drove/gateway/gateway.env \\\n    --volume /etc/drove/gateway:/etc/drove/gateway:ro \\\n    --volume /var/log/drove/gateway:/var/log/nginx \\\n    --network host \\\n    --hostname %H \\\n    --rm \\\n    --name drove.gateway \\\n    ghcr.io/phonepe/drove-gateway:latest\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Verify the file with the following command: <pre><code>systemd-analyze verify drove.gateway.service\n</code></pre></p> <p>Set permissions <pre><code>chmod 664 /etc/systemd/system/drove.gateway.service\n</code></pre></p>"},{"location":"cluster/setup/gateway.html#start-the-service-on-all-servers","title":"Start the service on all servers","text":"<p>Use the following to start the service:</p> <pre><code>systemctl daemon-reload\nsystemctl enable drove.gateway\nsystemctl start drove.gateway\n</code></pre>"},{"location":"cluster/setup/gateway.html#checking-logs","title":"Checking Logs","text":"<p>You can check logs using: <pre><code>journalctl -u drove.gateway -f\n</code></pre></p> <p>NGinx logs would be available at <code>/var/log/drove/gateway</code>. </p>"},{"location":"cluster/setup/gateway.html#log-rotation-for-nginx","title":"Log rotation for NGinx","text":"<p>The gateway sets up log rotation for the access and errors logs with the following config: <pre><code>/var/log/nginx/*.log {\n    rotate 5\n    size 10M\n    dateext\n    dateformat -%Y-%m-%d\n    missingok\n    compress\n    delaycompress\n    sharedscripts\n    notifempty\n    postrotate\n        test -r /var/run/nginx.pid &amp;&amp; kill -USR1 `cat /var/run/nginx.pid`\n    endscript\n}\n</code></pre></p> <p>This will rotate both error and access logs when they hit 10MB and keep 5 logs.</p> <p>Configure the above if you want and volume mount your config to <code>/etc/logrotate.d/nginx</code> to use different scheme as per your requirements.</p>"},{"location":"cluster/setup/maintenance.html","title":"Maintaining a Drove Cluster","text":"<p>There are a couple of constructs built into Drove to allow for easy maintenance.</p> <ul> <li>Cluster Maintenance Mode</li> <li>Executor node blacklisting</li> </ul>"},{"location":"cluster/setup/maintenance.html#maintenance-mode","title":"Maintenance mode","text":"<p>Drove supports a maintenance mode to allow for software updates without affecting the containers running on the cluster.</p> <p>Danger</p> <p>In maintenance mode, outage detection is turned off and container failure for applications are not acted upon even if detected.</p>"},{"location":"cluster/setup/maintenance.html#engaging-maintenance-mode","title":"Engaging maintenance mode","text":"<p>Set cluster to maintenance mode.</p> <p>Preconditions - Cluster must be in the following state: <code>MAINTENANCE</code></p> Drove CLIJSON <pre><code>drove -c local cluster maintenance-on\n</code></pre> <p>Sample Request <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/cluster/maintenance/set' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre></p> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"state\": \"MAINTENANCE\",\n        \"updated\": 1721630351178\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"cluster/setup/maintenance.html#disengaging-maintenance-mode","title":"Disengaging maintenance mode","text":"<p>Set cluster to normal mode.</p> <p>Preconditions - Cluster must be in the following state: <code>MAINTENANCE</code></p> Drove CLIJSON <pre><code>drove -c local cluster maintenance-off\n</code></pre> <p>Sample Request</p> <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/cluster/maintenance/unset' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"state\": \"NORMAL\",\n        \"updated\": 1721630491296\n    },\n    \"message\": \"success\"\n}\n</code></pre></p>"},{"location":"cluster/setup/maintenance.html#updating-drove-version-across-the-cluster-quickly","title":"Updating drove version across the cluster quickly","text":"<p>We recommend the following sequence of steps:</p> <ol> <li>Find the <code>leader</code> controller for the cluster using <code>drove ... cluster leader</code>.</li> <li> <p>Update the controller container on the nodes that are not the leader.</p> <p>If you are using the systemd file given here, you just need to restart the controller service using <code>systemctl restart drove.controller</code></p> </li> <li> <p>Set cluster to maintenance mode using <code>drove ... cluster maintenance-on</code>.</p> </li> <li> <p>Update the leader controller.</p> <p>If you are using the systemd file given here, you just need to restart the leader controller service: <code>systemctl restart drove.controller</code></p> </li> <li> <p>Update the executors.</p> <p>If you are using the systemd file given here, you just need to restart all executors: <code>systemctl restart drove.executor</code></p> </li> <li> <p>Take cluster out of maintenance mode: <code>drove ... cluster maintenance-off</code></p> </li> </ol>"},{"location":"cluster/setup/maintenance.html#executor-blacklisting","title":"Executor blacklisting","text":"<p>In cases where we want to take an executor node out of the cluster for planned maintenance, we need to ensure application instances running on the node are replaced by containers on other nodes and the ones running here are shut down cleanly.</p> <p>This is achieved by blacklisting the node.</p> <p>Tip</p> <p>Whenever blacklisting is done, it causes some flux in the application topology due to new container migration from blacklisted to normal nodes. To reduce the number of times this happens, plan to perform multiple operations togeter and blacklist and un-blacklist executors together.</p> <p>Drove will optimize bulk blacklisting related app migrations and will migrate containers together for an app only once rather than once for every node.</p> <p>Danger</p> <p>Task instances are not migrated out. This is because it is impossible for Drove to know if a task can be migrated or not (i.e. killed and spun up on a new node in any order).</p> <p>To blacklist executors do the following:</p> Drove CLIJSON <pre><code>drove -c local executor blacklist dd2cbe76-9f60-3607-b7c1-bfee91c15623 ex1 ex2 \n</code></pre> <p>Sample Request</p> <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/cluster/executors/blacklist?id=a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d&amp;id=ex1&amp;id=ex2' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"failed\": [\n            \"ex2\",\n            \"ex1\"\n        ],\n        \"successful\": [\n            \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\"\n        ]\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>To un-blacklist executors do the following:</p> Drove CLIJSON <pre><code>drove -c local executor unblacklist dd2cbe76-9f60-3607-b7c1-bfee91c15623 ex1 ex2 \n</code></pre> <p>Sample Request</p> <pre><code>curl --location --request POST 'http://drove.local:7000/apis/v1/cluster/executors/unblacklist?id=a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d&amp;id=ex1&amp;id=ex2' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data ''\n</code></pre> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"failed\": [\n            \"ex2\",\n            \"ex1\"\n        ],\n        \"successful\": [\n            \"a45442a1-d4d0-3479-ab9e-3ed0aa5f7d2d\"\n        ]\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Note</p> <p>Drove will not re-evaluate placement of existing Applications in <code>RUNNING</code> state once executors are brought back into rotation.</p>"},{"location":"cluster/setup/planning.html","title":"Planning your cluster","text":"<p>Running a drove cluster in production for critical workloads involves planning and preparation on factors like Availability, Scale, Security and Access management. The following issues should be considered while planning your drove cluster.</p>"},{"location":"cluster/setup/planning.html#criteria-for-planning","title":"Criteria for planning","text":"<p>The simplest form of a drove cluster would run controller, zookeeper, executor and gateway services all on the same machine while a highly available would separate out all components according to following considerations:</p> <ul> <li>Availability: Use of a single node in either executor, controller, gateway or zookeeper has a single point of failure. A typical highly available cluster consists of:<ul> <li>Separated controller and executor nodes</li> <li>Multi controller setup - typically minimum 3 controller nodes to provide adequate availability and majority for leader election</li> <li>Multiple gateway nodes to load balance traffic and provide fault tolerance</li> <li>Availability of sufficient executor nodes to follow placement policies suitable for high availability for application instances across executors</li> <li>Multiple node zookeeper cluster to provide adequate availability and quorum. Even number of nodes is not useful. Atleast three zookeeper servers are required.</li> </ul> </li> <li>Scale: You should size your components as per expected amount of traffic to your instances but also have a plan for expected demand growth and ways to scale your cluster accordingly</li> <li>Security and Access management: You can use authentication for intra-cluster communication and add encryption for secure communications. Access management can be devolved by creating multiple users with either read or write roles.</li> </ul>"},{"location":"cluster/setup/planning.html#cluster-configuration","title":"Cluster configuration","text":""},{"location":"cluster/setup/planning.html#controllers","title":"Controllers","text":"<p>Controllers will manage the cluster with application instances spread across multiple executors as per different placement policies. Controllers use leader-election to coordinate and will act as a single entity while each executor acts as a single entity that runs many different application instances.</p> <ul> <li>Multiple controllers: For high availability, there should be atleast three controllers. Drove uses leader election to coordinate across the controllers. If the leader fails, the other controllers would elect a leader and takeover.</li> <li>Separate Zookeeper service and snapshots: The zookeeper service used for state coordination by controller can either be run on same machines as controller or on separate machines. Zookeeper stores the configuration data for the whole cluster and zookeeper snapshots can be used to back up the data.</li> <li>Availability zones: You can split the controllers and/or zookeeper nodes across availability zones/data centers to improve resilience.</li> <li>Transit encryption and certificate Management: Controllers and executors can communicate via secure communications. TLS settings and certificates can be added by modifying applicationConnectors and adminConnectors parameters as per  Dropwizard HTTPS connector configuration</li> <li>Separate Gateways: The drove gateways will route and load balance application traffic across application instances. The gateway service can  either be run on same machines as controller or on separate machines.</li> <li>Resources: The required JVM maximum heap size for drove controller service will increase with the increase in number of executors,applications and application instances in the cluster. The JVM parameters should be reviewed as per your scale.</li> </ul>"},{"location":"cluster/setup/planning.html#zookeeper","title":"Zookeeper","text":"<ul> <li>Quorum: For replicated zookeeper, a minimum of three servers are required, and it is recommended that you have an odd number of servers. If you only have two servers, if one of them fails, there are not enough machines to form a majority quorum. Two servers are inherently less stable than a single server, because there are two single points of failure. Refer Running replicated ZooKeeper in ZooKeeper Getting Started Guide</li> <li>zxid rollover: zxid is the ZooKeeper transaction id and is 64 bit number. The zxid has two parts epoch and a counter which use the high order 32-bits for the epoch and the low order 32-bits for the counter. A zookeeper election is forced when the 32-bit counter rolls over. This will be more frequent as scale increases in your cluster.</li> <li>JVM parameters and resources: The required JVM maximum heap size for zookeeper  will increase with the increase in number of executors,applications and application instances in the cluster. The JVM parameters should be reviewed as per your scale. Refer ZooKeeper Administrator's Guide</li> </ul>"},{"location":"cluster/setup/planning.html#executors","title":"Executors","text":"<ul> <li>Containerisation Engine: Drove supports the Docker engine and has experimental support for Podman. Choose your engine as per your security, networking and performance considerations.</li> <li>Container Networking: The container engine and container networking should be configured as per your requirements. It is recommended to use Port forwarding based container networking if you choose to use Drove Gateway to route application traffic. Container engine settings can be modified to manage DNS and proxy parameters for containers.</li> <li>Placement policies and availability: Drove supports placement policies to set criteria for replication of instances across executors and avoiding single points of failure. Drove tags can be assigned to executors and placement policy can be used to pin certain applications to specific selected executors if you have any hardware or other considerations.</li> <li>Scaling: As your cluster scale increases, you can continue adding executors to the cluster. Placement policies should be used to manage availability criteria. Controller and ZooKeeper resource requirements will increase as your executor count increases and should be reviewed accordingly.</li> </ul>"},{"location":"cluster/setup/planning.html#gateways","title":"Gateways","text":"<ul> <li>Container Networking:  It is recommended to use Port forwarding based container networking if you choose to use Drove Gateways to route application traffic</li> <li>Load balancing: Gateways use Nginx as a web server and can use many different approaches to load balancing among multiple gateway nodes. Some examples include:<ul> <li>DNS Load balancing: Multiple gateway IP's can be added as A records to the virtual host domain to let clients use round-robin DNS and split load across gateway nodes</li> <li>Anycast/Network Load balancing: If any sort of anycast/network load balancing functionality is available in your network, it can be used to split traffic across gateway nodes</li> </ul> </li> <li>High Availability and Scaling: Many different methods are available to achieve high availability and scale NGINX. Any method can be used by adequately modifying the template used by Gateway to render Nginx configuration.</li> </ul>"},{"location":"cluster/setup/prerequisites.html","title":"Setting up the prerequisites","text":"<p>On all machines on the drove cluster, we would want to use the same user and have a consistent storage structure for configuration, logs etc.</p> <p>Note</p> <p>All commands o be issues as <code>root</code>. To get to admin/root mode issue the following command:</p> <pre><code>sudo su\n</code></pre>"},{"location":"cluster/setup/prerequisites.html#setting-up-user","title":"Setting up user","text":"<p>We shall create an user called <code>drove</code> to be used to run all services and containers and assign the file ownership to this user.</p> <p><pre><code>adduser --system --group \"drove\" --home /var/lib/misc --no-create-home &gt; /dev/null\n</code></pre> We want to user to be able to run docker containers, so we add the user to the docker group:</p> <pre><code>groupadd docker\nusermod -aG docker drove\n</code></pre>"},{"location":"cluster/setup/prerequisites.html#create-directories","title":"Create directories","text":"<p>We shall use the following locations to store configurations, logs etc:</p> <ul> <li><code>/etc/drove/...</code> - for configuration</li> <li><code>/var/log/drove/..</code> - for all logs</li> </ul> <p>We go ahead and create these locations and setup the correct permissions:</p> <pre><code>mkdir -p /etc/drove\nchown -R drove.drove /etc/drove\nchmod 700 /etc/drove\nchmod g+s /etc/drove\n\nmkdir -p /var/lib/drove\nchown -R drove.drove /var/lib/drove\nchmod 700 /var/lib/drove\n\nmkdir -p /var/log/drove\n</code></pre> <p>Danger</p> <p>Ensure you run the <code>chmod</code> commands to remove read access everyone other than the owner.</p>"},{"location":"cluster/setup/units.html","title":"Units Reference","text":"<p>In the configuration files for Drove, we use the <code>Duration</code> and <code>DataSize</code> units to make configuration easier.</p>"},{"location":"cluster/setup/units.html#data-size","title":"Data Size","text":"<p>Use the following shortcuts to express sizes in human readable form such as <code>2GB</code> etc:</p> <ul> <li><code>B</code> - Bytes </li> <li><code>byte</code> - Bytes </li> <li><code>Bytes</code> - Bytes </li> <li><code>K</code> - Kilobytes </li> <li><code>KB</code> - Kilobytes </li> <li><code>KiB</code> - Kibibytes</li> <li><code>Kilobyte</code> - Kilobytes </li> <li><code>kibibyte</code> - Kibibytes</li> <li><code>KiloBytes</code> - Kilobytes </li> <li><code>kibiBytes</code> - Kibibytes</li> <li><code>M</code> - Megabytes </li> <li><code>MB</code> - Megabytes </li> <li><code>MiB</code> - Mebibytes</li> <li><code>megabyte</code> - Megabytes </li> <li><code>mebibyte</code> - Mebibytes</li> <li><code>megaBytes</code> - Megabytes </li> <li><code>mebiBytes</code> - Mebibytes</li> <li><code>G</code> - Gigabytes </li> <li><code>GB</code> - Gigabytes </li> <li><code>GiB</code> - Gibibytes</li> <li><code>gigabyte</code> - Gigabytes </li> <li><code>gibibyte</code> - Gibibytes</li> <li><code>gigaBytes</code> - Gigabytes </li> <li><code>gibiBytes</code> - Gibibytes</li> <li><code>T</code> - Terabytes </li> <li><code>TB</code> - Terabytes </li> <li><code>TiB</code> - Tebibytes</li> <li><code>terabyte</code> - Terabytes </li> <li><code>tebibyte</code> - Tebibytes</li> <li><code>teraBytes</code> - Terabytes </li> <li><code>tebiBytes</code> - Tebibytes</li> <li><code>P</code> - Petabytes </li> <li><code>PB</code> - Petabytes </li> <li><code>PiB</code> - Pebibytes</li> <li><code>petabyte</code> - Petabytes </li> <li><code>pebibyte</code> - Pebibytes</li> <li><code>petaBytes</code> - Petabytes </li> <li><code>pebiBytes</code> - Pebibytes</li> </ul>"},{"location":"cluster/setup/units.html#duration","title":"Duration","text":"<p>Time durations in Drove can be expressed in human readable form, for example: <code>3d</code> can be used to signify 3 days and so on. The list of valid duration unit suffixes are:</p> <ul> <li><code>ns</code> - nanoseconds</li> <li><code>nanosecond</code> - nanoseconds</li> <li><code>nanoseconds</code> - nanoseconds</li> <li><code>us</code> - microseconds</li> <li><code>microsecond</code> - microseconds</li> <li><code>microseconds</code> - microseconds</li> <li><code>ms</code> - milliseconds</li> <li><code>millisecond</code> - milliseconds</li> <li><code>milliseconds</code> - milliseconds</li> <li><code>s</code> - seconds</li> <li><code>second</code> - seconds</li> <li><code>seconds</code> - seconds</li> <li><code>m</code> - minutes</li> <li><code>min</code> - minutes</li> <li><code>mins</code> - minutes</li> <li><code>minute</code> - minutes</li> <li><code>minutes</code> - minutes</li> <li><code>h</code> - hours</li> <li><code>hour</code> - hours</li> <li><code>hours</code> - hours</li> <li><code>d</code> - days</li> <li><code>day</code> - days</li> <li><code>days</code> - days</li> </ul>"},{"location":"cluster/setup/zookeeper.html","title":"Setting Up Zookeeper","text":"<p>We shall be running Zookeeper using the official Docker images. All data volumes etc will be mounted on the host machines.</p> <p>The following ports will be exposed:</p> <ul> <li><code>2181</code> - This is the main port for ZK clients to connect to the server</li> <li><code>2888</code> - The port used by Zookeeper for in-cluster communications between peers</li> <li><code>3888</code> - Port used for internal leader election</li> <li><code>8080</code> - Admin server port. We are going to turn this off.</li> </ul> <p>Danger</p> <p>The ZK admin server does not shut down cleanly from time to time. And is not needed for anything related to Drove. If not needed, you should turn it off.</p> <p>We assume the following to be the IP for the 3 zookeeper nodes:</p> <ul> <li>192.168.3.10</li> <li>192.168.3.11</li> <li>192.168.3.12</li> </ul>"},{"location":"cluster/setup/zookeeper.html#relevant-directories","title":"Relevant directories","text":"<p>Location for data and logs are as follows:</p> <ul> <li><code>/etc/drove/zk</code> - Configuration files</li> <li><code>/var/lib/drove/zk/</code> - Data and data logs</li> <li><code>/var/log/drove/zk</code> - Logs</li> </ul>"},{"location":"cluster/setup/zookeeper.html#important-files","title":"Important files","text":"<p>The zookeeper container stores snapshots, transaction logs and application logs on <code>/data</code>, <code>/datalog</code> and <code>/logs</code> directories respectively. We shall be volume mounting the following:</p> <ul> <li><code>/var/lib/drove/zk/data</code> to <code>/data</code> on the container</li> <li><code>/var/lib/drove/zk/datalog</code> to <code>/datalog</code> on the container</li> <li><code>/var/logs/drove/zk</code> to <code>/logs</code> on the container</li> </ul> <p>Docker will create these directories when container comes up for the first time.</p> <p>Tip</p> <p>The zk server id (as set above using the <code>ZOO_MY_ID</code>) can also be set by putting the server number in a file named <code>myid</code> in the <code>/data</code> directory.</p> <p>Prerequisite Setup</p> <p>If not done already, lease complete the prerequisite setup on all machines earmarked for the cluster.</p>"},{"location":"cluster/setup/zookeeper.html#setup-configuration-files","title":"Setup configuration files","text":"<p>Let's create the config directory:</p> <pre><code>mkdir -p /etc/drove/zk\n</code></pre> <p>We shall be creating 3 different configuration files to configure zookeeper:</p> <ul> <li><code>zk.env</code> - Environment variables to be used by zookeeper container</li> <li><code>java.env</code> - Setup JVM related options</li> <li><code>logbaxk.xml</code> - Logging configuration</li> </ul>"},{"location":"cluster/setup/zookeeper.html#setup-environment-variables","title":"Setup environment variables","text":"<p>Let us prepare the configuration. Put the following in a file: <code>/etc/drove/zk/zk.env</code>:</p> <pre><code>#(1)!\nZOO_TICK_TIME=2000\nZOO_INIT_LIMIT=10\nZOO_SYNC_LIMIT=5\nZOO_STANDALONE_ENABLED=false\nZOO_ADMINSERVER_ENABLED=false\n\n#(2)!\nZOO_AUTOPURGE_PURGEINTERVAL=12\nZOO_AUTOPURGE_SNAPRETAINCOUNT=5\n\n#(3)!\nZOO_MY_ID=1\nZOO_SERVERS=server.1=192.168.3.10:2888:3888;2181 server.2=192.168.3.11:2888:3888;2181 server.3=192.168.3.12:2888:3888;2181\n</code></pre> <ol> <li>This is cluster level configuration to ensure the cluster topology remains stable through minor flaps</li> <li>This will control how much data we retain</li> <li>This section needs to change per server. Each server should have a different <code>ZOO_MY_ID</code> set. And the same numbers get referred to in <code>ZOO_SERVERS</code> section.</li> </ol> <p>Warning</p> <ul> <li> <p>The <code>ZOO_MY_ID</code> value needs to be different on every server.So it would be:</p> <ul> <li>192.168.3.10 - 1</li> <li>192.168.3.11 - 2</li> <li>192.168.3.12 - 3</li> </ul> </li> <li> <p>The format for <code>ZOO_SERVERS</code> is <code>server.id=&lt;address1&gt;:&lt;port1&gt;:&lt;port2&gt;[:role];[&lt;client port address&gt;:]&lt;client port&gt;</code>.</p> </li> </ul> <p>Info</p> <p>Exhaustive set of options can be found on the Official Docker Page.</p>"},{"location":"cluster/setup/zookeeper.html#setup-jvm-parameters","title":"Setup JVM parameters","text":"<p>Put the following in <code>/etc/drove/zk/java.env</code></p> <pre><code>export SERVER_JVMFLAGS='-Djute.maxbuffer=0x9fffff -Xmx4g -Xms4g -Dfile.encoding=utf-8 -XX:+UseG1GC -XX:+UseNUMA -XX:+ExitOnOutOfMemoryError'\n</code></pre> <p>Configuring Max Data Size</p> <p>Drove data per node can get a bit on the larger side from time to time depending on your application configuration. To be on the safe side, we need to increase the maximum data size per node. This is achieved by setting the JVM option <code>-Djute.maxbuffer=0x9fffff</code> on all cluster nodes in Drove. This is 10MB (approx). The actual payload doesn't reach anywhere close. However we shall be picking up payload compression in a future version to stop this variable from needing to be set.</p> <p>For the Zookeeper Docker, the environment variable <code>SERVER_JVMFLAGS</code> needs to be set to <code>-Djute.maxbuffer=0x9fffff</code>.</p> <p>Please refer to Zookeeper Advanced Configuration for further properties that can be tuned.</p> <p>JVM Size</p> <p>We set 4GB JVM heap size for ZK by adding appropriate options in <code>SERVER_JVMFLAGS</code>. Please make sure you have sized your machines to have 10-16GB of RAM at the very least. Tune the JVM size and machine size according to your needs.</p> <p>q</p> <p><code>JVMFLAGS</code> environment variable</p> <p>Do not set this variable in <code>zk.env</code>. Couple of reasons:</p> <ul> <li>This will affect both the zk server as well as the client.</li> <li>There is an issue and the flag (nor the <code>SERVER_JVMFLAGS</code>) are not used properly by the startup scripts.</li> </ul>"},{"location":"cluster/setup/zookeeper.html#configure-logging","title":"Configure logging","text":"<p>We want to have physical log files on disk for debugging and audits and want the container to be ephemeral to allow for easy updates etc. To achieve this, put the following in <code>/etc/drove/zk/logback.xml</code>:</p> <pre><code>&lt;!--\n Copyright 2022 The Apache Software Foundation\n\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\n Define some default values that can be overridden by system properties\n--&gt;\n&lt;configuration&gt;\n  &lt;!-- Uncomment this if you would like to expose Logback JMX beans --&gt;\n  &lt;!--jmxConfigurator /--&gt;\n\n  &lt;property name=\"zookeeper.console.threshold\" value=\"INFO\" /&gt;\n\n  &lt;property name=\"zookeeper.log.dir\" value=\"/logs\" /&gt;\n  &lt;property name=\"zookeeper.log.file\" value=\"zookeeper.log\" /&gt;\n  &lt;property name=\"zookeeper.log.threshold\" value=\"INFO\" /&gt;\n  &lt;property name=\"zookeeper.log.maxfilesize\" value=\"256MB\" /&gt;\n  &lt;property name=\"zookeeper.log.maxbackupindex\" value=\"20\" /&gt;\n\n  &lt;!--\n    console\n    Add \"console\" to root logger if you want to use this\n  --&gt;\n  &lt;appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n    &lt;encoder&gt;\n      &lt;pattern&gt;%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n&lt;/pattern&gt;\n    &lt;/encoder&gt;\n    &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt;\n      &lt;level&gt;${zookeeper.console.threshold}&lt;/level&gt;\n    &lt;/filter&gt;\n  &lt;/appender&gt;\n\n  &lt;!--\n    Add ROLLINGFILE to root logger to get log file output\n  --&gt;\n  &lt;appender name=\"ROLLINGFILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt;\n    &lt;File&gt;${zookeeper.log.dir}/${zookeeper.log.file}&lt;/File&gt;\n    &lt;encoder&gt;\n      &lt;pattern&gt;%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n&lt;/pattern&gt;\n    &lt;/encoder&gt;\n    &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt;\n      &lt;level&gt;${zookeeper.log.threshold}&lt;/level&gt;\n    &lt;/filter&gt;\n    &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\"&gt;\n      &lt;maxIndex&gt;${zookeeper.log.maxbackupindex}&lt;/maxIndex&gt;\n      &lt;FileNamePattern&gt;${zookeeper.log.dir}/${zookeeper.log.file}.%i&lt;/FileNamePattern&gt;\n    &lt;/rollingPolicy&gt;\n    &lt;triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\"&gt;\n      &lt;MaxFileSize&gt;${zookeeper.log.maxfilesize}&lt;/MaxFileSize&gt;\n    &lt;/triggeringPolicy&gt;\n  &lt;/appender&gt;\n\n  &lt;!--\n    Add TRACEFILE to root logger to get log file output\n    Log TRACE level and above messages to a log file\n  --&gt;\n  &lt;!--property name=\"zookeeper.tracelog.dir\" value=\"${zookeeper.log.dir}\" /&gt;\n  &lt;property name=\"zookeeper.tracelog.file\" value=\"zookeeper_trace.log\" /&gt;\n  &lt;appender name=\"TRACEFILE\" class=\"ch.qos.logback.core.FileAppender\"&gt;\n    &lt;File&gt;${zookeeper.tracelog.dir}/${zookeeper.tracelog.file}&lt;/File&gt;\n    &lt;encoder&gt;\n      &lt;pattern&gt;%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n&lt;/pattern&gt;\n    &lt;/encoder&gt;\n    &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt;\n      &lt;level&gt;TRACE&lt;/level&gt;\n    &lt;/filter&gt;\n  &lt;/appender--&gt;\n\n  &lt;!--\n    zk audit logging\n  --&gt;\n  &lt;property name=\"zookeeper.auditlog.file\" value=\"zookeeper_audit.log\" /&gt;\n  &lt;property name=\"zookeeper.auditlog.threshold\" value=\"INFO\" /&gt;\n  &lt;property name=\"audit.logger\" value=\"INFO, RFAAUDIT\" /&gt;\n\n  &lt;appender name=\"RFAAUDIT\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt;\n    &lt;File&gt;${zookeeper.log.dir}/${zookeeper.auditlog.file}&lt;/File&gt;\n    &lt;encoder&gt;\n      &lt;pattern&gt;%d{ISO8601} %p %c{2}: %m%n&lt;/pattern&gt;\n    &lt;/encoder&gt;\n    &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt;\n      &lt;level&gt;${zookeeper.auditlog.threshold}&lt;/level&gt;\n    &lt;/filter&gt;\n    &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\"&gt;\n      &lt;maxIndex&gt;10&lt;/maxIndex&gt;\n      &lt;FileNamePattern&gt;${zookeeper.log.dir}/${zookeeper.auditlog.file}.%i&lt;/FileNamePattern&gt;\n    &lt;/rollingPolicy&gt;\n    &lt;triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\"&gt;\n      &lt;MaxFileSize&gt;10MB&lt;/MaxFileSize&gt;\n    &lt;/triggeringPolicy&gt;\n  &lt;/appender&gt;\n\n  &lt;logger name=\"org.apache.zookeeper.audit.Slf4jAuditLogger\" additivity=\"false\" level=\"${audit.logger}\"&gt;\n    &lt;appender-ref ref=\"RFAAUDIT\" /&gt;\n  &lt;/logger&gt;\n\n  &lt;root level=\"INFO\"&gt;\n    &lt;appender-ref ref=\"CONSOLE\" /&gt;\n    &lt;appender-ref ref=\"ROLLINGFILE\" /&gt;\n  &lt;/root&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Tip</p> <p>This is a customization of the original file from Zookeeper source tree. Please refer to documentation to configure logging.</p>"},{"location":"cluster/setup/zookeeper.html#create-systemd-file","title":"Create Systemd File","text":"<p>Create a <code>systemd</code> file. Put the following in <code>/etc/systemd/system/drove.zookeeper.service</code>:</p> <pre><code>[Unit]\nDescription=Drove Zookeeper Service\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nUser=drove\nGroup=docker\nTimeoutStartSec=0\nRestart=always\nExecStartPre=-/usr/bin/docker pull zookeeper:3.8\nExecStart=/usr/bin/docker run \\\n    --env-file /etc/drove/zk/zk.env \\\n    --volume /var/lib/drove/zk/data:/data \\\n    --volume /var/lib/drove/zk/datalog:/datalog \\\n    --volume /var/log/drove/zk:/logs \\\n    --volume /etc/drove/zk/logback.xml:/conf/logback.xml \\\n    --volume /etc/drove/zk/java.env:/conf/java.env \\\n    --publish 2181:2181 \\\n    --publish 2888:2888 \\\n    --publish 3888:3888 \\\n    --rm \\\n    --name drove.zookeeper \\\n    zookeeper:3.8\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Verify the file with the following command: <pre><code>systemd-analyze verify drove.zookeeper.service\n</code></pre></p> <p>Set permissions <pre><code>chmod 664 /etc/systemd/system/drove.zookeeper.service\n</code></pre></p>"},{"location":"cluster/setup/zookeeper.html#start-the-service-on-all-servers","title":"Start the service on all servers","text":"<p>Use the following to start the service:</p> <pre><code>systemctl daemon-reload\nsystemctl enable drove.zookeeper\nsystemctl start drove.zookeeper\n</code></pre> <p>You can check server status using the following:</p> <pre><code>echo srvr | nc localhost 2181\n</code></pre> <p>Tip</p> <p>Replace <code>localhost</code> on the above command with the actual ZK server IPs to test remote connectivity.</p> <p>Note</p> <p>You can access the ZK client from the container using the following command:</p> <pre><code>docker exec -it drove.zookeeper bin/zkCli.sh\n</code></pre> <p>To connect to remote host you can use the following: <pre><code>docker exec -it drove.zookeeper bin/zkCli.sh -server &lt;server name or ip&gt;:2181\n</code></pre></p>"},{"location":"extra/cli.html","title":"Drove CLI","text":"<p>Details for the Drove CLI, including installation and usage can be found in the cli repo.</p> <p>Repo link: https://github.com/PhonePe/drove-cli.</p>"},{"location":"extra/epoch.html","title":"Epoch","text":"<p>Epoch is a cron type scheduler that spins up container jobs on Drove.</p> <p>Details for using epoch can be found in the epoch repo.</p> <p>Link for Epoch repo: https://github.com/PhonePe/epoch.</p>"},{"location":"extra/epoch.html#epoch-cli","title":"Epoch CLI","text":"<p>There is a cli client for interaction with epoch. Details for installation and usage can be found in the epoch CLI repo.</p> <p>Link for Epoch CLI repo: https://github.com/phonepe/epoch-cli.</p>"},{"location":"extra/libraries.html","title":"Libraries","text":"<p>Drove is written in Java. We provide a few libraries that can be used to integrate with a Drove cluster.</p>"},{"location":"extra/libraries.html#setup","title":"Setup","text":"<p>Setup the drove version <pre><code>&lt;properties&gt;\n    &lt;!--other properties--&gt;\n    &lt;drove.version&gt;1.31&lt;/drove.version&gt;\n&lt;/properties&gt;\n</code></pre></p> <p>Checking the latest version</p> <p>Latest version can be checked at the github packages page here</p> <p>All libraries are located in sub packages of the top level package <code>com.phonepe.drove</code>.</p> <p>Java Version Compatibility</p> <p>Using Drove libraries would need Java versions 17+.</p>"},{"location":"extra/libraries.html#drove-model","title":"Drove Model","text":"<p>The model library for the classes used in request and response. It has dependency on <code>jackson</code> and <code>dropwizard-validation</code>.</p>"},{"location":"extra/libraries.html#dependency","title":"Dependency","text":"<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.phonepe.drove&lt;/groupId&gt;\n    &lt;artifactId&gt;drove-models&lt;/artifactId&gt;\n    &lt;version&gt;${drove.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"extra/libraries.html#drove-client","title":"Drove Client","text":"<p>We provide a client library that can be used to connect to a Drove cluster. The cluster accepts controller endpoints as parameter (among other things) and automatically tracks the leader controller. If a single controller endpoint is provided, this functionality is turned off.</p> <p>Please note that the client does not provide specific functions corresponding to different api calls from the controller, it acts as a simple endpoint discovery mechanism for drove cluster. Please refer to API section for details on individual apis.</p>"},{"location":"extra/libraries.html#transport","title":"Transport","text":"<p>The transport layer in the client is used to actually make HTTP calls to the Drove server. A new transport can be used by implementing the <code>get()</code>, <code>post()</code>, <code>put()</code> and <code>delete()</code> methods in the <code>DroveHttpTransport</code> interface.</p> <p>By default Drove client uses Java internal HTTP client as a trivial transport implementation. We also provide an Apache Http Components based implementation.</p> <p>Tip</p> <p>Do not use the default transport in production. Please use the HTTP Components based transport or your custom ones.</p>"},{"location":"extra/libraries.html#dependencies","title":"Dependencies","text":"<pre><code> &lt;dependency&gt;\n    &lt;groupId&gt;com.phonepe.drove&lt;/groupId&gt;\n    &lt;artifactId&gt;drove-client&lt;/artifactId&gt;\n    &lt;version&gt;${drove.version}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.phonepe.drove&lt;/groupId&gt;\n    &lt;artifactId&gt;drove-client-httpcomponent-transport&lt;/artifactId&gt;\n    &lt;version&gt;${drove.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"extra/libraries.html#sample-code","title":"Sample code","text":"<pre><code>public class DroveCluster implements AutoCloseable {\n\n    @Getter\n    private final DroveClient droveClient;\n\n    public DroveCluster() {\n        final var config = new DroveConfig()\n            .setEndpoints(List.of(\"http://controller1:4000,http://controller2:4000\"));\n\n        this.droveClient = new DroveClient(config,\n                                      List.of(new BasicAuthDecorator(\"guest\", \"guest\")),\n                                           new DroveHttpComponentsTransport(config.getCluster()));\n    }\n\n    @Override\n    public void close() throws Exception {\n        this.droveClient.close();\n    }\n}\n</code></pre> <p>RequestDecorator</p> <p>This interface can be implemented to augment requests with special headers like for example Authorization, as well as for other stuff like adding content type etc etc.</p>"},{"location":"extra/libraries.html#drove-event-listener","title":"Drove Event Listener","text":"<p>This library provides callbacks that can be used to listen and react to events happening on the Drove cluster.</p>"},{"location":"extra/libraries.html#dependencies_1","title":"Dependencies","text":"<pre><code>&lt;!--Include Drove client--&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.phonepe.drove&lt;/groupId&gt;\n    &lt;artifactId&gt;drove-events-client&lt;/artifactId&gt;\n    &lt;version&gt;${drove.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"extra/libraries.html#sample-code_1","title":"Sample Code","text":"<pre><code>final var droveClient = ... //build your java transport, client here\n\n//Create and setup your object mapper\nfinal var mapper = new ObjectMapper();\nmapper.registerModule(new ParameterNamesModule());\nmapper.setSerializationInclusion(JsonInclude.Include.NON_EMPTY);\nmapper.setSerializationInclusion(JsonInclude.Include.NON_NULL);\nmapper.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS);\nmapper.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES);\nmapper.enable(MapperFeature.ACCEPT_CASE_INSENSITIVE_ENUMS);\n\nfinal var listener = new DroveRemoteEventListener(droveClient, //Create listener\n                                                    mapper,\n                                                    new DroveEventPollingOffsetInMemoryStore(),\n                                                    Duration.ofSeconds(1));\n\nlistener.onEventReceived() //Connect signal handlers\n    .connect(events -&gt; {\n        log.info(\"Remote Events: {}\", events);\n    });\n\nlistener.start(); //Start listening\n\n\n//Once done close the listener\nlistener.close();\n</code></pre> <p>Event Types</p> <p>Please check the <code>com.phonepe.drove.models.events</code> package for the different event types and classes.</p> <p>Event Polling Offset Store</p> <p>The event poller library uses polling to find new events based on an offset. The event polling offset store is used to store and retrieve this offset. The <code>DroveEventPollingOffsetInMemoryStore</code> default store stores this information in-memory. Implement <code>DroveEventPollingOffsetStore</code> to a more permanent storage if you want this to be more permanent.</p>"},{"location":"extra/libraries.html#drove-hazelcast-cluster-discovery","title":"Drove Hazelcast Cluster Discovery","text":"<p>Drove provides an implementation of the Hazelcast discovery SPI so that containers deployed on a drove cluster can discover each other. This client uses the token injected by drove in the <code>DROVE_APP_INSTANCE_AUTH_TOKEN</code> environment variable to get sibling information from the controller.</p>"},{"location":"extra/libraries.html#dependencies_2","title":"Dependencies","text":"<pre><code>&lt;!--Include Drove client--&gt;\n&lt;!--Include Hazelcast--&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.phonepe.drove&lt;/groupId&gt;\n    &lt;artifactId&gt;drove-events-client&lt;/artifactId&gt;\n    &lt;version&gt;${drove.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"extra/libraries.html#sample-code_2","title":"Sample Code","text":"<pre><code>//Setup hazelcast\nConfig config = new Config();\n\n// Enable discovery\nconfig.setProperty(\"hazelcast.discovery.enabled\", \"true\");\nconfig.setProperty(\"hazelcast.discovery.public.ip.enabled\", \"true\");\nconfig.setProperty(\"hazelcast.socket.client.bind.any\", \"true\");\nconfig.setProperty(\"hazelcast.socket.bind.any\", \"false\");\n\n//Setup networking\nNetworkConfig networkConfig = config.getNetworkConfig();\nnetworkConfig.getInterfaces().addInterface(\"0.0.0.0\").setEnabled(true);\nnetworkConfig.setPort(port); //Port is the port exposed on the container for hazelcast clustering\n\n// Setup Drove discovery\nJoinConfig joinConfig = networkConfig.getJoin();\n\nDiscoveryConfig discoveryConfig = joinConfig.getDiscoveryConfig();\nDiscoveryStrategyConfig discoveryStrategyConfig =\n        new DiscoveryStrategyConfig(new DroveDiscoveryStrategyFactory());\ndiscoveryStrategyConfig.addProperty(\"drove-endpoint\", \"http://controller1:4000,http://controller2:4000\"); //Controller endpoints\ndiscoveryStrategyConfig.addProperty(\"port-name\", \"hazelcast\"); // Name of the hazelcast port defined in Application spec\ndiscoveryStrategyConfig.addProperty(\"transport\", \"com.phonepe.drove.client.transport.httpcomponent.DroveHttpComponentsTransport\");\ndiscoveryStrategyConfig.addProperty(\"cluster-by-app-name\", true); //Cluster container across multiple app versions\ndiscoveryConfig.addDiscoveryStrategyConfig(discoveryStrategyConfig);\n\n//Create hazelcast node\nval node = Hazelcast.newHazelcastInstance(config);\n\n//Once connected, node.getCluster() will be non null\n</code></pre> <p>Peer discovery modes</p> <p>By default the containers will only discover and connect to containers from the same application id. If you need to connect to containers from all versions of the same application please set the <code>cluster-by-app-name</code> property to <code>true</code> as in the above example.</p>"},{"location":"extra/libraries.html#drove-apache-ignite-discovery","title":"Drove Apache Ignite Discovery","text":"<p>Drove provides an implementation of the apache ignite discovery so that containers deployed on a drove cluster can discover each other. This client uses the token injected by drove in the <code>DROVE_APP_INSTANCE_AUTH_TOKEN</code> environment variable to get sibling information from the controller.</p>"},{"location":"extra/libraries.html#dependencies_3","title":"Dependencies","text":"<pre><code>&lt;!--Include Drove client--&gt;\n&lt;!--Include apache ignite--&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.phonepe.drove&lt;/groupId&gt;\n    &lt;artifactId&gt;drove-ignite-discovery&lt;/artifactId&gt;\n    &lt;version&gt;${drove.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"extra/libraries.html#sample-code_3","title":"Sample Code","text":"<pre><code>//Setup ignite\nIgniteConfigProvider igniteConfigProvider = new IgniteConfigProvider();\n\nIgniteConfiguration igniteConfiguration = igniteConfigProvider.provideIgniteConfiguration(DroveIgniteConfig.builder()\n        .communicationPortName(\"igniteComm\") // Communication port name\n        .droveEndpoint(\"http://controller1:4000,http://controller2:4000\") //Controller endpoints\n        .useAppNameForDiscovery(true) //Cluster container across multiple app versions\n        .discoveryPortName(\"igniteDiscovery\") // Discovery port name\n        .build());\n\n// Start ignite\nIgnite ignite = Ignition.start(configuration);\n</code></pre> <p>Peer discovery modes</p> <p>By default the containers will only discover and connect to containers from the same application id. If you need to connect to containers from all versions of the same application please set the <code>cluster-by-app-name</code> property to <code>true</code> as in the above example.</p>"},{"location":"extra/nvidia.html","title":"Setting up Nvidia GPU computation on executor","text":"<p>Prerequisite: Docker version <code>19.0.3+</code>. Check Docker versions and nvidia for details.</p> <p>Below steps are for ubuntu primarily for other distros check the associated links.</p>"},{"location":"extra/nvidia.html#install-nvidia-drivers-on-hosts","title":"Install nvidia drivers on hosts","text":"<p>Ubuntu provides packaged drivers for nvidia. Driver installation Guide</p> <p>Recommended <pre><code>ubuntu-drivers list --gpgpu\nubuntu-drivers install --gpgpu nvidia:535-server\n</code></pre></p> <p>Alternatively <code>apt</code> can be used, but may require additional steps Manual install <pre><code># Check for the latest stable version \napt search nvidia-driver.*server\napt install -y nvidia-driver-535-server  nvidia-utils-535-server \n</code></pre></p> <p>For other distros check Guide</p>"},{"location":"extra/nvidia.html#install-nvidia-container-toolkit","title":"Install Nvidia-container-toolkit","text":"<p>Add nvidia repo</p> <p><pre><code>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg   &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list |     sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' |     sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n\napt install -y nvidia-container-toolkit\n</code></pre> For other distros check guide here</p> <p>Configure docker with nvidia toolkit</p> <pre><code>nvidia-ctk runtime configure --runtime=docker\n\nsystemctl restart docker #Restart Docker\n</code></pre>"},{"location":"extra/nvidia.html#verify-installation","title":"Verify installation","text":"<p>On Host <code>nvidia-smi -l</code>  In docker container <code>docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi</code></p> <p><pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 535.86.10    Driver Version: 535.86.10    CUDA Version: 12.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> Verification guide</p>"},{"location":"extra/nvidia.html#enable-nvidia-support-on-drove","title":"Enable nvidia support on drove","text":"<p>Enable Nvidia support in drove-executor.yml and restart drove-executor <pre><code>...\nresources:\n  ...\n  enableNvidiaGpu: true\n...\n</code></pre></p>"},{"location":"tasks/index.html","title":"Introduction","text":"<p>A task is a representation for transient containerized workloads on the cluster. A task instance is supposed to have a much shorter life-time than an application instance. Use tasks to spin up things like automation scripts etc.</p>"},{"location":"tasks/index.html#primary-differences-with-an-application","title":"Primary differences with an application","text":"<p>Please note the following important differences between a task instance and application instances</p> <ul> <li>Tasks cannot expose ports and virtual hosts for incoming traffic</li> <li>There are no readiness checks, health checks or shutdown hooks for a task</li> <li>Task instances cannot be scaled up or down</li> <li>Tasks cannot be restarted</li> <li>A task is typically owned by an application running on the cluster.</li> <li>Task instances are not replaced if the corresponding executor node goes down during execution</li> <li>Unlike applications there is no task + task instance, the only representation of task on a Drove cluster is a task instance.</li> </ul> <p>Tip</p> <p>Use epoch to spin up tasks in a periodic manner</p> <p>A task specification contains the following sections:</p> <ul> <li>Source App Name - Name of the application that created this task</li> <li>Task ID - User supplied task ID unique in the same <code>sourceAppName</code> scope</li> <li>Executable - The container to deploy on the cluster</li> <li>Resources - CPU and Memory required for the container</li> <li>Placement Policy - How containers are to be placed in the cluster</li> <li>Environment Variables - Environment variables and values</li> <li>Volumes - Volumes to be mounted into the container</li> <li>Configs - Configs/files to be mounted into the container</li> <li>Logging details - Logging spec (for example rsyslog server)</li> <li>Tags - A map of strings for additional metadata</li> </ul>"},{"location":"tasks/index.html#task-id","title":"Task ID","text":"<p>Identification of a task is a bit more complicated on Drove. There is a Task ID (<code>{sourceAppName}-{taskId}</code>) which is used internally in drove. This is returned to the client when task is created.</p> <p>However, clients are supposed to use the <code>{sourceAppName,taskId}</code> combo they have sent in the task spec to address and send commands to their tasks.</p>"},{"location":"tasks/index.html#task-states-and-operations","title":"Task States and operations","text":"<p>Tasks on Drove have their own life cycle modelled as a state machine. State transitions can be triggered by issuing operations using the APIs.</p>"},{"location":"tasks/index.html#states","title":"States","text":"<p>Tasks on a Drove cluster can be one of the following states:</p> <ul> <li>PENDING - Task has been submitted, yet to be provisioned</li> <li>PROVISIONING - Task is assigned to an executor and docker image i ng -ownloaded</li> <li>PROVISIONING_FAILED - Docker image download failed</li> <li>STARTING - Docker run is starting</li> <li>RUNNING - Task is running currently</li> <li>RUN_COMPLETED - Task run has completed. Whether passed or failed n -o be checked from the task result.</li> <li>DEPROVISIONING - Docker image cleanup underway</li> <li>STOPPED - Task cleanup completed. This is a terminal state.</li> <li>LOST - Task disappeared while executor was down.</li> <li>UNKNOWN - All tasks that are running are put in this state when executor has been restarted and startup recovery has not kicked in yet</li> </ul>"},{"location":"tasks/index.html#operations","title":"Operations","text":"<p>The following task operations are recognized by Drove:</p> <ul> <li>CREATE - Create a task. The Task definition/spec is provided as an argument to this.</li> <li>KILL - Kill a task. The task ID is taken as a parameter.</li> </ul> <p>Tip</p> <p>All operations need Cluster Operation Spec which can be used to control the timeout and parallelism of tasks generated by the operation.</p>"},{"location":"tasks/index.html#task-state-machine","title":"Task State Machine","text":"<p>The following state machine signifies the states and transitions as affected by cluster state and operations issued.</p> <p></p>"},{"location":"tasks/operations.html","title":"Task Operations","text":"<p>This page discusses operations relevant to Task management. Please go over the Task State Machine to understand the different states a task  can be in and how operations applied (and external changes) move a task from one state to another.</p> <p>Note</p> <p>Please go through Cluster Op Spec to understand the operation parameters being sent.</p> <p>For tasks only the <code>timeout</code> parameter is relevant.</p> <p>Note</p> <p>Only one operation can be active on a particular task identified by a <code>{sourceAppName,taskId}</code> at a time.</p> <p>Warning</p> <p>Only the leader controller will accept and process operations. To avoid confusion, use the controller endpoint exposed by Drove Gateway to issue commands.</p>"},{"location":"tasks/operations.html#cluster-operation-specification","title":"Cluster Operation Specification","text":"<p>When an operation is submitted to the cluster, a cluster op spec needs to be specified. This is needed to control different aspects of the operation, including parallelism of an operation or increase the timeout for the operation and so on. </p> <p>The following aspects of an operation can be configured:</p> Name Option Description Timeout <code>timeout</code> The duration after which Drove considers the operation to have timed out. Parallelism <code>parallelism</code> Parallelism of the task. (Range: 1-32) Failure Strategy <code>failureStrategy</code> Set this to <code>STOP</code>. <p>Note</p> <p>For internal recovery operations, Drove generates it's own operations. For that, Drove applies the following cluster operation spec:</p> <ul> <li>timeout - 300 seconds</li> <li>parallelism - 1</li> <li>failureStrategy - <code>STOP</code></li> </ul> <p>The default operation spec can be configured in the controller configuration file. It is recommended to set this to a something like 8 for faster recovery.</p>"},{"location":"tasks/operations.html#how-to-initiate-an-operation","title":"How to initiate an operation","text":"<p>Tip</p> <p>Use the Drove CLI to perform all manual operations.</p> <p>All operations for task lifecycle management need to be issued via a POST HTTP call to the leader controller endpoint on the path <code>/apis/v1/tasks/operations</code>. API will return HTTP OK/200 and relevant json response as payload.</p> <p>Sample api call:</p> <pre><code>curl --location 'http://drove.local:7000/apis/v1/tasks/operations' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Basic YWRtaW46YWRtaW4=' \\\n--data '{\n    \"type\": \"KILL\",\n    \"sourceAppName\" : \"TEST_APP\",\n    \"taskId\" : \"T0012\",\n    \"opSpec\": {\n        \"timeout\": \"5m\",\n        \"parallelism\": 1,\n        \"failureStrategy\": \"STOP\"\n    }\n}'\n</code></pre> <p>Note</p> <p>In the above examples, <code>http://drove.local:7000</code> is the endpoint of the leader. <code>TEST_APP</code> is the <code>name</code> of the application that started this task and <code>taskId</code> is a unique client generated id. Authorization is basic auth.</p> <p>Warning</p> <p>Task operations are not cancellable.</p>"},{"location":"tasks/operations.html#create-a-task","title":"Create a task","text":"<p>A task can be created issuing the following command.</p> <p>Preconditions: - Task with same <code>{sourceAppName,taskId}</code> should not exist on the cluster.</p> <p>State Transition:</p> <ul> <li>none \u2192 <code>PENDING</code> \u2192 <code>PROVISIONING</code> \u2192 <code>STARTING</code> \u2192 <code>RUNNING</code> \u2192 <code>RUN_COMPLETED</code> \u2192 <code>DEPROVISIONING</code> \u2192 <code>STOPPED</code></li> </ul> <p>To create a task a Task Spec needs to be created first.</p> <p>Once ready, CLI command needs to be issued or the following payload needs to be sent:</p> Drove CLIJSON <pre><code>drove -c local tasks create sample/test_task.json\n</code></pre> <p>Sample Request Payload <pre><code>{\n    \"type\": \"CREATE\",\n    \"spec\": {...}, //(1)!\n    \"opSpec\": { //(2)!\n        \"timeout\": \"5m\",\n        \"parallelism\": 1,\n        \"failureStrategy\": \"STOP\"\n    }\n}\n</code></pre></p> <ol> <li>Spec as mentioned in Task Specification</li> <li>Operation spec as mentioned in Cluster Op Spec</li> </ol> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"taskId\": \"TEST_APP-T0012\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Warning</p> <p>There are no separate create/run steps in a task. Creation will start execution automatically and immediately.</p>"},{"location":"tasks/operations.html#kill-a-task","title":"Kill a task","text":"<p>A task can be created issuing the following command.</p> <p>Preconditions: - Task with same <code>{sourceAppName,taskId}</code> needs to exist on the cluster.</p> <p>State Transition:</p> <ul> <li><code>RUNNING</code> \u2192 <code>RUN_COMPLETED</code> \u2192 <code>DEPROVISIONING</code> \u2192 <code>STOPPED</code></li> </ul> <p>CLI command needs to be issued or the following payload needs to be sent:</p> Drove CLIJSON <pre><code>drove -c local tasks kill TEST_APP T0012\n</code></pre> <p>Sample Request Payload <pre><code>{\n    \"type\": \"KILL\",\n    \"sourceAppName\" : \"TEST_APP\",//(1)!\n    \"taskId\" : \"T0012\",//(2)!\n    \"opSpec\": {//(3)!\n        \"timeout\": \"5m\",\n        \"parallelism\": 1,\n        \"failureStrategy\": \"STOP\"\n    }\n}\n</code></pre></p> <ol> <li>Source app name as mentioned in spec during task creation</li> <li>Task ID as mentioned in the spec</li> <li>Operation spec as mentioned in Cluster Op Spec</li> </ol> <p>Sample response <pre><code>{\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"taskId\": \"T0012\"\n    },\n    \"message\": \"success\"\n}\n</code></pre></p> <p>Note</p> <p>Task metadata will remain on the cluster for some time. Metadata cleanup for tasks is automatic and can be configured in the controller configuration.</p>"},{"location":"tasks/specification.html","title":"Task Specification","text":"<p>A task is defined using JSON. We use a sample configuration below to explain the options.</p>"},{"location":"tasks/specification.html#sample-task-definition","title":"Sample Task Definition","text":"<pre><code>{\n    \"sourceAppName\": \"TEST_APP\",//(1)!\n    \"taskId\": \"T0012\",//(2)!\n    \"executable\": {//(3)!\n        \"type\": \"DOCKER\", // (4)!\n        \"url\": \"ghcr.io/appform-io/test-task\",//(5)!\n        \"dockerPullTimeout\": \"100 seconds\"//(6)!\n    },\n     \"resources\": [//(7)!\n        {\n            \"type\": \"CPU\",\n            \"count\": 1//(8)!\n        },\n        {\n            \"type\": \"MEMORY\",\n            \"sizeInMB\": 128//(9)!\n        }\n    ],\n    \"volumes\": [//(10)!\n        {\n            \"pathInContainer\": \"/data\",//(11)!\n            \"pathOnHost\": \"/mnt/datavol\",//(12)!\n            \"mode\" : \"READ_WRITE\"//(13)!\n        }\n    ],\n    \"configs\" : [//(14)!\n        {\n            \"type\" : \"INLINE\",//(15)!\n            \"localFilename\": \"/testfiles/drove.txt\",//(16)!\n            \"data\" : \"RHJvdmUgdGVzdA==\"//(17)!\n        }\n    ],\n    \"placementPolicy\": {//(18)!\n        \"type\": \"ANY\"//(19)!\n    },\n    \"env\": {//(20)!\n        \"CORES\": \"8\"\n    },\n    \"args\" : [] //(27)!\n    \"tags\": { //(21)!\n        \"superSpecialApp\": \"yes_i_am\",\n        \"say_my_name\": \"heisenberg\"\n    },\n    \"logging\": {//(22)!\n        \"type\": \"LOCAL\",//(23)!\n        \"maxSize\": \"100m\",//(24)!\n        \"maxFiles\": 3,//(25)!\n        \"compress\": true//(26)!\n    }\n}\n</code></pre> <ol> <li>Name of the <code>application</code> that has started the task. Make sure this is a valid application on the cluster.</li> <li>An unique ID for this task. Uniqueness is up to the user, Drove will scope it in the <code>sourceAppName</code> namespace.</li> <li>Coordinates for the executable. Refer to Executable Specification for details.</li> <li>Right now the only type supported is <code>DOCKER</code>.</li> <li>Docker container address</li> <li>Timeout for container pull.</li> <li>Volumes to be mounted. Refer to Volume Specification for details.</li> <li>Path that will be visible inside the container for this mount.</li> <li>Actual path on the host machine for the mount.</li> <li>Mount mode can be <code>READ_WRITE</code> and <code>READ_ONLY</code></li> <li>Configuration to be injected as file inside the container. Please refer  Config Specification for details.</li> <li>Type of config. Can be <code>INLINE</code>, <code>EXECUTOR_LOCAL_FILE</code>, ONTROLLER_HTTP_FETCH<code>and</code>EXECUTOR_HTTP_FETCH`. Specifies how drove will t the contents to be injected..</li> <li>File name for the config inside the container.</li> <li>Serialized form of the data, this and other parameters will vary cording to the <code>type</code> specified above.</li> <li>List of resources required to run this application. Check Resource Requirements Specification for more tails.</li> <li>Number of CPU cores to be allocated.</li> <li>Amount of memory to be allocated expressed in Megabytes</li> <li>Specifies how the container will be placed on the cluster. Check Placement Policy for details.</li> <li>Type of placement can be <code>ANY</code>, <code>ONE_PER_HOST</code>, <code>MATCH_TAG</code>, <code>NO_TAG</code>, <code>RULE_BASED</code>, <code>ANY</code> and <code>COMPOSITE</code>. Rest of the parameters in this section will depend on the type.</li> <li>Custom environment variables. Additional variables are injected by Drove  as well. See Environment Variables section for tails.</li> <li>Key value metadata that can be used in external systems.</li> <li>Specify how docker log files are configured. Refer to Logging Specification</li> <li>Log to local file</li> <li>Maximum File Size</li> <li>Number of latest log files to retain</li> <li>Log files will be compressed</li> <li>List of command line arguments. See Command Line Arguments for details.</li> </ol> <p>Warning</p> <p>Please make sure <code>sourceAppName</code> is set to a correct application name as specified in the <code>name</code> parameter of a running application on the cluster.</p> <p>If this is not done, stale task metadata will not be cleaned up and your metadata store performance will get affected over time.</p>"},{"location":"tasks/specification.html#executable-specification","title":"Executable Specification","text":"<p>Right now Drove supports only docker containers. However as engines, both docker and podman are supported. Drove executors will fetch the executable directly from the registry based on the configuration provided.</p> Name Option Description Type <code>type</code> Set type to <code>DOCKER</code>. URL <code>url</code> Docker container URL`. Timeout <code>dockerPullTimeout</code> Timeout for docker image pull. <p>Note</p> <p>Drove supports docker registry authentication. This can be configured in the executor configuration file.</p>"},{"location":"tasks/specification.html#resource-requirements-specification","title":"Resource Requirements Specification","text":"<p>This section specifies the hardware resources required to run the container. Right now only CPU and MEMORY are supported as resource types that can be reserved for a container.</p>"},{"location":"tasks/specification.html#cpu-requirements","title":"CPU Requirements","text":"<p>Specifies number of cores to be assigned to the container.</p> Name Option Description Type <code>type</code> Set type to <code>CPU</code> for this. Count <code>count</code> Number of cores to be assigned."},{"location":"tasks/specification.html#memory-requirements","title":"Memory Requirements","text":"<p>Specifies amount of memory to be allocated to a container.</p> Name Option Description Type <code>type</code> Set type to <code>MEMORY</code> for this. Count <code>sizeInMB</code> Amount of memory (in Mega Bytes) to be allocated. <p>Sample <pre><code>[\n    {\n        \"type\": \"CPU\",\n        \"count\": 1\n    },\n    {\n        \"type\": \"MEMORY\",\n        \"sizeInMB\": 128\n    }\n]\n</code></pre></p> <p>Note</p> <p>Both <code>CPU</code> and <code>MEMORY</code> configurations are mandatory.</p>"},{"location":"tasks/specification.html#volume-specification","title":"Volume Specification","text":"<p>Files and directories can be mounted from the executor host into the container. The <code>volumes</code> section contains a list of volumes that need to be mounted.</p> Name Option Description Path In Container <code>pathInContainer</code> Path that will be visible inside the container for this mount. Path On Host <code>pathOnHost</code> Actual path on the host machine for the mount. Mount Mode <code>mode</code> Mount mode can be <code>READ_WRITE</code> and <code>READ_ONLY</code> to allow the containerized process to write or read to the volume. <p>Info</p> <p>We do not support mounting remote volumes as of now.</p>"},{"location":"tasks/specification.html#config-specification","title":"Config Specification","text":"<p>Drove supports injection of configuration files into containers. The specifications for the same are discussed below.</p>"},{"location":"tasks/specification.html#inline-config","title":"Inline config","text":"<p>Inline configuration can be added in the Application Specification itself. This will manifest as a file inside the container.</p> <p>The following details are needed for this:</p> Name Option Description Type <code>type</code> Set the value to <code>INLINE</code> Local Filename <code>localFilename</code> File name for the config inside the container. Data <code>data</code> Base64 encoded string for the data. The value for this will be masked on UI. <p>Config file: <pre><code>port: 8080\nlogLevel: DEBUG\n</code></pre> Corresponding config specification: <pre><code>{\n    \"type\" : \"INLINE\",\n    \"localFilename\" : \"/config/service.yml\",\n    \"data\" : \"cG9ydDogODA4MApsb2dMZXZlbDogREVCVUcK\"\n}\n</code></pre></p> <p>Warning</p> <p>The full base 64 encoded config data will get stored in Drove ZK and will be pushed to executors inline. It is not recommended to stream large config files to containers using this method. This will probably need additional configuration on your ZK cluster.</p>"},{"location":"tasks/specification.html#locally-loaded-config","title":"Locally loaded config","text":"<p>Config file from a path on the executor directly. Such files can be distributed to the executor host using existing configuration management systems such as OpenTofu, Salt etc.</p> <p>The following details are needed for this:</p> Name Option Description Type <code>type</code> Set the value to <code>EXECUTOR_LOCAL_FILE</code> Local Filename <code>localFilename</code> File name for the config inside the container. File path <code>filePathOnHost</code> Path to the config file on executor host. <p>Sample config specification: <pre><code>{\n    \"type\" : \"EXECUTOR_LOCAL_FILE\",\n    \"localFilename\" : \"/config/service.yml\",\n    \"data\" : \"/mnt/configs/myservice/config.yml\"\n}\n</code></pre></p>"},{"location":"tasks/specification.html#controller-fetched-config","title":"Controller fetched Config","text":"<p>Config file can be fetched from a remote server by the controller. Once fetched, these will be streamed to the executor as part of the instance specification for starting a container.</p> <p>The following details are needed for this:</p> Name Option Description Type <code>type</code> Set the value to <code>CONTROLLER_HTTP_FETCH</code> Local Filename <code>localFilename</code> File name for the config inside the container. HTTP Call Details <code>http</code> HTTP Call related details. Please refer to HTTP Call Specification for details. <p>Sample config specification: <pre><code>{\n    \"type\" : \"CONTROLLER_HTTP_FETCH\",\n    \"localFilename\" : \"/config/service.yml\",\n    \"http\" : {\n        \"protocol\" : \"HTTP\",\n        \"hostname\" : \"configserver.internal.yourdomain.net\",\n        \"port\" : 8080,\n        \"path\" : \"/configs/myapp\",\n        \"username\" : \"appuser\",\n        \"password\" : \"secretpassword\"\n    }\n}\n</code></pre></p> <p>Note</p> <p>The controller will make an API call for every single time it asks an executor to spin up a container. Please make sure to account for this in your configuration management system.</p>"},{"location":"tasks/specification.html#executor-fetched-config","title":"Executor fetched Config","text":"<p>Config file can be fetched from a remote server by the executor before spinning up a container. Once fetched, the payload will be injected as a config file into the container.</p> <p>The following details are needed for this:</p> Name Option Description Type <code>type</code> Set the value to <code>EXECUTOR_HTTP_FETCH</code> Local Filename <code>localFilename</code> File name for the config inside the container. HTTP Call Details <code>http</code> HTTP Call related details. Please refer to HTTP Call Specification for details. <p>Sample config specification: <pre><code>{\n    \"type\" : \"EXECUTOR_HTTP_FETCH\",\n    \"localFilename\" : \"/config/service.yml\",\n    \"http\" : {\n        \"protocol\" : \"HTTP\",\n        \"hostname\" : \"configserver.internal.yourdomain.net\",\n        \"port\" : 8080,\n        \"path\" : \"/configs/myapp\",\n        \"username\" : \"appuser\",\n        \"password\" : \"secretpassword\"\n    }\n}\n</code></pre></p> <p>Note</p> <p>All executors will make an API call for every single time they spin up a container for this application. Please make sure to account for this in your configuration management system.</p>"},{"location":"tasks/specification.html#http-call-specification","title":"HTTP Call Specification","text":"<p>This section details the options that can set when making http calls to a configuration management system from controllers or executors.</p> <p>The following options are available for HTTP call:</p> Name Option Description Protocol <code>protocol</code> Protocol to use for upstream call. Can be <code>HTTP</code> or <code>HTTPS</code>. Hostname <code>hostname</code> Host to call. Port <code>port</code> Provide custom port. Defaults to 80 for http and 443 for https. API Path <code>path</code> Path component of the URL. Include query parameters here. Defaults to <code>/</code> HTTP Method <code>verb</code> Type of call, use <code>GET</code>, <code>POST</code> or <code>PUT</code>. Defaults to <code>GET</code>. Success Code <code>successCodes</code> List of HTTP status codes which is considered as success. Defaults to <code>[200]</code> Payload <code>payload</code> Data to be used for POST and PUT calls Connection Timeout <code>connectionTimeout</code> Timeout for upstream connection. Operation timeout <code>operationTimeout</code> Timeout for actual operation. Username <code>username</code> Username to be used basic auth. This field is masked out on the UI. Password <code>password</code> Password to be used for basic auth. This field is masked on the UI. Authorization Header <code>authHeader</code> Data to be passed in HTTP <code>Authorization</code> header. This field is masked on the UI. Additional Headers <code>headers</code> Any other headers to be passed to the upstream in the HTTP calls. This is a map of Skip SSL Checks <code>insecure</code> Skip hostname and certification checks during SSL handshake with the upstream."},{"location":"tasks/specification.html#placement-policy-specification","title":"Placement Policy Specification","text":"<p>Placement policy governs how Drove deploys containers on the cluster. The following sections discuss the different placement policies available and how they can be configured to achieve optimal placement of containers.</p> <p>Warning</p> <p>All policies will work only at a <code>{appName, version}</code> combination level. They will not ensure constraints at an <code>appName</code> level. This means that for somethinge like a one per node placement, for the same <code>appName</code>, multiple containers can run on the same host if multiple deployments with different <code>version</code>s are active in a cluster. Same applies for all policies like N per host and so on.</p> <p>Important details about executor tagging</p> <ul> <li>All hosts have at-least one tag, it's own hostname.</li> <li>The <code>TAG</code> policy will consider them as valid tags. This can be used to place containers on specific hosts if needed.</li> <li>This is handled specially in all other policy types and they will consider executors having only the hostname tag as untagged.</li> <li>A host with a tag (other than host) will not have any containers running if not placed on them specifically using the <code>MATCH_TAG</code> policy</li> </ul>"},{"location":"tasks/specification.html#any-placement","title":"Any Placement","text":"<p>Containers for a <code>{appName, version}</code> combination can run on any un-tagged executor host.</p> Name Option Description Policy Type <code>type</code> Put <code>ANY</code> as policy. <p>Sample: <pre><code>{\n    \"type\" : \"ANY\"\n}\n</code></pre></p> <p>Tip</p> <p>For most use-cases this is the placement policy to use.</p>"},{"location":"tasks/specification.html#one-per-host-placement","title":"One Per Host Placement","text":"<p>Ensures that only one container for a particular <code>{appName, version}</code> combination is running on an executor host at a time.</p> Name Option Description Policy Type <code>type</code> Put <code>ONE_PER_HOST</code> as policy. <p>Sample: <pre><code>{\n    \"type\" : \"ONE_PER_HOST\"\n}\n</code></pre></p>"},{"location":"tasks/specification.html#max-n-per-host-placement","title":"Max N Per Host Placement","text":"<p>Ensures that at most N containers for a <code>{appName, version}</code> combination is running on an executor host at a time.</p> Name Option Description Policy Type <code>type</code> Put <code>MAX_N_PER_HOST</code> as policy. Max count <code>max</code> The maximum num of containers that can run on an executor. Range: 1-64 <p>Sample: <pre><code>{\n    \"type\" : \"MAX_N_PER_HOST\",\n    \"max\": 3\n}\n</code></pre></p>"},{"location":"tasks/specification.html#match-tag-placement","title":"Match Tag Placement","text":"<p>Ensures that containers for a <code>{appName, version}</code> combination are running on an executor host that has the tags as mentioned in the policy.</p> Name Option Description Policy Type <code>type</code> Put <code>MATCH_TAG</code> as policy. Max count <code>tag</code> The tag to match. <p>Sample: <pre><code>{\n    \"type\" : \"MATCH_TAG\",\n    \"tag\": \"gpu_enabled\"\n}\n</code></pre></p>"},{"location":"tasks/specification.html#no-tag-placement","title":"No Tag Placement","text":"<p>Ensures that containers for a <code>{appName, version}</code> combination are running on an executor host that has no tags.</p> Name Option Description Policy Type <code>type</code> Put <code>NO_TAG</code> as policy. <p>Sample: <pre><code>{\n    \"type\" : \"NO_TAG\"\n}\n</code></pre></p> <p>Info</p> <p>The NO_TAG policy is mostly for internal use, and does not need to be specified when deploying containers that do not need any special placement logic.</p>"},{"location":"tasks/specification.html#composite-policy-based-placement","title":"Composite Policy Based Placement","text":"<p>Composite policy can be used to combine policies together to create complicated placement requirements.</p> Name Option Description Policy Type <code>type</code> Put <code>COMPOSITE</code> as policy. Polices <code>policies</code> List of policies to combine Combiner <code>combiner</code> Can be <code>AND</code> and <code>OR</code> and signify all-match and any-match logic on the <code>policies</code> mentioned. <p>Sample: <pre><code>{\n    \"type\" : \"COMPOSITE\",\n    \"policies\": [\n        {\n            \"type\": \"ONE_PER_HOST\"\n        },\n        {\n            \"type\": \"MATH_TAG\",\n            \"tag\": \"gpu_enabled\"\n        }\n    ],\n    \"combiner\" : \"AND\"\n}\n</code></pre> The above policy will ensure that only one container of the relevant <code>{appName,version}</code> will run on GPU enabled machines.</p> <p>Tip</p> <p>It is easy to go into situations where no executors match complicated placement policies. Internally, we tend to keep things rather simple and use the ANY placement for most cases and maybe tags in a few places with over-provisioning or for hosts having special hardware </p>"},{"location":"tasks/specification.html#environment-variables","title":"Environment variables","text":"<p>This config can be used to inject custom environment variables to containers. The values are defined as part of deployment specification, are same across the cluster and immutable to modifications from inside the container (ie any overrides from inside the container will not be visible across the cluster).</p> <p>Sample: <pre><code>{\n    \"MY_VARIABLE_1\": \"fizz\",\n    \"MY_VARIABLE_2\": \"buzz\"\n}\n</code></pre></p> <p>The following environment variables are injected by Drove to all containers:</p> Variable Name Value HOST Hostname where the container is running. This is for marathon compatibility. PORT_<code>PORT_NUMBER</code> A variable for every port specified in <code>exposedPorts</code> section. The value is the actual port on the host, the specified port is mapped to.  For example if ports 8080 and 8081 are specified, two variables called <code>PORT_8080</code> and <code>PORT_8081</code> will be injected. DROVE_EXECUTOR_HOST Hostname where container is running. DROVE_CONTAINER_ID Container that is deployed DROVE_APP_NAME App name as specified in the Application Specification DROVE_INSTANCE_ID Actual instance ID generated by Drove DROVE_APP_ID Application ID as generated by Drove DROVE_APP_INSTANCE_AUTH_TOKEN A JWT string generated by Drove that can be used by this container to call <code>/apis/v1/internal/...</code> apis. <p>Warning</p> <p>Do not pass secrets using environment variables. These variables are all visible on the UI as is. Please use Configs to inject secrets files and so on.</p>"},{"location":"tasks/specification.html#command-line-arguments","title":"Command line arguments","text":"<p>A list of command line arguments that are sent to the container engine to execute inside the container. This is provides ways for you to configure your container behaviour based off such arguments. Please refer to docker documentation for details.</p> <p>Danger</p> <p>This might have security implications from a system point of view. As such Drove provides administrators a way to disable passing arguments at the cluster level by setting <code>disableCmdlArgs</code> to <code>true</code> in the controller configuration.</p>"},{"location":"tasks/specification.html#logging-specification","title":"Logging Specification","text":"<p>Can be used to configure how container logs are managed on the system. </p> <p>Note</p> <p>This section affects the docker log driver. Drove will continue to stream logs to it's own logger which can be configured at executor level through the executor configuration file.</p>"},{"location":"tasks/specification.html#local-logger-configuration","title":"Local Logger configuration","text":"<p>This is used to configure the <code>json-file</code> log driver.</p> Name Option Description Type <code>type</code> Set the value to <code>LOCAL</code> Max Size <code>maxSize</code> Maximum file size. Anything bigger than this will lead to rotation. Max Files <code>maxFiles</code> Maximum number of logs files to keep. Range: 1-100 Compress <code>compress</code> Enable log file compression. <p>Tip</p> <p>If <code>logging</code> section is omitted, the following configuration is applied by default: - File size: 10m - Number of files: 3 - Compression: on</p>"},{"location":"tasks/specification.html#rsyslog-configuration","title":"Rsyslog configuration","text":"<p>In case suers want to stream logs to an rsyslog server, the logging configuration needs to be set to RSYSLOG mode.</p> Name Option Description Type <code>type</code> Set the value to <code>RSYSLOG</code> Server <code>server</code> URL for the rsyslog server. Tag Prefix <code>tagPrefix</code> Prefix to add at the start of a tag Tag Suffix <code>tagSuffix</code> Suffix to add at the en of a tag. <p>Note</p> <p>The default tag is the <code>DROVE_INSTANCE_ID</code>. The <code>tagPrefix</code> and <code>tagSuffix</code> will to before and after this</p>"}]}